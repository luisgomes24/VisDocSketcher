{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1535e6ab",
   "metadata": {},
   "source": [
    "# ü§∑‚Äç 10  Classses Text Classification Yahoo Answers 42.51%% CNN\n",
    "\n",
    "![inbox_10030651_92afb049c490aa1de7885836ddb4626e_download (1).jpg](attachment:3d17cde9-7c09-4d75-b51d-5bcb6cf1f61b.jpg)\n",
    "\n",
    "1. Download and Imports\n",
    "2. Functions\n",
    "3. Loading Dataset (Yahoo Answers)\n",
    "4. Shuffeling and splitting the Data\n",
    "5. Preprocessing and Tokenization\n",
    "6. Dataset Visualization\n",
    "7. Sequence Normalization and Encoding Labels\n",
    "8. Model Preparation and Training\n",
    "9. Visualizing and Evaluating the Results\n",
    "10. Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee6bd0a",
   "metadata": {},
   "source": [
    "# 1. Download and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75028739",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions\n",
    "!pip install textsearch\n",
    "!pip install tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13832c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow and Keras and sklearn\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Charts\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import f1_score, accuracy_score,confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "#Performance Plot\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 3541\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1af439",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_time function\n",
    "\n",
    "def date_time(x):\n",
    "    if x==1:\n",
    "        return 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
    "    if x==2:    \n",
    "        return 'Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
    "    if x==3:  \n",
    "        return 'Date now: %s' % datetime.datetime.now()\n",
    "    if x==4:  \n",
    "        return 'Date today: %s' % datetime.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53719dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Plot\n",
    "\n",
    "def plot_performance(history=None, figure_directory=None, ylim_pad=[0, 0]):\n",
    "    xlabel = 'Epoch'\n",
    "    legends = ['Training', 'Validation']\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    y1 = history.history['accuracy']\n",
    "    y2 = history.history['val_accuracy']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[0]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[0]\n",
    "\n",
    "\n",
    "    plt.subplot(121)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Accuracy\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Accuracy', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['loss']\n",
    "    y2 = history.history['val_loss']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(122)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Loss\\n'+date_time(1), fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Loss', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "    if figure_directory:\n",
    "        plt.savefig(figure_directory+\"/history\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2077146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing Function\n",
    "\n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "        \n",
    "        if doc != float:\n",
    "            doc = strip_html_tags(doc)\n",
    "            doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "            doc = doc.lower()\n",
    "            doc = remove_accented_chars(doc)\n",
    "            doc = contractions.fix(doc)\n",
    "            # lower case and remove special characters\\whitespaces\n",
    "            doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "            doc = re.sub(' +', ' ', doc)\n",
    "            doc = doc.strip()  \n",
    "            norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d8e14",
   "metadata": {},
   "source": [
    "# 3. Loading Dataset (Yahoo Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e62dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading DAtaset from csv files to Pandas dataframes\n",
    "dataset_train = pd.read_csv('../input/yahoo-answers-10-categories-for-nlp-csv/10_categories_of_yahoo_answers_for_nlp_tasks_csv/train.csv')\n",
    "dataset_test = pd.read_csv('../input/yahoo-answers-10-categories-for-nlp-csv/10_categories_of_yahoo_answers_for_nlp_tasks_csv/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if there is NaN Columns\n",
    "#dataset_train[dataset_train['best_answer'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c998c1b",
   "metadata": {},
   "source": [
    "# 4. Shuffeling and splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287744bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting NaN Rows\n",
    "dataset_train=dataset_train.dropna()\n",
    "dataset_test=dataset_test.dropna()\n",
    "\n",
    "# Shuffling Data\n",
    "\n",
    "train = dataset_train.sample(frac=1)\n",
    "test = dataset_test.sample(frac=1)\n",
    "\n",
    "# Taking only a small peice of the dataset to avoid long training time\n",
    "\n",
    "test = dataset_test.iloc[:100000,:]\n",
    "val = dataset_train.iloc[:50000,:]\n",
    "train = dataset_train.iloc[50000:,:]\n",
    "train = dataset_train.iloc[:100000,:]\n",
    "\n",
    "# Splitting data to train and validation sets manually, only including neccessary columns\n",
    "\n",
    "X_train = train['best_answer'].values\n",
    "y_train = train['class_index'].values\n",
    "\n",
    "X_val = val['best_answer'].values\n",
    "y_val = val['class_index'].values\n",
    "\n",
    "X_test = test['best_answer'].values\n",
    "y_test = test['class_index'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c1c5f",
   "metadata": {},
   "source": [
    "# 5. Preprocessing and Tokenization\n",
    "\n",
    "To prepare text data for our deep learning model, we transform each review into a sequence.\n",
    "Every word in the review is mapped to an integer index and thus the sentence turns into a sequence of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Pre-processing the Data (Yahoo Answers)\n",
    "\n",
    "X_train = pre_process_corpus(X_train)\n",
    "X_val = pre_process_corpus(X_val)\n",
    "X_test = pre_process_corpus(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenazing the Data (Yahoo Answers)\n",
    "\n",
    "t = Tokenizer(oov_token='<UNK>')\n",
    "# fit the tokenizer on train documents\n",
    "t.fit_on_texts(X_train)\n",
    "t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming Answers to Sequences\n",
    "\n",
    "X_train = t.texts_to_sequences(X_train)\n",
    "X_test = t.texts_to_sequences(X_test)\n",
    "X_val = t.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cadca3",
   "metadata": {},
   "source": [
    "# 6. Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3486ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Vocabulary Size and the number of Answers\n",
    "\n",
    "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "print(\"Number of Answers={}\".format(t.document_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the size of the sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_lens = [len(s) for s in X_train]\n",
    "test_lens = [len(s) for s in X_test]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "h1 = ax[0].hist(train_lens)\n",
    "h2 = ax[1].hist(test_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bea4e2",
   "metadata": {},
   "source": [
    "# 7. Sequence Normalization and Encoding Labels\n",
    "\n",
    "Not all answers are of same length. To handle this difference in length of answers, we define a maximum length.\n",
    "For answers which are smaller than this length, we pad them with zeros which longer ones are truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9156f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the dataset to a maximum review length in words\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=800)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=800)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b3a666",
   "metadata": {},
   "source": [
    "The dataset contains labels of the form positive/negative. The following step encodes the labels using ```sklearn's``` ```LabelEncoder```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Labels\n",
    "\n",
    "le = LabelEncoder()\n",
    "num_classes=10 # 1 = Society & Culture 2 = Science & Mathematics 3 = Health 4 = Education & Reference 5 = Computers & Internet 6 = Sports 7 = Business & Finance 8 = Entertainment & Music 9 = Family & Relationships 10 = Politics & Government\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "y_val = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a4b6f",
   "metadata": {},
   "source": [
    "# 8. Model Preparation and Training\n",
    "\n",
    "Before preparing the model, we need to set some variables that will be needed later# Setting Vocabulary size and Emmbedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883336bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Vocabulary size and Emmbedding size\n",
    "\n",
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 800\n",
    "\n",
    "# Setting an Early_stopping callback to monitor accuracy with a patience degree of 2\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d56d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "model = Sequential()\n",
    "# The Embedding layer\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=800))\n",
    "# The first one dimensional convolutional layer (32,4,same,relu)\n",
    "model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
    "# The first Max pooling layer (2)\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# The first Dropout layer (15%)\n",
    "model.add(Dropout(rate=0.15))\n",
    "# The second one dimensional convolutional layer (32,4,same,relu)\n",
    "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "# The second Max pooling layer (2)\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# The second Dropout layer (10%)\n",
    "model.add(Dropout(rate=0.15))\n",
    "# The third one dimensional convolutional layer (32,4,same,relu)\n",
    "model.add(Conv1D(filters=256, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(Conv1D(filters=256, kernel_size=4, padding='same', activation='relu'))\n",
    "# The third Max pooling layer (2)\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# The third Dropout layer (10%)\n",
    "model.add(Dropout(rate=0.20))\n",
    "# The Flattening layer\n",
    "model.add(Flatten())\n",
    "# The First Dense Layer (256,relu)\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# The Second Dense Layer or Prediction layer (1,sigmoid)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# Compiling the Model using the Binary_Crossontropy as a loss function and accuracy as a meseaure and Adam as an Optimizer\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])\n",
    "# Displaying the Model Schema\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model with GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    history1 = model.fit(X_train, y_train, validation_data=(X_val,y_val),epochs=25, batch_size=64, verbose=1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025b248",
   "metadata": {},
   "source": [
    "# 9. Visualizing and Evaluating the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy / Validation_Accuracy  and Loss / Validation_Loss Plot\n",
    "\n",
    "plot_performance(history=history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81679393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c225ae",
   "metadata": {},
   "source": [
    "# 10. Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fef035",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('10_Classes_Yahoo_Answers_CNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4af26d",
   "metadata": {},
   "source": [
    "If you have any questions, feel free to contact me at yassiracharki@gmail.com or on Kaggle (:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
