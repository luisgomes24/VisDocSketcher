{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf29927",
   "metadata": {},
   "source": [
    "# Title\n",
    "Back to basics! Let's revisit basic ML algorithms from scratch. In this notebook, I implement a vanilla version of decision tree classifier from scratch using pandas and numpy. In addition, also implemented ROC Curve from scratch and ended with comparing both decision tree and roc functions with SKlearn's implementation on breast cancer dataset from Sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47188e1d",
   "metadata": {},
   "source": [
    "## Libraries for basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb108b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC\n",
    "import pandas as pd \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebf911",
   "metadata": {},
   "source": [
    "## Vanilla Decision Tree Classifier Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00fe630",
   "metadata": {},
   "source": [
    "Some points to note:\n",
    "- Evaluation metric used to create splits: Weighted gini impurity. Recommended reading: https://www.learndatasci.com/glossary/gini-impurity/\n",
    "- Regularization methods: max_depth (maximum depth of decision tree) and min_leaf_size (minimum samples in each leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaDecisionTreeClassifier(ABC):\n",
    "    def __init__(self, target='y', max_depth=3, min_leaf_size=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_size = min_leaf_size \n",
    "        self.target = target # name of target feature. single dataframe with feature and target is passed during fit\n",
    "        self.lowest_impurity = 1e3 # initialization of high number\n",
    "    \n",
    "    def total_split_impurity(self, left_df, right_df):\n",
    "        \"\"\"\n",
    "        Measurement used to find best feature-value pair for each split of classification decision trees.  \n",
    "        \n",
    "        This function gives total gini impurity value (scalar) given data that goes in left and right nodes\n",
    "        \n",
    "        left_df: (pd.DataFrame) Data going to left node after split\n",
    "        right_df: (pd.DataFrame) Data going to right node after split\n",
    "        \n",
    "        Return: total gini impurity by node split weighted by samples going in left and right nodes\n",
    "        \"\"\"\n",
    "        gini_left = self.gini_impurity(left_df)\n",
    "        gini_right = self.gini_impurity(right_df)\n",
    "        total_impurity = gini_left*(left_df.shape[0]/(left_df.shape[0] + right_df.shape[0])) + \\\n",
    "        gini_right*(right_df.shape[0]/(left_df.shape[0] + right_df.shape[0]))\n",
    "        return total_impurity\n",
    "    \n",
    "    def gini_impurity(self, df):\n",
    "        \"\"\"\n",
    "        Gini impurity of single node. For more info: https://www.learndatasci.com/glossary/gini-impurity/\n",
    "        Ranges betwee 0 to 0.5 where 0 means most pure node \n",
    "        and a great candidate for split\n",
    "        \n",
    "        df: (pd.DataFrame) Data in a given node\n",
    "        \n",
    "        Return: gini impurity\n",
    "        \"\"\"\n",
    "        ypos = (df[self.target] == 1).sum()\n",
    "        yneg = (df[self.target] == 0).sum()\n",
    "        ppos = ypos/(ypos+yneg)\n",
    "        pneg = 1-ppos\n",
    "        return 1 - ((ppos**2) + (pneg**2))\n",
    "        \n",
    "                          \n",
    "    def find_best_split(self, df, current_depth=0):\n",
    "        \"\"\"\n",
    "        Given data in current node (df), it finds feature name and it's value that would give best possible split.\n",
    "        Uses total split impurity as measurement of best split. i.e. total split impurity should be lowest among all options\n",
    "        This function is called recursively starting from root to leaf until stopping criteria is reached.\n",
    "        \n",
    "        df: (pd.DataFrame) Data in current node that we are trying to split\n",
    "        current_depth: (int) Current depth of tree. \n",
    "                             Required to track the depth in order to stop when max depth is reached\n",
    "                             \n",
    "        Return: tree_dict (dict) Nested dictionary containing tree node attributes from root to leaves  \n",
    "        \"\"\"\n",
    "        # only split if the new nodes give lower impurity than what we have in current node\n",
    "        self.lowest_impurity = self.gini_impurity(df) \n",
    "        \n",
    "        # Check all features to find feature value pair giving that would give best split \n",
    "        for feat_name in df.drop(self.target,axis=1).columns:  \n",
    "            self.find_best_split_single_feat(df, feat_name)\n",
    "        \n",
    "        # if no better split found after all search, just return the y_mean value at this node\n",
    "        if self.lowest_impurity >= self.gini_impurity(df) or current_depth > self.max_depth: \n",
    "            return df[self.target].mean()\n",
    "        \n",
    "        best_feat_name, y_bucket, best_feat_val = self.best_feat_name, self.y_bucket, self.best_feat_val\n",
    "        # tree dict is used to save nested tree structure that will later be used to predict\n",
    "        tree_dict = defaultdict() \n",
    "        tree_dict[best_feat_name]  = {}\n",
    "        tree_dict[best_feat_name]['feat_value'] = best_feat_val\n",
    "        tree_dict[best_feat_name]['ymean'] = y_bucket\n",
    "        \n",
    "        # simply sort by selected feature name and take left-right\n",
    "        df_sort = df.copy().sort_values(by=self.best_feat_name, ascending=True)  # Not optimal to sort each time\n",
    "        left_tree_df = df_sort[df_sort[self.best_feat_name] <= self.best_feat_val]\n",
    "        right_tree_df = df_sort[df_sort[self.best_feat_name] > self.best_feat_val]\n",
    "\n",
    "        if (len(left_tree_df)==0) or (len(right_tree_df)==0):  # Creating balanced tree (optional)\n",
    "            return\n",
    "                \n",
    "        # Go down the tree if it finds better splits. If it does, create nested dicts and replace value at nodes\n",
    "        current_depth+=1\n",
    "        tree_dict[best_feat_name]['left'] = self.find_best_split(left_tree_df, current_depth)\n",
    "        tree_dict[best_feat_name]['right'] = self.find_best_split(right_tree_df, current_depth)\n",
    "        \n",
    "        return tree_dict\n",
    "    \n",
    "    def find_best_split_single_feat(self, df, feat_name):\n",
    "        \n",
    "        \"\"\"\n",
    "        Given data in current node (df) and feature name to check, find the value that gives best possible \n",
    "        node split (of df) at this feature. Finding a feature value that gives lower impurity than \n",
    "        currently found impurity is not guaranteed. It will only select feature-value pair if it reduces the impurity.\n",
    "        \n",
    "        df: (pd.DataFrame) Data in current node we are trying to split\n",
    "        feat_name: (str) Feature name we are trying to split on\n",
    "        \n",
    "        Return: None. Updates characterstics of best split value in class object. \n",
    "        \"\"\"\n",
    "        \n",
    "        # It is not optimal to sort and reset index each time\n",
    "        # But we are not really optimizing for now\n",
    "        # This is a simple vanilla implementation for understanding\n",
    "        df_sort= df.copy().sort_values(by=feat_name)\n",
    "        df_sort.reset_index(drop=True, inplace=True)\n",
    "        uniq_vals = df_sort[feat_name].unique()  # only check unique values to save some redundancy\n",
    "        for i in uniq_vals: #range(self.min_leaf_size, df_sort.shape[0] - self.min_leaf_size): \n",
    "            \n",
    "            # Left and Right datasets should be equal or larger than min_leaf_size. \n",
    "            left_df = df_sort[df_sort[feat_name] <= i] \n",
    "            right_df = df_sort[df_sort[feat_name] > i]\n",
    "            \n",
    "            if len(left_df)<self.min_leaf_size or len(right_df)<self.min_leaf_size:\n",
    "                continue # skip this value if it gives a split node smaller than min accepted leaf size\n",
    "\n",
    "            new_impurity = self.total_split_impurity(left_df, right_df)\n",
    "\n",
    "            if new_impurity < self.lowest_impurity:\n",
    "                # Update best split node attributes if new impurity is lower than lowest sofar\n",
    "                self.lowest_impurity = new_impurity\n",
    "                self.best_feat_name = feat_name\n",
    "                self.best_feat_val = i\n",
    "                self.y_bucket = df_sort[self.target].mean()\n",
    "\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Calls find_best_split function and fits the tree with input dataframe\n",
    "        \n",
    "        df: (pd.DataFrame) Training data to fit the decision tree classfier\n",
    "        \n",
    "        Returns: (dict) Fitted tree \n",
    "        \"\"\"\n",
    "        self.fitted_tree = self.find_best_split(df)\n",
    "        return self.fitted_tree\n",
    "        \n",
    "#     @property\n",
    "#     def better_split_not_found(self): return self.lowest_impurity == 1e3  # impurity is not reduced \n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_leaf_node(subtree):\n",
    "        \"\"\"\n",
    "        subtree: (dict) Given a branch of tree, check whether it is leaf\n",
    "        \n",
    "        Return (bool) True if given tree branch is leaf node, False otherwise\n",
    "        \"\"\"\n",
    "        if not subtree['left'] and not subtree['right']:  # if leaf, would not have any children\n",
    "            return True  \n",
    "        else: return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def _predict_row(fitted_tree, row):\n",
    "        \"\"\"\n",
    "        fitted_tree: (dict) Output of self.fit\n",
    "        row: (pd.DataFrame) Single row of test dataframe\n",
    "        \n",
    "        Return (Float) Prediction value for given input row. Between 0 to 1\n",
    "        \n",
    "        From top to bottom, traverse the whole tree and find which node does this data point lies in\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(fitted_tree) == float:\n",
    "            return fitted_tree # leaf has no further children\n",
    "        \n",
    "        key = list(fitted_tree.keys())[0] # feature index of current node\n",
    "        fitted_key = fitted_tree[key]\n",
    "        \n",
    "        if VanillaDecisionTreeClassifier._is_leaf_node(fitted_key):\n",
    "            return fitted_key['ymean']\n",
    "        \n",
    "        if fitted_key['feat_value'] >= row[key]:\n",
    "            return VanillaDecisionTreeClassifier._predict_row(fitted_key['left'], row)\n",
    "        else:\n",
    "            return VanillaDecisionTreeClassifier._predict_row(fitted_key['right'], row)\n",
    "        \n",
    "\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Runs predict_row for each row of test dataframe\n",
    "        \n",
    "        fitted_tree: (dict) Output of self.fit\n",
    "        X_test: (pd.DataFrame) Dataframe containing test data you want to predict using fitted decision tree\n",
    "        \n",
    "        Return: (List) Predictions from tree in given test data\n",
    "        \"\"\"\n",
    "        \n",
    "        preds = [VanillaDecisionTreeClassifier._predict_row(self.fitted_tree, X_test.iloc[i,:])\\\n",
    "                 for i in range(X_test.shape[0])]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de02297",
   "metadata": {},
   "source": [
    "### Toy Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.DataFrame({\"x1\": [1, 10, 432, 1, 60],\n",
    "                         \"x2\":[2, 5, 10, 6, 6],\n",
    "                         \"x3\":[10, 10, 10, 10, 10],\n",
    "                         \"y\":[1, 1, 0, 0, 0],\n",
    "                        })\n",
    "toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad652bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = VanillaDecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_dt = dt.fit(toy_data)\n",
    "fitted_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ea207",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.predict(toy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80da59",
   "metadata": {},
   "source": [
    "## Toy Data 2 (sklearn breast cancer dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de4434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_data = load_breast_cancer()\n",
    "X, y = sklearn_data.data, sklearn_data.target\n",
    "sklearn_datadf = pd.DataFrame(X)\n",
    "sklearn_datadf['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbaa584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split, random 75-25% splits\n",
    "\n",
    "sklearn_datadf_train = sklearn_datadf.sample(frac=0.75, random_state=10)\n",
    "\n",
    "test_idx = [i for i in range(len(sklearn_datadf)) if i not in sklearn_datadf_train.index]\n",
    "sklearn_datadf_test = sklearn_datadf.iloc[test_idx, :]\n",
    "sklearn_datadf_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5defb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = VanillaDecisionTreeClassifier(max_depth=4, min_leaf_size=2)\n",
    "fitted_dt = dt.fit(sklearn_datadf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7527a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbbf418",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = dt.predict(sklearn_datadf_test)\n",
    "train_preds = dt.predict(sklearn_datadf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dccfd9c",
   "metadata": {},
   "source": [
    "## ROC curve from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859598ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_scratch(y_labels, y_scores):\n",
    "    thresholds = np.linspace(0, 1, 1001)\n",
    "    fprs, tprs = [], []\n",
    "    for thres in thresholds:\n",
    "        tp, tn, fp, fn = confusion_matrix(y_labels, y_scores, thres)\n",
    "        tprs.append(tpr(tp, tn, fp, fn))\n",
    "        fprs.append(fpr(tp, tn, fp, fn))\n",
    "        \n",
    "    # force first and last fpr, tpr at 0 and 1 thresholds\n",
    "    fprs[0] = 1\n",
    "    fprs[-1] = 0\n",
    "    tprs[0] = 1\n",
    "    tprs[-1] = 0\n",
    "    return fprs, tprs, thresholds\n",
    "    \n",
    "def confusion_matrix(y_labels, y_scores, thres):\n",
    "    y_preds = (y_scores >= thres).astype(int)\n",
    "    tp = (np.equal(y_labels, 1) & np.equal(y_preds, 1)).sum()\n",
    "    tn = (np.equal(y_labels, 0) & np.equal(y_preds, 0)).sum()\n",
    "    fp = (np.equal(y_labels, 0) & np.equal(y_preds, 1)).sum()\n",
    "    fn = (np.equal(y_labels, 1) & np.equal(y_preds, 0)).sum()\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def tpr(tp, tn, fp, fn):\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def fpr(tp, tn, fp, fn):\n",
    "    return fp/(fp+tn)\n",
    "\n",
    "def auc_scratch(fprs, tprs):\n",
    "    \"\"\"\n",
    "    Cut in small rectangles and sum areas\n",
    "    \"\"\"\n",
    "    total_auc = 0.\n",
    "    for i in range(1000):  # divide curve in 1000 rectangles\n",
    "        total_auc += (fprs[i] - fprs[i+1])*((tprs[i+1] + tprs[i])/2.)\n",
    "    return total_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703bb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs, tprs, thresholds = roc_curve_scratch(sklearn_datadf_test['y'], np.array(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7df925",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scratch(fprs, tprs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ae559",
   "metadata": {},
   "source": [
    "## Comparing with sklearn implementations of DecisionTreeClassifier and roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=4, min_samples_leaf=2)\n",
    "dtc.fit(sklearn_datadf_train.drop('y', axis=1), sklearn_datadf_train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_sk = dtc.predict_proba(sklearn_datadf_test.drop('y', axis=1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d83846",
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs_sk, tprs_sk, _ = roc_curve(sklearn_datadf_test['y'], test_preds_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(fprs_sk, tprs_sk, label= f'Sklearn implementations. AUC: {auc(fprs_sk, tprs_sk):0.3f}', color='g')\n",
    "ax.plot(fprs, tprs, label= f'Scratch implementations. AUC: {auc_scratch(fprs, tprs):0.3f}', color='r')\n",
    "\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af897476",
   "metadata": {},
   "source": [
    "Pretty close performance. Yay!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0eb5c2",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
