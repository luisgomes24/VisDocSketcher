{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebdbcaf8",
   "metadata": {},
   "source": [
    "# Exploratory data analysis (EDA)\n",
    "\n",
    "The tasks that are assigned to analysts are quite diverse. However, it all starts with data.\n",
    "\n",
    "In this course, we will not touch on the business side of data analysis, but at the same time, we need to understand that data is not taken \"out of thin air\". As well as the tasks associated with it. In the book written by [Bill Franks](https://play.google.com/store/books/details/%D0%91_%D0%A4%D1%80%D1%8D%D0%BD%D0%BA%D1%81_%D0%A0%D0%B5%D0%B2%D0%BE%D0%BB%D1%8E%D1%86%D0%B8%D1%8F_%D0%B2_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D1%82%D0%B8%D0%BA%D0%B5_%D0%9A%D0%B0%D0%BA_%D0%B2_%D1%8D%D0%BF%D0%BE%D1%85%D1%83_Big_Dat?id=yPvkDQAAQBAJ) about operational analytics, the author focuses on the fact that ill-conceived investments in the collection and storage of data on the principle of \"what if they come in handy later\" often do not justify themselves. Only after a specific goal has been set the process of collecting (or possibly buying) and analyzing data can begin.\n",
    "\n",
    "Unfortunately, in practice, raw data is usually unsuitable and/or unusable for analysis. The process of preparing and cleaning the data (data preparation, preprocessing, data cleaning) can be **very time-consuming** and take more time than actually building and validating models based on data. Let's highlight some of the components of this process:\n",
    "\n",
    "- data specification and understanding\n",
    "- data editing, error correction\n",
    "- working with missing values\n",
    "- normalization (standartization)\n",
    "- feature extraction and selection\n",
    "\n",
    "As a result, we obtain data in a convenient format, usually in a tabular format. A table (or dataframe) has a \"objects-features\" structure: rows correspond to individual entities (objects, examples, instances), and columns correspond to attributes of these entities (features). We will see the example very soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da933af6",
   "metadata": {},
   "source": [
    "## Python library ecosystem. NumPy\n",
    "\n",
    "Python is a high-level general purpose programming language. Today it is the most popular programming language in Data Science and Machine Learning. However, \"pure\" Python has a number of disadvantages, mainly related to code execution time. Traditional data structures like lists and tuples, as well as \"for-\" and \"while-\" loops, are \"slow\", and in the case of big data analysis this becomes a problem.\n",
    "\n",
    "Probably the most clear explanation of \"slowness\" of standard data types and loops we can see in the book by Jake VanderPlas: [https://jakevdp.github.io/PythonDataScienceHandbook/](https://jakevdp.github.io/PythonDataScienceHandbook/) (chapter 2, the paragraph \"Understanding Data Types in Python\").\n",
    "\n",
    "The [NumPy](https://numpy.org/) library is designed to work with multidimensional arrays (ndarrays) in such a way that the execution time for large data operations is **significantly faster** (sometimes hundreds or even thousands of times) than using \"pure\" Python. The library contains a large number of fast and high-level operations with one-, two- and multidimensional arrays (tensors), as well as a number of vector and matrix algebra functions. All higher-level libraries in the Python ecosystem work on the basis of NumPy arrays (Pandas, Matplotlib, Scikit-Learn, Tensorflow deep learning libraries, PyTorch and many others), which makes studying the ideology of NumPy arrays and the capabilities of this library a \"must-have\" skill for a data analyst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760a876",
   "metadata": {},
   "source": [
    "## Pandas library\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is a Python library for loading, preprocessing, transforming and combining data as well as for exploratory data analysis. Exploratory analysis precedes directly the construction of predictive machine learning models and is designed to help the researcher better understand the features of the dataset, the relationship (correlation) between features, and also draw the first simple conclusions based on the data. However, \"simple\" does not mean \"bad\". These (at first glance) primitive conclusions provide baselines for subsequent more complex models, and it may turn out that it is the patterns found at the exploratory analysis stage that will help achieve the desired goal without diving into complex machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f42ff9",
   "metadata": {},
   "source": [
    "## Data visualization. Matplotlib and Seaborn libraries\n",
    "\n",
    "An important component of exploratory data analysis is data visualization. High-quality graphs and charts help you see more than boring and monotonous tables. The Pandas library has built-in visualization tools based on Matplotlib graphics. The [Matplotlib](https://matplotlib.org/) library itself provides many low-level graphical tools, so that the researcher can control literally everything --- from the color of points to fonts on the coordinate axes. The [Seaborn](https://seaborn.pydata.org/) library contains more high-level features and is intended to make life easier for Matplotlib users to some extent by automating many routine things. Typically, the built-in Pandas graphics, Matplotlib and Seaborn libraries are used together, which we will demonstrate later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e055a",
   "metadata": {},
   "source": [
    "## BI tools for data analysis\n",
    "\n",
    "Intense competition in global industries has made businesses seek out ways of managing their business processes for sustained growth. Organizations are seeking business intelligence (BI) solutions to achieve competitiveness through advanced data analysis (especially in the era of Big Data).\n",
    "\n",
    "These tools step up into collecting, analyzing, monitoring, and predicting future business scenarios by creating a clear perspective of all the data a company manages. Identifying trends, enabling self-service analytics, utilizing powerful visualizations, and offering professional **BI dashboards** are becoming the standard in business operations, strategic development, and ultimately, indispensable tools in increasing profit. And not just that, the self-service nature of these solutions gives access to every feature we just mentioned to all levels of users, without the need for any technical skills or specialized training. Making them the perfect solution to democratize the data analysis process and boost business performance.\n",
    "\n",
    "Here are some well-known BI tools: Microsoft Power BI, Tableau, SAS Business Intelligence, Zoho Analytics etc. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda6086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of useful libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(); # fancy graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27c6cd",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Pre-prepared and processed data is usually in a tabular format and stored as CSV files (as well as TSV, XLS, XLSX etc.). In this case, you should use the [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method. Data can also be loaded directly from tabular databases, and Pandas has the [read_sql](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html) method for this purpose. In other cases, [read_json](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html) and [other methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) can help.\n",
    "\n",
    "In this course, we are going to work not only with \"traditional\" tabular data, but also with text and images.\n",
    "\n",
    "The [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method has many configurable options. The most significant of them are: file name (or URL), type of cell separator (comma by default), presence of a header row (its number is indicated; by default, feature names are read from the first line of the file), presence of a column with row indices (identifiers) (a number is also specified; by default, --- is absent). For other options, please refer to the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb05e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/cardio_train.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c582490",
   "metadata": {},
   "source": [
    "## First look at the data\n",
    "\n",
    "The [head(n)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) method is designed to view the first **n** rows of the table (**n=5** by default). Similarly, the [tail(n)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tail.html) method returns the last **n** lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ae190",
   "metadata": {},
   "source": [
    "If there are too many features (columns), it could be useful to transpose the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a693d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d556b2",
   "metadata": {},
   "source": [
    "The [info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) method allows you to display general information about the dataset. We can find out the type of each feature, as well as whether there are missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4849a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8dd21",
   "metadata": {},
   "source": [
    "The [describe()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) method allows you to collect some statistics for each numerical feature. For easier reading, the resulting table could be transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b7404",
   "metadata": {},
   "source": [
    "Note that some of the features are binary (**smoke**, **alco**, **active**, **cardio**), so standard descriptive statistics --- mean, standard deviation, median, quartiles --- do not make sense for them. In this case, the simple calculation of values, [value_counts()](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html), will be more useful. For example, this way we can find out how many patients with identified cardiovascular disease (CVD) are in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cardio'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71001fe9",
   "metadata": {},
   "source": [
    "We see that we have an approximately equal number of healthy and sick people, that is, classes 0 and 1 are balanced (we will talk about the problem of unbalanced classes later).\n",
    "\n",
    "Finally, the **normalize** parameter allows you to find out the percentage of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cardio'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52598cea",
   "metadata": {},
   "source": [
    "## One-dimensional feature analysis\n",
    "\n",
    "Let's look at the distribution of patient height values. The theory says that the height is a variable that usually has a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['height'].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785159b0",
   "metadata": {},
   "source": [
    "The default chart turned out to be uninformative. Let's try to improve the situation by adding the **bins** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbd832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['height'].hist(bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c5d62",
   "metadata": {},
   "source": [
    "As expected, we have something similar to the normal distribution histogram. However, **outliers** --- values that \"stand out\" from the overall picture --- are not visible on it. Therefore, sometimes it is more useful to use **boxplot** (\"box and whisker diagram\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['height']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15a330",
   "metadata": {},
   "source": [
    "The width of the \"box\" is equal to interquartile range (IQR, the difference between third $Q_3$ and first $Q_1$ quartiles). The vertical line inside the box shows the median (second quartile). \"Whiskers\" limit the points that fall into the interval $[Q_1-1.5*IQR; Q_3+1.5*IQR]$. Finally, individual points on the graph correspond to outliers --- values that are not typical for this sample. As you can see, there were quite a few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4f8d4",
   "metadata": {},
   "source": [
    "## Two- and more-dimensional feature analysis\n",
    "\n",
    "For example, a researcher may be interested in the question: what is the average age of healthy and sick patients? The age attribute has an inconvenient unit of measurement --- days, so let's convert it to a number of years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2542fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = (df['age'] / 365).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15e4e1",
   "metadata": {},
   "source": [
    "Note that we're using a **method** here, not a standard round **function**. This greatly speeds up the calculations. The operation \"divide a column by a number\" works intuitively --- each element is divided by this number. NumPy magic in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e213b",
   "metadata": {},
   "source": [
    "### GROUP BY\n",
    "\n",
    "Attention: here we become familiar with one very useful operation --- grouping. The [groupby()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) method works similar to the GROUP BY operator in SQL and allows you to group data by one or more attributes, then calculate the aggregates in each group. The syntax is quite simple, concise and intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cardio')['age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e847b",
   "metadata": {},
   "source": [
    "Calculations show that the average age of people with CVD is slightly higher than that of healthy people. These calculations can also be visualized using Pandas' built-in graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b777cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cardio')['age'].mean().plot(kind='bar') \n",
    "plt.ylabel('Age')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e21ec0",
   "metadata": {},
   "source": [
    "### Countplot\n",
    "Now let's try to see how the number of healthy and sick patients is distributed by age groups. The [countplot](https://seaborn.pydata.org/generated/seaborn.countplot.html) of the Seaborn library will help us here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "sns.countplot(y='age', hue='cardio', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da9a80",
   "metadata": {},
   "source": [
    "An important observation --- starting from the age of 55, the number of sick patients exceeds the number of healthy ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284dbe5",
   "metadata": {},
   "source": [
    "### Scatter plot\n",
    "\n",
    "A useful type of plot for investigating pairs of numeric features is a scatter plot. Consider the age and height of patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd573c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['age'], df['height']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c3642",
   "metadata": {},
   "source": [
    "Here it becomes clear that our outliers in the data are simply input errors. Unless, of course, we did not conduct a study among Lilliputians :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea1575",
   "metadata": {},
   "source": [
    "To study the joint distribution of two numerical features, the jointplot of the Seaborn library can be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='height', y='weight', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ad6b7",
   "metadata": {},
   "source": [
    "Errors and anomalies in the data are clearly visible in this graph as well. It can also be concluded that, without taking into account outliers, height and weight have distributions that are close to normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf697b",
   "metadata": {},
   "source": [
    "### Pivot tables\n",
    "\n",
    "For the study of three or more features, pivot tables are useful tools. This tool is well known to advanced users of Excel spreadsheets, Google Spreadsheets. Consider how to use the pivot table to answer the questions:\n",
    "- is it true that with age people become more prone to drinking alcohol;\n",
    "- is it true that among smokers the percentage of CVD is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab514f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values - features by which the values of the aggfunc function are calculated\n",
    "# index - features by which grouping is performed\n",
    "df.pivot_table(values=['age', 'cardio'], index=['smoke', 'alco'], aggfunc='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e07ff8",
   "metadata": {},
   "source": [
    "As you can see, the answers to both questions are negative. Drinking habits do not appear to be correlated with age, and CVD rates are higher among non-smokers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561996f",
   "metadata": {},
   "source": [
    "To understand how drinking and smoking are related, let's look at the crosstab (contingency table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a671f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['smoke'], df['alco'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee55950",
   "metadata": {},
   "source": [
    "So far, we can only say that there are significantly more non-drinking and non-smoking patients than all the rest. For reasonable conclusions about the relationship, one should turn to numerical calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcbe04c",
   "metadata": {},
   "source": [
    "## Selecting data by condition. Indexing methods in Pandas\n",
    "\n",
    "Sometimes we need to perform calculations not on the entire training set, but on some part of it. To do this, you need to know and understand how to access cells in dataframes.\n",
    "\n",
    "Let's start with the study of one feature \"in itself\". Let's take **height** as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = df['height']\n",
    "type(h) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6686330",
   "metadata": {},
   "source": [
    "Thus, we see that the table (DataFrame) is a set of named columns (Series). Columns are accessed by the key --- column name, as in Python dictionaries. Technically, you can think of a dataframe as a dictionary of columns.\n",
    "\n",
    "But what about the rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9bcf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_patient = df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d56ea",
   "metadata": {},
   "source": [
    "Oops! We got an error: KeyError means there is no column named \"0\". That is, we cannot access the string through a regular index. To do this, we will need an \"implicit\" index (implicit loc, iloc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_patient = df.iloc[0]\n",
    "print(first_patient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43e2d0",
   "metadata": {},
   "source": [
    "Again, we see that technically a dataframe row is a dictionary. The keys of the dictionary are the names of the columns, the values are the values of the features for the given row.\n",
    "\n",
    "To find out, for example, the age of the first patient (without storing it in a separate variable), you need to apply explicit indexing (loc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0, 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b94b01",
   "metadata": {},
   "source": [
    "Let's return now to the variable **h**. Recall that we stored in it all the values from the **height** column. Height is in centimeters. Let's convert to meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_meters = h / 100 \n",
    "h_meters[:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de12e9c5",
   "metadata": {},
   "source": [
    "Above, in several charts, we saw that there are errors among the height values. Let's see how many patients are shorter than 125 cm. Attention, the question is --- how to solve this problem in the \"classical\" style?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0be4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "lilliputs = 0\n",
    "for value in h:\n",
    "    if value < 125:\n",
    "        lilliputs = lilliputs + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e425d79",
   "metadata": {},
   "source": [
    "Now let's solve the same problem in NumPy style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d66e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "h[h < 125].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350da5c",
   "metadata": {},
   "source": [
    "So, the second method turned out to be faster by about 5 times on a data set of 70,000 values (relatively small). As the length of the vector grows, loops become hundreds and thousands of times slower than vectorized NumPy operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f4873e",
   "metadata": {},
   "source": [
    "The selection from an array of values can be performed by a conditional index. It works similarly for selecting rows in a dataframe. Let's calculate the average age of smokers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e117b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['smoke'] == 1]['age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747adc5e",
   "metadata": {},
   "source": [
    "Condition also can be complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['smoke'] == 1) & (df['cardio'] == 1)]['age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f72571",
   "metadata": {},
   "source": [
    "## Dataframe filtering. Deleting rows and columns\n",
    "\n",
    "The [drop()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) method is used to remove rows and columns in a dataframe. Consider deleting by keys and by condition.\n",
    "\n",
    "Let's delete the target feature, **cardio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb93df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = df.drop('cardio', axis=1)\n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd0e8e",
   "metadata": {},
   "source": [
    "Now, let's delete the first 100 patients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = df.drop(np.arange(100), axis=0)\n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a3789f",
   "metadata": {},
   "source": [
    "And finally, we will remove all the patients with a height below 125 cm, as well as above 200 cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = df.drop(df[(df['height'] < 125) | (df['height'] > 200)].index)\n",
    "dummy_df.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa46da",
   "metadata": {},
   "source": [
    "As you can see, the percentage of outliers is small --- the remaining sample is 99.9% of the original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60137a3c",
   "metadata": {},
   "source": [
    "## Adding new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a50e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['height_cm'] = df['height'] / 100\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee292ca",
   "metadata": {},
   "source": [
    "## Encoding feature values\n",
    "\n",
    "Our dataset contains only numerical values, but often there are **categorical** features. In this case one of the encoding procedures must be applied at the preprocessing stage. The simplest type of encoding is the replacement of some values with others (**label encoding**). In this case, we will have to (solely for the purpose of demonstrating the operation of the method) apply the inverse operation. For example, we recode the feature \"cholesterol level\" according to the principle:\n",
    "\n",
    "- 1 --- \"low\"\n",
    "- 2 ---\"normal\"\n",
    "- 3 ---\"high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values = {1:'low', 2:'normal', 3:'high'}\n",
    "df['dummy_cholesterol'] = df['cholesterol'].map(new_values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3574a",
   "metadata": {},
   "source": [
    "Let's recode the target feature **cardio** into boolean (True/False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857de197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cardio'] = df['cardio'].astype(bool)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88deaaa4",
   "metadata": {},
   "source": [
    "# Задания для самостоятельной работы\n",
    "\n",
    "1. Определите количество мужчин и женщин среди испытуемых. Обратите внимание, что способ кодирования переменной gender мы не знаем. Воспользуемся медицинским фактом, а именно: мужчины в среднем выше женщин.\n",
    "\n",
    "2. Верно ли, что мужчины более склонны к употреблению алкоголя, чем женщины?\n",
    "\n",
    "3. Каково различие между процентами курящих мужчин и женщин?\n",
    "\n",
    "4. Какова разница между средними значениями возраста для курящих и некурящих?\n",
    "\n",
    "5. Создайте новый признак --- BMI (body mass index, индекс массы тела). Для этого разделите вес в килограммах на квадрат роста в метрах. Считается, что нормальные значения ИМТ составляют от 18.5 до 25. Выберите верные утверждения:\n",
    "\n",
    "    (a) Средний ИМТ находится в диапазоне нормальных значений ИМТ.\n",
    "\n",
    "    (b) ИМТ для женщин в среднем выше, чем для мужчин.\n",
    "\n",
    "    (c) У здоровых людей в среднем более высокий ИМТ, чем у людей с ССЗ.\n",
    "\n",
    "    (d) Для здоровых непьющих мужчин ИМТ ближе к норме, чем для здоровых непьющих женщин\n",
    "\n",
    "6. Удалите пациентов, у которых диастолическое давление выше систолического. Какой процент от общего количества пациентов они составляли?\n",
    "\n",
    "7. На сайте Европейского общества кардиологов представлена шкала [SCORE](https://www.escardio.org/static_file/Escardio/Subspecialty/EACPR/Documents/score-charts.pdf). Она используется для расчёта риска смерти от сердечно-сосудистых заболеваний в ближайшие 10 лет. \n",
    "\n",
    "    Рассмотрим верхний правый прямоугольник, который показывает подмножество курящих мужчин в возрасте от 60 до 65 лет (значения по вертикальной оси на рисунке представляют верхнюю границу).\n",
    "\n",
    "    Мы видим значение 9 в левом нижнем углу прямоугольника и 47 в правом верхнем углу. Это означает, что для людей этой возрастной группы с систолическим давлением менее 120 и низким уровнем холестерина риск сердечно-сосудистых заболеваний оценивается примерно в 5 раз ниже, чем для людей с давлением в интервале [160, 180] и высоким уровнем холестерина.\n",
    "\n",
    "    Вычислите аналогичное соотношение для наших данных.\n",
    "\n",
    "8. Визуализируйте распределение уровня холестерина для различных возрастных категорий.\n",
    "\n",
    "9. Как распределена переменная BMI? Есть ли выбросы \n",
    "\n",
    "10. Как соотносятся ИМТ и наличие ССЗ? Придумайте подходящую визуализацию.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
