{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b3a2b0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Task\n",
    "\n",
    "In this competition your task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal records recovered from the ship's damaged computer system.\n",
    "\n",
    "## Guide description\n",
    "\n",
    "In this notebook We will go step-by-step through my solution. This covers initial reading of the data, initial exploration, dealing with missing values, exploratory data analysis, feature engineering and putting all steps together with classification models in a compact pipeline. In the end we will take a look at ensemble techniques of stacking and voting classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3529db0",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e039aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7cd4b0",
   "metadata": {},
   "source": [
    "## Initial Data Exploration and preprocessing\n",
    "\n",
    "Initially when we read the data for the first time, we want to get to know the data. This could mean looking at the feature values, the ranges of these values, the data types, missing values, distributions of the values described by statictics and so on. The first functions you'll usually see are `head()`, `info()` and `describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ba9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c875684e",
   "metadata": {},
   "source": [
    "From the first look at the data, the data will require some processing. The machine learning models usually require numerical features. We could drop the non-numerical features but we would lose a lot of information. We will look at various techniques on how to deal with this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5622d29",
   "metadata": {},
   "source": [
    "The minimal `Age` of 0 seems suspicious. Other ranges of numerical values seem okay, but the distributions are heavily skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].sort_values().hist(bins=int(df['Age'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec0f23",
   "metadata": {},
   "source": [
    "By looking at the histogram of `Age` we can see that 0 is either used as missing value or there was a lot of infants onboard :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15cc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4f59f",
   "metadata": {},
   "source": [
    "We can see that some of the values are missing. Let's explore those first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4365b49",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f63c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.isna().any(axis=1)==True].shape[0], 'rows have atleast 1 value equal to NaN.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb69fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[df.isna().any(axis=1)==True].isna().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc844ff4",
   "metadata": {},
   "source": [
    "From the data description we know that passengers in cryosleep are confined to their cabins. Therefore their spendings on services (`RoomService`,`FoodCourt`, etc.) should equal to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['CryoSleep']==True].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30cf990",
   "metadata": {},
   "source": [
    "By confirming the hypothesis we can use this information to fill in the missing values in services of passengers in cryo sleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84464599",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "df_preprocess = df.copy()\n",
    "df_preprocess.loc[\n",
    "    (df_preprocess['CryoSleep']==True) &\n",
    "    (df_preprocess[service_cols].isna().any(axis=1))\n",
    "    ,service_cols]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de15e50",
   "metadata": {},
   "source": [
    "Additionally we can take a look at how passengers spend w.r.t. their age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73688bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in service_cols:\n",
    "    sns.scatterplot(df_preprocess, x='Age',y=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess.groupby('Age')[service_cols].sum().max(axis=1).loc[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751d038",
   "metadata": {},
   "source": [
    "We can see that passengers younger than 13 years old do not spend any money on services. Therefore we can fill missing values for service spendings for passengers younger than 13 years old with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61476b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess.loc[(df_preprocess['Age'] < 13) & (df_preprocess[service_cols].isna().any(axis=1)),service_cols] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b04228",
   "metadata": {},
   "source": [
    "### Summary - missing values\n",
    "\n",
    "* We have filled missing values where the value could be determined from the data.\n",
    "* For other values we have couple of options:\n",
    "    * Impute mean, mode, median of feature or a set constant .\n",
    "    * Use kNN to impute the value of nearest neighbors values.\n",
    "    * Learn a classification of regression model for imputing.\n",
    "* We will try the first 2 options later as we have to fit those only on the training part of the dataset to avoid data leakage.\n",
    "* By filling in the values determined by data we have reduced rows with atleast 1 missing value by around 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39caf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_preprocess[df_preprocess.isna().any(axis=1)==True].shape[0], 'rows have atleast 1 value equal to NaN.')\n",
    "sns.heatmap(df_preprocess[df_preprocess.isna().any(axis=1)==True].isna().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74fab0",
   "metadata": {},
   "source": [
    "## EDA and Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbafbd1c",
   "metadata": {},
   "source": [
    "Now that we have (sort of) dealt with missing values we can move further on EDA. In this section we will take a look at certain features w.r.t. our target feature `Transported`. From the insights found we will try to create new features that could help our future model in classification.\n",
    "____\n",
    "\n",
    "Sort of dealt with missing values - Not all missing values are filled right now, we are gonna use some of the mentioned techniques but for that we would have to further split our data to train and validation sets so that we do not leak any information and we are gonna do that as a last step before modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252fc8d8",
   "metadata": {},
   "source": [
    "Before we do any further exploration, we can create new features, feature `TotalSpend` summing all expenses of a passenger and feature `Spent` boolean feature indicating whether passenger spent any money. As we saw the services feature distributions are heavily skewed, where the richest spent a lot but majority of the people did not spend any money. We will also create feature transformed using logarithmic transformation `{service}_log'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess = (df_preprocess\n",
    "    .assign(\n",
    "        TotalSpend=lambda x: x[service_cols].sum(axis=1),\n",
    "        Spent=lambda x: (x['TotalSpend'] > 0).astype(int),\n",
    "        TotalSpend_log=lambda x: np.log(x['TotalSpend']+1),\n",
    "        **{f'{s}_log': lambda x: np.log(x[s]+1) for s in service_cols}\n",
    "    )\n",
    ")\n",
    "df_preprocess.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a7959",
   "metadata": {},
   "source": [
    "Now we will look at distributions of categorical features w.r.t. target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a06376",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_preprocess.select_dtypes(object).columns.tolist():\n",
    "    if df_preprocess[c].unique().size < 10:\n",
    "        sns.countplot(df_preprocess, x=c, hue='Transported')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0048ea4",
   "metadata": {},
   "source": [
    "We can do the same for the `Age` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_preprocess, x='Age', hue='Transported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4cb5c4",
   "metadata": {},
   "source": [
    "There is a pretty big difference in the distributions for passengers under the age of 13. As we saw earlier these passengers did not spend any money on board. Let's take a look at the distributions of categorical features w.r.t. target feature only for passengers of under the age of 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_preprocess.select_dtypes(object).columns.tolist():\n",
    "    if df_preprocess[df_preprocess['Age'] == 0.0][c].unique().size < 10:\n",
    "        sns.countplot(df_preprocess[(df_preprocess['Age'] < 13.0)], x=c, hue='Transported')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7775fb9",
   "metadata": {},
   "source": [
    "From the plots we can see that almost everyone under the age of 13 not coming from Earth got transported. We can create a feature for that. Firstly we will create a `AgeLimit` binary feature and then binary feature from underaged from Earth.\n",
    "\n",
    "We also see that passengers under that age limit are never VIP. We can take a look at if there are any missing values for VIP of underaged passengers and fill them if it is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527313bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess[df_preprocess['Age'] < 13 & df_preprocess['VIP'].isna()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess.loc[(df_preprocess['Age']) < 13 & (df_preprocess['VIP'].isna()),'VIP'] = False\n",
    "df_preprocess = (df_preprocess\n",
    "    .assign(\n",
    "        AgeLimit=lambda x: (x['Age']<13).astype(int),\n",
    "        UnderageEarth=lambda x: ((x['Age']< 13) & (x['HomePlanet'] == 'Earth')).astype(int)\n",
    "    )\n",
    ")\n",
    "df_preprocess.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab81ad6",
   "metadata": {},
   "source": [
    "Now we are gonna focus on feature extraction from given features. Feature `PassengerId` can be split into passenger group and passenger id within the group. Group ID could be a useful feature so we will add it. After that we will split the cabin feature into `Deck`, `Num` and `Side` Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cabin(cabin: str) -> pd.Series:\n",
    "    if not pd.isna(cabin):\n",
    "        deck, num, side = cabin.split('/')\n",
    "    else:\n",
    "        deck, num, side = np.NaN, np.NaN, np.NaN\n",
    "    return pd.Series({'Deck':deck, 'Num':float(num), 'Side':side})\n",
    "\n",
    "def process_passengerid(passengerid: str) -> int:\n",
    "    group, _ = passengerid.split('_')\n",
    "    return int(group)\n",
    "\n",
    "df_preprocess.loc[:,['Deck','Num','Side']] = df_preprocess['Cabin'].apply(process_cabin)\n",
    "df_preprocess.loc[:,'Side'] = df_preprocess['Side'].replace({'P':0.0, 'S':1.0})\n",
    "df_preprocess.loc[:,'Group'] = df_preprocess['PassengerId'].apply(process_passengerid)\n",
    "df_preprocess.loc[:,'GroupLen'] = df_preprocess.groupby('Group').transform('size')\n",
    "df_preprocess.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d11f91",
   "metadata": {},
   "source": [
    "For this guide I personally will skip feature engineering on the feature `Name`. One could split the feature into a First and Last name and then possibly used Label Encoder to map the feature values to numbers for additional information.\n",
    "\n",
    "Additionally before moving to the next step, We will drop redundant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess = df_preprocess.drop(columns=['Cabin','PassengerId','Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1369f8",
   "metadata": {},
   "source": [
    "## Modelling and Pipeline composition\n",
    "\n",
    "In this section we will move to modelling. Now that we have done majority of the EDA we will convert all the preprocessing code into functions and classes. To make the code more readable and maintainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd9f083",
   "metadata": {},
   "source": [
    "Some of the code is applicable to the data before imputing and some after so we will split our pipeline into few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd77c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PreprocessBeforeImputing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, service_cols) -> None:\n",
    "        super().__init__()\n",
    "        self.service_cols = service_cols\n",
    "\n",
    "    def fit(self, X:pd.DataFrame, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X:pd.DataFrame, y=None)->pd.DataFrame:\n",
    "        X = X.copy()\n",
    "\n",
    "        # Impute values based on data relationships\n",
    "        X.loc[(X['CryoSleep']==True) & (X[self.service_cols].isna().any(axis=1)),self.service_cols]=0\n",
    "        X.loc[(X['Age'] < 13) & (X[self.service_cols].isna().any(axis=1)),self.service_cols]=0\n",
    "        X.loc[(X['Age']) < 13 & (X['VIP'].isna()),'VIP'] = False\n",
    "\n",
    "        # Extract features from PassengerId\n",
    "        X.loc[:,'Group'] = X['PassengerId'].apply(self.process_passengerid)\n",
    "        X.loc[:,'GroupLen'] = X.groupby('Group').transform('size')\n",
    "        X = X.drop(columns=['PassengerId','Name'])\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def process_passengerid(self, passengerid: str) -> int:\n",
    "        group, _ = passengerid.split('_')\n",
    "        return int(group)\n",
    "        \n",
    "\n",
    "class PreprocessAfterImputing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, service_cols) -> None:\n",
    "        super().__init__()\n",
    "        self.service_cols = service_cols\n",
    "    \n",
    "    def fit(self, X:pd.DataFrame, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X:pd.DataFrame, y=None) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "\n",
    "        # Create features based on Age\n",
    "        X.loc[:,'AgeLimit'] = (X['Age']<13).astype(int)\n",
    "        X.loc[:,'UnderageEarth'] = ((X['Age']< 13) & (X['HomePlanet'] == 'Earth')).astype(int)\n",
    "        \n",
    "        # Feature extraction from Cabin\n",
    "        X.loc[:,['Deck','Num','Side']] = X['Cabin'].apply(self.process_cabin)\n",
    "        X = X.drop(columns=['Cabin'])\n",
    "        X['Side'] = X['Side'].replace({'P':0.0, 'S':1.0})\n",
    "        X['Num'] = X['Num'].astype(int)\n",
    "        \n",
    "        # Features from services\n",
    "        X['TotalSpend'] = X[self.service_cols].sum(axis=1)\n",
    "        X['Spent'] = (X['TotalSpend'] > 0).astype(float)\n",
    "        for col in self.service_cols:\n",
    "            X[f'{col}_log']=np.log(X[col]+1)\n",
    "        X['TotalSpend_log'] = np.log(X['TotalSpend']+1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def process_cabin(self, cabin: str) -> pd.Series:\n",
    "        if not pd.isna(cabin):\n",
    "            deck, num, side = cabin.split('/')\n",
    "        else:\n",
    "            deck, num, side = np.NaN, np.NaN, np.NaN\n",
    "        return pd.Series({'Deck':deck, 'Num':float(num), 'Side':side})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c3811",
   "metadata": {},
   "source": [
    "Now let's start putting all together.\n",
    "\n",
    "Firstly import all everything needed to build the pipeline and `set_config(transform_output='pandas')` so that the output of transformers is pd.Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c98a76",
   "metadata": {},
   "source": [
    "Read again data and seperate data from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Transported'\n",
    "\n",
    "# Read data\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Seperate training data and Label\n",
    "X_train = df_train.drop(columns=[target])\n",
    "y_train = df_train[target].astype(float)\n",
    "X_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c5748",
   "metadata": {},
   "source": [
    "Next, we will create ColumnTransformer that will impute all the missing values, it consists of an Imputer for numerical features and imputer for categorical features. After that we will create OneHotEncoder transformer that will encode all categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute = ColumnTransformer([\n",
    "    ('numimp',KNNImputer(),make_column_selector(dtype_include=np.number)),\n",
    "    ('catimp',SimpleImputer(strategy='most_frequent'),make_column_selector(dtype_include=object)),\n",
    "    ], verbose_feature_names_out=False)\n",
    "\n",
    "ohe = ColumnTransformer([(\n",
    "        'ohe',\n",
    "        OneHotEncoder(sparse_output=False ,drop='if_binary', handle_unknown='infrequent_if_exist'),\n",
    "        make_column_selector(dtype_include=object)\n",
    "    )], remainder='passthrough', verbose_feature_names_out=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d4261",
   "metadata": {},
   "source": [
    "Now, last step, putting together the entire pipeline. We will use RandomForestClassifier as the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aac9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_cols = [\n",
    "    'RoomService',\n",
    "    'FoodCourt',\n",
    "    'ShoppingMall',\n",
    "    'Spa',\n",
    "    'VRDeck'\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('bp', PreprocessBeforeImputing(service_cols=service_cols)),\n",
    "    ('imp', impute),\n",
    "    ('ap', PreprocessAfterImputing(service_cols=service_cols)),\n",
    "    ('ohe', ohe),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197427b",
   "metadata": {},
   "source": [
    "One of the advantages that the sklearn's pipelines offer is the ability to optimize hyperparameters over the entire pipeline. In next step, we will create a hyperparameter grid. We will try to find the best hyperparameters for chosen classifier and the imputing techniques. Another advantage of the pipeline is that we do not have to worry about data leakages. Even in the cross validation setting all the steps of the pipeline will be fitted only on the training folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'clf__n_estimators': [100],\n",
    "    'clf__max_depth': list(range(3,17)),\n",
    "    'clf__min_samples_split': [2**i for i in range(1,7)],\n",
    "    'clf__min_samples_leaf': [2**i for i in range(0,7)],\n",
    "    'imp__numimp':[SimpleImputer(strategy='mean'), SimpleImputer(strategy='median'), KNNImputer()],\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "est = RandomizedSearchCV(pipe, param_grid, scoring='accuracy', cv=kf, n_iter=100, n_jobs=8, verbose=1)\n",
    "est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455e168",
   "metadata": {},
   "source": [
    "Let's see how the model performed with different imputing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(est.cv_results_)\n",
    "res['imp'] = [str(i) for i in res['param_imp__numimp'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(res,x='mean_test_score', y='imp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0248de57",
   "metadata": {},
   "source": [
    "Based on the previous results we will use SimpleImputer with `strategy=mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de05662",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute = ColumnTransformer([\n",
    "    ('numimp',SimpleImputer(strategy='mean'),make_column_selector(dtype_include=np.number)),\n",
    "    ('catimp',SimpleImputer(strategy='most_frequent'),make_column_selector(dtype_include=object)),\n",
    "    ], verbose_feature_names_out=False)\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('bp', PreprocessBeforeImputing(service_cols=service_cols)),\n",
    "    ('imp', impute),\n",
    "    ('ap', PreprocessAfterImputing(service_cols=service_cols)),\n",
    "    ('ohe', ohe),\n",
    "])\n",
    "\n",
    "X_train = pipe.fit_transform(X_train, y_train)\n",
    "X_test = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19ff50",
   "metadata": {},
   "source": [
    "Now, that we have a pipeline for data processing we can fully move to modelling and predictions. We will use RandomForestClassifier and various boosted trees algorithms. We will be training many instances of the algorithms and then see the performance of the best instance on test data and then we will try creating stacking and voting classifier ensembles to see whether this will perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06266a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "def optimize_model(model, X_train, y_train, *, param_grid, n_iter):\n",
    "    kf = KFold()\n",
    "    est = RandomizedSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            scoring='accuracy',\n",
    "            cv=kf,\n",
    "            n_iter=n_iter,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "    )\n",
    "    est.fit(X_train, y_train)\n",
    "    print(model)\n",
    "    print(est.best_score_)\n",
    "    return est\n",
    "\n",
    "def make_predictions_and_write_csv(model, X_test, df_test, filename):\n",
    "    y = model.predict(X_test)\n",
    "    pd.concat(\n",
    "        [df_test['PassengerId'],pd.Series(y,name='Transported',dtype=bool)],\n",
    "        axis=1\n",
    "    ).to_csv(f'{filename}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': list(range(3,17)),\n",
    "    'min_samples_split': [2**i for i in range(1,7)],\n",
    "    'min_samples_leaf': [2**i for i in range(0,7)],\n",
    "}\n",
    "\n",
    "gb_pg = {\n",
    "    'n_estimators' :[100, 200],\n",
    "    'max_depth' : list(range(3,17)),\n",
    "    'max_leaves' : [2**i for i in range(4,8)],\n",
    "    'learning_rate':[0.1],\n",
    "}\n",
    "\n",
    "lgb_pg = {\n",
    "    'n_estimators' :[100, 200],\n",
    "    'max_depth' : list(range(3,20)),\n",
    "    'num_leaves' : [2**i for i in range(4,10)],\n",
    "    'learning_rate':[0.1],\n",
    "    'min_child_samples' : [2**i for i in range(2,7)],\n",
    "}\n",
    "\n",
    "g_pg = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': list(range(3,17)),\n",
    "    'min_samples_split': [2**i for i in range(1,7)],\n",
    "    'min_samples_leaf': [2**i for i in range(0,7)],\n",
    "}\n",
    "\n",
    "hist_pg = {\n",
    "    'learning_rate':[0.1],\n",
    "    'max_iter':[100, 200],\n",
    "    'max_leaf_nodes':[2**i for i in range(4,10)],\n",
    "    'max_depth':list(range(3,20)),\n",
    "    'min_samples_leaf':[2**i for i in range(2,7)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_jobs=1),\n",
    "    xgb.XGBClassifier(n_jobs=1,),\n",
    "    lgb.LGBMClassifier(n_jobs=1, objective='binary'),\n",
    "    GradientBoostingClassifier(),\n",
    "    HistGradientBoostingClassifier(),\n",
    "]\n",
    "pgs = [\n",
    "    pg,\n",
    "    gb_pg,\n",
    "    lgb_pg,\n",
    "    g_pg,\n",
    "    hist_pg\n",
    "]\n",
    "\n",
    "counts = [3,2,5,2,5]\n",
    "abbrs = ['rf','xgb','lgb','gbc','hgbc']\n",
    "n_iters = [50, 35, 50, 35, 50]\n",
    "\n",
    "ests = []\n",
    "for i, (model, pg) in enumerate(zip(models, pgs)):\n",
    "    for j in range(counts[i]):\n",
    "        est = optimize_model(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            param_grid=pg,\n",
    "            n_iter=n_iters[i]\n",
    "        )\n",
    "        est = est.best_estimator_\n",
    "        ests.append((f'{abbrs[i]}_{j}', est))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d0a6d",
   "metadata": {},
   "source": [
    "## Final Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fabf15",
   "metadata": {},
   "source": [
    "Best estimator was: LGBMClassifier\n",
    "\n",
    "With LeaderBoard score of 0.80102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44388b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions_and_write_csv(ests[5][1], X_test, df_test, 'preds_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d53c87",
   "metadata": {},
   "source": [
    "Stacking classifier achieved LeaderBoard score of 0.7912\n",
    "\n",
    "Voting classifier achieved LeaderBoard score of 0.80289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1671e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "\n",
    "sc = StackingClassifier(\n",
    "    ests,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1)\n",
    "sc.fit(X_train, y_train)\n",
    "make_predictions_and_write_csv(sc, X_test, df_test, 'preds_sc')\n",
    "\n",
    "vc = VotingClassifier(\n",
    "    ests,\n",
    "    voting='hard',\n",
    "    n_jobs=-1)\n",
    "vc.fit(X_train, y_train)\n",
    "make_predictions_and_write_csv(vc, X_test, df_test, 'preds_vc')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
