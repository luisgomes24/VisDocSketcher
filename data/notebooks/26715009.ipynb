{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fdec47",
   "metadata": {},
   "source": [
    "lunana  \n",
    "last update 2022 05 05  \n",
    "ゆっくりしていってね"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08dcc0",
   "metadata": {},
   "source": [
    "# Data list  \n",
    "* [**train.csv**](#train.csv)  \n",
    "* [**train_image**](#train_image)  \n",
    "* [**test.csv**](#test.csv)  \n",
    "* [**sample_submission.csv**](#sample_submission.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df203a03",
   "metadata": {},
   "source": [
    "**霊夢:今日は画像のコンペだね。  \n",
    "魔理沙:まずは概略を見るぞ。**\n",
    "\n",
    "**Reimu: Today is an image competition.  \n",
    "Marisa: First, let's take a look at the outline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6edde",
   "metadata": {},
   "source": [
    "2019年には、世界中で推定500万人が胃腸管の癌と診断されました。これらの患者のうち、約半数が放射線療法の対象であり、通常、1日10〜15分かけて1〜6週間投与されます。放射線腫瘍医は、胃や腸を避けながら、腫瘍に向けられたX線ビームを使用して高線量の放射線を照射しようとします。統合磁気共鳴画像法やMR-Linacsとしても知られる線形加速器システムなどの新しい技術により、腫瘍学者は腫瘍と腸の日々の位置を視覚化することができます。これは日々変化する可能性があります。これらのスキャンでは、放射線腫瘍医は、X線ビームの方向を調整して腫瘍への線量送達を増やし、胃と腸を避けるために、胃と腸の位置を手動で輪郭を描く必要があります。これは時間と労力を要するプロセスであり、治療を1日15分から1時間に延長できます。これは、ディープラーニングがセグメンテーションプロセスの自動化に役立つ場合を除いて、患者が耐えるのが難しい場合があります。胃と腸をセグメント化する方法は、治療をはるかに速くし、より多くの患者がより効果的な治療を受けることを可能にします。\n",
    "\n",
    "UW-マディソンカーボンがんセンターは、MR-Linacベースの放射線治療のパイオニアであり、2015年以来、毎日の解剖学的構造に基づいてMRIガイド放射線治療で患者を治療してきました。UW-マディソンは、治療を受けた患者の匿名MRIを提供するこのプロジェクトを支援することに寛大に同意しました。 UW-マディソンカーボンがんセンターで。ウィスコンシン大学マディソン校は、ウィスコンシン州マディソンにある公立のランドグラント研究大学です。ウィスコンシンのアイデアは、州、国、そして世界に対する大学の誓約であり、彼らの努力はすべての市民に利益をもたらすでしょう。\n",
    "\n",
    "このコンテストでは、MRIスキャンで胃と腸を自動的にセグメント化するモデルを作成します。MRIスキャンは、放射線治療中の別々の日に1〜5回のMRIスキャンを受けた実際の癌患者からのものです。これらのスキャンのデータセットに基づいてアルゴリズムを作成し、がん患者がより良いケアを受けるのに役立つ創造的な深層学習ソリューションを考え出します。  \n",
    "\n",
    "In 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks. Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, oncologists are able to visualize the daily position of the tumor and intestines, which can vary day to day. In these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines. This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate—unless deep learning could help automate the segmentation process. A method to segment the stomach and intestines would make treatments much faster and would allow more patients to get more effective treatment.\n",
    "\n",
    "The UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015. UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin. The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens.\n",
    "\n",
    "In this competition, you’ll create a model to automatically segment the stomach and intestines on MRI scans. The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment. You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae825f",
   "metadata": {},
   "source": [
    "**霊夢:Sartoriusコンペとそっくりだな。**  　\n",
    "\n",
    "**Reimu: It's just like the Sartorius competition.**\n",
    "\n",
    "https://www.kaggle.com/competitions/sartorius-cell-instance-segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4955ac",
   "metadata": {},
   "source": [
    "# train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f05796",
   "metadata": {},
   "source": [
    "**霊夢:次はtrainデータを見てみよう**  \n",
    "\n",
    "**Reimu: Let's look at the train data next**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edefe5ce",
   "metadata": {},
   "source": [
    "train.csv - IDs and masks for all training objects.train - a folder of case/day folders, each containing slice images for a particular case on a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import os\n",
    "from os import listdir\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9eff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87367ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c498b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[194]['segmentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f13fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[194]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2118db",
   "metadata": {},
   "source": [
    "# train_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f24ba0",
   "metadata": {},
   "source": [
    "**霊夢:train画像を見てみよう**\n",
    "\n",
    "**Reimu: Let's take a look at the train image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99812589",
   "metadata": {},
   "source": [
    "train - a folder of case/day folders, each containing slice images for a particular case on a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "im=Image.open('../input/uw-madison-gi-tract-image-segmentation/train/case114/case114_day0/scans/slice_0006_360_310_1.50_1.50.png')\n",
    "im_list = np.asarray(im)\n",
    "plt.imshow(im_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "im=Image.open('../input/uw-madison-gi-tract-image-segmentation/train/case114/case114_day0/scans/slice_0023_360_310_1.50_1.50.png')\n",
    "im_list = np.asarray(im)\n",
    "plt.imshow(im_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ffb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "im=Image.open('../input/uw-madison-gi-tract-image-segmentation/train/case123/case123_day20/scans/slice_0065_266_266_1.50_1.50.png')\n",
    "im_list = np.asarray(im)\n",
    "plt.imshow(im_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b5984",
   "metadata": {},
   "source": [
    "**霊夢:画像に色付けをしてどこが大腸か小腸かわかるようにするよ。  \n",
    "魔理沙:Awsaf氏のコードを参考にするよ。**  \n",
    "\n",
    "**Reimu: I'll color the image so you can see where the large intestine or the small intestine is.  \n",
    "Marisa: I'll refer to Mr. Awsaf's code.**\n",
    "\n",
    "https://www.kaggle.com/code/awsaf49/uwmgi-mask-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecdf541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = np.asarray(mask_rle.split(), dtype=int)\n",
    "    starts = s[0::2] - 1\n",
    "    lengths = s[1::2]\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape) \n",
    "\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e332679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from matplotlib.patches import Rectangle\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(row):\n",
    "    data = row['id'].split('_')\n",
    "    case = int(data[0].replace('case',''))\n",
    "    day = int(data[1].replace('day',''))\n",
    "    slice_ = int(data[-1])\n",
    "    row['case'] = case\n",
    "    row['day'] = day\n",
    "    row['slice'] = slice_\n",
    "    return row\n",
    "\n",
    "def path2info(row):\n",
    "    path = row['image_path']\n",
    "    data = path.split('/')\n",
    "    slice_ = int(data[-1].split('_')[1])\n",
    "    case = int(data[-3].split('_')[0].replace('case',''))\n",
    "    day = int(data[-3].split('_')[1].replace('day',''))\n",
    "    width = int(data[-1].split('_')[2])\n",
    "    height = int(data[-1].split('_')[3])\n",
    "    row['height'] = height\n",
    "    row['width'] = width\n",
    "    row['case'] = case\n",
    "    row['day'] = day\n",
    "    row['slice'] = slice_\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2mask(id_):\n",
    "    idf = df_train[df_train['id']==id_]\n",
    "    wh = idf[['height','width']].iloc[0]\n",
    "    shape = (wh.height, wh.width, 3)\n",
    "    mask = np.zeros(shape, dtype=np.uint8)\n",
    "    for i, class_ in enumerate(['large_bowel', 'small_bowel', 'stomach']):\n",
    "        cdf = idf[idf['class']==class_]\n",
    "        rle = cdf.segmentation.squeeze()\n",
    "        if len(cdf) and not pd.isna(rle):\n",
    "            mask[..., i] = rle_decode(rle, shape[:2])\n",
    "    return mask\n",
    "\n",
    "def rgb2gray(mask):\n",
    "    pad_mask = np.pad(mask, pad_width=[(0,0),(0,0),(1,0)])\n",
    "    gray_mask = pad_mask.argmax(-1)\n",
    "    return gray_mask\n",
    "\n",
    "def gray2rgb(mask):\n",
    "    rgb_mask = tf.keras.utils.to_categorical(mask, num_classes=4)\n",
    "    return rgb_mask[..., 1:].astype(mask.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    img = img.astype('float32') \n",
    "    img = (img - img.min())/(img.max() - img.min())*255.0 \n",
    "    img = img.astype('uint8')\n",
    "    return img\n",
    "\n",
    "def show_img(img, mask=None):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    img = clahe.apply(img)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img, cmap='bone')\n",
    "    \n",
    "    if mask is not None:\n",
    "        plt.imshow(mask, alpha=0.5)\n",
    "        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n",
    "        labels = [ \"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n",
    "        plt.legend(handles,labels)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f12153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.progress_apply(get_metadata, axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5dff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob('/kaggle/input/uw-madison-gi-tract-image-segmentation/train/*/*/*/*')\n",
    "path_df = pd.DataFrame(paths, columns=['image_path'])\n",
    "path_df = path_df.progress_apply(path2info, axis=1)\n",
    "df_train = df_train.merge(path_df, on=['case','day','slice'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046cc27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row=1; col=4\n",
    "plt.figure(figsize=(5*col,5*row))\n",
    "for i, id_ in enumerate(df_train[~df_train.segmentation.isna()].sample(frac=1.0)['id'].unique()[:row*col]):\n",
    "    print(id_)\n",
    "    img = load_img(df_train[df_train['id']==id_].image_path.iloc[0])\n",
    "    mask = id2mask(id_)*255\n",
    "    show_img(img, mask=mask)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6edaf2",
   "metadata": {},
   "source": [
    "**魔理沙:縦に4つ表示してるけど、表示できたからまあ良し。**  \n",
    "\n",
    "**Marisa: I'm displaying four vertically, but it's okay because I was able to display them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e6b8e",
   "metadata": {},
   "source": [
    "# sample_submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237ab20",
   "metadata": {},
   "source": [
    "**魔理沙:最後にsample_submissionを見てみよう**  \n",
    "\n",
    "**Marisa: Finally, let's take a look at sample_submission**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795d5be",
   "metadata": {},
   "source": [
    "sample_submission.csv - a sample submission file in the correct format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa9f32",
   "metadata": {},
   "source": [
    "提出ファイル  \n",
    "送信ファイルのサイズを減らすために、メトリックはピクセル値にランレングスエンコーディングを使用します。セグメンテーションのインデックスの完全なリストを送信する代わりに、開始位置と実行長を含む値のペアを送信します。たとえば、「1 3」は、ピクセル1から開始し、合計3ピクセル（1,2,3）を実行することを意味します。  \n",
    "\n",
    "エンコード時には、マスクはバイナリである必要があることに注意してください。つまり、画像内のすべてのオブジェクトのマスクが1つの大きなマスクに結合されます。値0はマスクされていないピクセルを示し、値1はマスクされているピクセルを示します。  \n",
    "\n",
    "競技形式では、スペースで区切られたペアのリストが必要です。たとえば、「1 3 10 5」は、ピクセル1、2、3、10、11、12、13、14がマスクに含まれることを意味します。メトリックは、ペアがソートされ、正であり、デコードされたピクセル値が複製されていないことを確認します。ピクセルには上から下、次に左から右に番号が付けられます。1はピクセル（1,1）、2はピクセル（2,1）などです。  \n",
    "\n",
    "ファイルにはヘッダーが含まれ、次の形式である必要があります。  \n",
    "\n",
    "id、class、predicted  \n",
    "1、large_bowel、1 1 5 1  \n",
    "1、small_bowel、1 1  \n",
    "1、stomach、1 1  \n",
    "2、large_bowel、1 5217 \n",
    "など。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumple_submission=pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\n",
    "sumple_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131aafd",
   "metadata": {},
   "source": [
    "**霊夢:今回はここまでです。**\n",
    "\n",
    "**Reimu: That's all for this time.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
