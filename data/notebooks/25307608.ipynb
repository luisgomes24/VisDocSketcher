{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7798ce",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:120%;text-align:center;border-radius:10px 10px;\">Kmeans clustering from scratch</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdfc927",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:120%;text-align:center;border-radius:10px 10px;\">Model code</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b29441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans:\n",
    "    \"\"\"\n",
    "    One of the Clustering methods part of unsupervised models.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clucters, iteration = 500, eval_metric = \"inertia\", n_kmenas=100):\n",
    "        \"\"\"\n",
    "        :param n_cluster: number of clusters should use.\n",
    "        :param iteration: number of time the kmeans should work on the given data.\n",
    "        :param eval_metric: Evaluation metric to qualify the clusters. Can be inertia, dann_index, cost_fun.\n",
    "        :param n_kmens: run the kmeans this much time to get the best clusters.\n",
    "        \"\"\"\n",
    "        self.c = None # Centroid points value\n",
    "        self.m = None # number of samples\n",
    "        self.n = None # number of features\n",
    "        self.n_clusters = n_clucters\n",
    "        self.max_iteration = iteration\n",
    "        self.eval_metrix = eval_metric\n",
    "        self.n_kmeans = n_kmenas\n",
    "        \n",
    "    \n",
    "    def _init_clusters(self, X):\n",
    "        \"\"\" Intializa the centroids at the very beginning randomly\"\"\"\n",
    "        centroid_points = np.random.choice(range(0,self.m), self.n_clusters, replace=False) # replace= False won't choise the same 2 centroids\n",
    "        centroids = X[centroid_points].astype(np.float)\n",
    "        return centroids\n",
    "\n",
    "\n",
    "    def _euclidean_distance(self, x, y):\n",
    "        \"\"\"\n",
    "        Find the euclidean distance of two data points.\n",
    "\n",
    "        :param x: this a array which specifies the x data point. (2 or more)\n",
    "        :param y: this a array which specifies the y data point. (2 or more)\n",
    "        \"\"\"\n",
    "        return np.linalg.norm(x - y) #default norm parameter is 2 and L2 norm is also called euclidean distance.\n",
    "\n",
    "\n",
    "    def _find_centroid(self, x, centroids):\n",
    "        \"\"\" Find the closest centroid for given sample x\"\"\"\n",
    "        distance = []\n",
    "        for centroid in centroids:\n",
    "            distance.append(self._euclidean_distance(x, centroid))\n",
    "        return np.argmin(np.array(distance)) # return the index of smaller distance centroid (0 - k-1)\n",
    "\n",
    "\n",
    "    def _update_centroid(self, samples_inthat_cluster):\n",
    "        \"\"\" Update the centroid based ont the average value of each feature\"\"\"\n",
    "        centroid = []\n",
    "        for f in range(self.n):\n",
    "            centroid.append(np.mean(samples_inthat_cluster[:, f]))\n",
    "        return centroid\n",
    "\n",
    "    def _find_inertia(self, X, centroids, x_centroids):\n",
    "        \"\"\" FInd inertia of the clusters \"\"\"\n",
    "        # this can be find sum(all samples  distance to it's centroid)\n",
    "        distance = []\n",
    "        for index in range(0, self.m):\n",
    "            sample, centroid = X[index], centroids[x_centroids[index]]\n",
    "            dis = self._euclidean_distance(sample, centroid)\n",
    "            distance.append(dis)\n",
    "        \n",
    "        return np.sum(np.array(distance))\n",
    "\n",
    "    def _evalute(self, X, centroids, x_centroids, eval_metix):\n",
    "        \"\"\" Evaluate how well the model did clustering \"\"\"\n",
    "        if eval_metix == \"inertia\":\n",
    "            value = self._find_inertia(X, centroids, x_centroids)\n",
    "        return value\n",
    "\n",
    "\n",
    "    def single_train(self, X):\n",
    "        \"\"\"\n",
    "        Method for training the data --split the data into clusters.\n",
    "\n",
    "        :param X: train data.\n",
    "                \n",
    "        Steps:\n",
    "        * Randomly choice #n_clucters datapoint as centroids.\n",
    "        * For each data find the distance between that data and every centroid -- assign the sample to one of the centroids which has low dis.\n",
    "        * update the centroids -- by the mean of distance of the samples and that centroid.\n",
    "        * repeat 2 & 3 for \n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        self.m = X.shape[0]\n",
    "        self.n = X.shape[1]\n",
    "        # initialize the centroids.\n",
    "        centroids = self._init_clusters(X)\n",
    "        #print(centroids)\n",
    "        \n",
    "        # run it for max iteration or until it converges.\n",
    "        for _ in range(self.max_iteration):\n",
    "            # save the current centroid for coniditon check.\n",
    "            previous_centroid = centroids.copy()\n",
    "            # find the centroids each samples belong to.\n",
    "            x_centroids = [self._find_centroid(sample, centroids) for sample in X]\n",
    "            #print(x_centroids)\n",
    "\n",
    "            # update the centroid\n",
    "            for i in range(0, self.n_clusters):\n",
    "                samples_inthat_cluster = X[np.where(np.array(x_centroids) == i)[0]]\n",
    "                #print(samples_inthat_cluster)\n",
    "                centroids[i] = self._update_centroid(samples_inthat_cluster)\n",
    "                #print(centroids)\n",
    "            # if no cluster centroids has changed --> means it converged.\n",
    "            #print(\"current centroid\",centroids,\"previou centroid\", previous_centroid)\n",
    "            diff = centroids - previous_centroid\n",
    "            if not diff.any():\n",
    "                break\n",
    "        \n",
    "        # find the inertia.\n",
    "        inertia = self._evalute(X, centroids, x_centroids, self.eval_metrix)\n",
    "        return inertia, centroids, x_centroids\n",
    "\n",
    "    def train(self, X):\n",
    "        \"\"\"Train the X and find the clusters\"\"\"\n",
    "        global_cost_func = 999999\n",
    "        global_centroid = None\n",
    "        global_x_centroids = None\n",
    "        # The randome initalization is worse some times -- final cluster output depend on the initialization -- so the initialization of the cluster may change the clusters\n",
    "        # so we need to do kmeans for multiple times and take the cluster with minimum inertia (diff evaluation metrics can be used here we are using this)\n",
    "        for km in range(self.n_kmeans):\n",
    "            print(\"{:=^100}\".format(\" Training new kmeans \" ), end=\"---->\")\n",
    "            print(km)\n",
    "            # fit the kmeans model and get the cost function value and centroids.\n",
    "            cost_fun, centroids, x_centroids = self.single_train(X)\n",
    "            # if the cost function is low than update the centroids and x centroid points.\n",
    "            print(\"Evaluation metric of this kmeans --->\",cost_fun)\n",
    "            if cost_fun < global_cost_func:\n",
    "                global_cost_func = cost_fun\n",
    "                global_centroid = centroids\n",
    "                global_x_centroids = x_centroids\n",
    "\n",
    "        self.c = global_centroid\n",
    "        \n",
    "        return global_x_centroids\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0759b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1,2], [3,4] ,[6,6], [7,8], [2,3], [4,2], [8,8] , [9,6]])\n",
    "data = pd.DataFrame(data)\n",
    "kmeans = Kmeans(n_clucters=2, n_kmenas=10)\n",
    "kmeans.train(X=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2019a3",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:120%;text-align:center;border-radius:10px 10px;\">Load data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52701e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# don't need y here \n",
    "X, y = make_blobs(n_samples=50, centers=3, n_features=2,\n",
    "                   random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f1291",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:center;border-radius:10px 10px;\">Find cluster using sklearn k-means</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00095c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as kMeans_sklearn\n",
    "kmean_sklearn = kMeans_sklearn(n_clusters=3)\n",
    "cluter = kmean_sklearn.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d5a12",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:center;border-radius:10px 10px;\">Plot the clusters</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)\n",
    "# plot the dataset\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.scatter(x=X[:, 0], y=X[:, 1], c=cluter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945e50c",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:center;border-radius:10px 10px;\">Kmeans based on my own model :) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = Kmeans(n_clucters=3, n_kmenas=10)\n",
    "y_pred = kmeans.train(X)\n",
    "\n",
    "#plot the cluster based on my own Kmeans model\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.scatter(x=X[:, 0], y=X[:, 1], c=y_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047d6cd",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:center;border-radius:10px 10px;\">Both are 99% same :) :) -- My model is clustering well </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45adc7",
   "metadata": {},
   "source": [
    "**<span style=\"color:#5811D3;\"> If you like this Notebook -- please do upvote</span>**\n",
    "\n",
    "**<span style=\"color:#5811D3;\">If you have any questions -- Comment it</span>**\n",
    "\n",
    "\n",
    "<img src='https://c.tenor.com/npsLhz89w5AAAAAC/yes-i-did-it-success.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96d41e",
   "metadata": {},
   "source": [
    "you can also check....\n",
    "# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:center;border-radius:10px 10px;\">Supervised Machine Learning models scratch series.... </p>\n",
    "\n",
    "\n",
    "\n",
    "- 1) Linear Regression         ---> https://www.kaggle.com/ninjaac/linear-regression-from-scratch\n",
    "- 2) Lasso Regression          ---> https://www.kaggle.com/ninjaac/lasso-ridge-regression \n",
    "- 3) Ridge Regression          ---> https://www.kaggle.com/ninjaac/lasso-ridge-regression \n",
    "- 4) ElasticNet Regression     ---> https://www.kaggle.com/ninjaac/elasticnet-regression \n",
    "- 5) Polynomail Regression     ---> https://www.kaggle.com/ninjaac/polynomial-and-polynomialridge-regression \n",
    "- 5) PolynomailRidge Regression---> https://www.kaggle.com/ninjaac/polynomial-and-polynomialridge-regression \n",
    "- 6) KNN Classifier            ---> https://www.kaggle.com/ninjaac/knnclassifier-from-scratch \n",
    "\n",
    "# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:center;border-radius:10px 10px;\">Unsupervised Machine learning models .... </p>\n",
    "\n",
    "- 1) Kmeans                    ---> https://www.kaggle.com/ninjaac/scratch-kmeans-from-scratch (Same Notebook you are looking now)\n",
    "- 2) Hierartical clustering ------ loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75216fbe",
   "metadata": {},
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
