{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c22aee",
   "metadata": {},
   "source": [
    "<img src='https://yastatic.net/s3/ml-handbook/admin/5_1_848ce5004c.png' width='1200px'>\n",
    "\n",
    "<a href='https://academy.yandex.ru/handbook/ml/article/gradientnyj-busting'>image source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97c531",
   "metadata": {},
   "source": [
    "# INTRO\n",
    "\n",
    "Gradient descent and boosting are optimization methods used to improve the accuracy of machine learning models. These methods allow you to adjust the parameters of the model to minimize prediction error.\n",
    "\n",
    "Below, I'll describe the step-by-step process of how these methods work using a simple example. For simplicity, I specifically did not use mathematical formulas and greatly simplified the description of the algorithms.\n",
    "\n",
    "If you're interested in learning more about these methods, I recommend studying the topic of gradient boosting in more depth, for example, by reading <a href='https://www.kaggle.com/code/kashnitsky/topic-10-gradient-boosting'>this Kaggle topic</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278038b",
   "metadata": {},
   "source": [
    "# GRADIENT DESCENT\n",
    "\n",
    "We are going to  use a sample of 10 observations, each with 3 parameters (features or x) and a target variable y. The target value is linearly dependent on the predictors x. The value of the target variable y is determined by the function:\n",
    "y = w0 + w1x1 + w2x2 + w3x3\n",
    "\n",
    "The value of w(i) is unknown to us, our task is to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a random dataset and represent it in a table for easier visualization.\n",
    "X, y = make_regression(n_samples=10, n_features=3, noise=.01, random_state=42)\n",
    "df = pd.DataFrame(X, columns=['x1', 'x2', 'x3'])\n",
    "df['y_true'] = y\n",
    "df.head()\n",
    "# The value of target 'y_true' is linearly dependent on x1, x2 and x3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946650e",
   "metadata": {},
   "source": [
    "### STEP 1\n",
    "Ok, let's randomly initialize the values of w0, w1, w2, and w3. This is usually done to start the optimization process for finding the best possible values for these coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e591bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w1, w2, w3 = 1, 1, 1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde1c53",
   "metadata": {},
   "source": [
    "### STEP 2\n",
    "To make the prediction of the target variable y, we can use our linear dependency function with the given random values of w0, w1, w2 and w3, and the values of x1, x2, and x3 for each observation in the dataset. The predicted value of y can then be calculated by plugging in the values into the linear dependency equation:\n",
    "\n",
    "y = w0 + w1x1 + w2x2 + w3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_pred'] = w0+w1*df['x1']+w2*df['x2']+w3*df['x3']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a317d7",
   "metadata": {},
   "source": [
    "### STEP 3\n",
    "Let's understand how well we did with the prediction task. To do this, we will use the mean squared  error loss function. It shows the difference between the actual and predicted target values. We chose it for the example and any other loss function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab74eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(df['y_true'], df['y_pred'] )\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f942909",
   "metadata": {},
   "source": [
    "### STEP 4\n",
    "It looks like we are far from perfect. Let's improve the accuracy of our prediction. To do this, we will use the gradient.\n",
    "\n",
    "The gradient is a vector that shows the direction of the function's increase. The antigradient is used to find the minimum.\n",
    "We will move along the loss function's graph line until we reach the local  minimum point. Jumping ahead, I will say that the gradient descent method only allows to get closer to the local minimum point.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/betelgeus/study/master/images/descent.png' width='400px'>\n",
    "\n",
    "\n",
    "Gradient descent requires finding the partial derivatives of the loss function with respect to each of the coefficients (or parameters) in the model. If you are not familiar with derivatives, they can be calculated using an <a href='https://www.derivative-calculator.net'>online calculator</a>. \n",
    "\n",
    "The gradient computation is essentially the process of finding the slope of the loss function at the current parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed5850",
   "metadata": {},
   "source": [
    "### FIND PARTIAL DERIVATIVES\n",
    "\n",
    "**Linear regression function**\n",
    "$$f(w, x_i) = w_0 + w_i x_{ki}$$\n",
    "\n",
    "$$f(w, x_i) = w_0 + w_1 x_1  + w_2 x_2 + w_3 x_3$$\n",
    "\n",
    "**Loss function**\n",
    "$$\\mathcal{L}(y, f(w, x_i)) = \\frac{1}{N} \\displaystyle\\sum_{i=1}^{N}(y_i - f(w, x_i))^2 \\rightarrow min$$\n",
    "\n",
    "$$\\mathcal{L}(y, f(w, x_i)) = \\frac{1}{N} \\displaystyle\\sum_{i=1}^{N}(y_i - ( w_0 + w_1 x_1  + w_2 x_2 + w_3 x_3))^2 \\rightarrow min$$\n",
    "\n",
    "**Find partial derivatives for each weight: w0, w1, w2, w3**\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(y, f(w, x_i))}{\\partial w_0} = \\frac{2(w_0 - y + w_1 x_1  + w_2 x_2 + w_3 x_3)}{N}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(y, f(w, x_i))}{\\partial w_1} = \\frac{2x_1(w_1x_1 - y + w_0  + w_2 x_2 + w_3 x_3)}{N}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(y, f(w, x_i))}{\\partial w_2} = \\frac{2x_2(w_2x_2 - y + w_0  + w_1 x_1 + w_3 x_3)}{N}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(y, f(w, x_i))}{\\partial w_3} = \\frac{2x_3(w_3x_3 - y + w_0  + w_1 x_1 + w_2 x_2)}{N}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_derivative = np.mean(2*(w0-df['y_true']+w1*df['x1']+w2*df['x2']+w3*df['x3']))\n",
    "w1_derivative = np.mean(2*df['x1']*(w1*df['x1']-df['y_true']+w0+w2*df['x2']+w3*df['x3']))\n",
    "w2_derivative = np.mean(2*df['x2']*(w2*df['x2']-df['y_true']+w0+w1*df['x1']+w3*df['x3']))\n",
    "w3_derivative = np.mean(2*df['x3']*(w3*df['x3']-df['y_true']+w0+w1*df['x1']+w2*df['x2']))\n",
    "print('w0_derivative:', w0_derivative)\n",
    "print('w1_derivative:', w1_derivative)\n",
    "print('w2_derivative:', w2_derivative)\n",
    "print('w3_derivative:', w3_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3388974",
   "metadata": {},
   "source": [
    "### STEP 5\n",
    "Look's good! Let's use gradient to update the parameters. \n",
    "\n",
    "This is done by subtracting a scaled version of the gradient from the current parameter values. The scale factor is called the learning rate, and it determines the size of the step we take towards the minimum. A small learning rate ensures that the parameters are updated slowly and converge to the minimum, while a large learning rate can lead to overshooting and oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "w0 = w0 - learning_rate * w0_derivative\n",
    "w1 = w1 - learning_rate * w1_derivative\n",
    "w2 = w2 - learning_rate * w2_derivative\n",
    "w3 = w3 - learning_rate * w3_derivative\n",
    "\n",
    "df['y_pred'] = w0+w1*df['x1']+w2*df['x2']+w3*df['x3']\n",
    "mse = mean_squared_error(df['y_true'], df['y_pred'] )\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c294e5",
   "metadata": {},
   "source": [
    "### STEP 6\n",
    "Wonderful! We have minimized the error. Let's repeat previos steps until a stopping criteria is met, such as a maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "loss = []\n",
    "\n",
    "for _ in range(n):\n",
    "    df['y_pred'] = w0+w1*df['x1']+w2*df['x2']+w3*df['x3']\n",
    "    loss.append(mean_squared_error(df['y_true'], df['y_pred']))\n",
    "    \n",
    "    w0_derivative = np.mean(2*(w0-df['y_true']+w1*df['x1']+w2*df['x2']+w3*df['x3']))\n",
    "    w1_derivative = np.mean(2*df['x1']*(w1*df['x1']-df['y_true']+w0+w2*df['x2']+w3*df['x3']))\n",
    "    w2_derivative = np.mean(2*df['x2']*(w2*df['x2']-df['y_true']+w0+w1*df['x1']+w3*df['x3']))\n",
    "    w3_derivative = np.mean(2*df['x3']*(w3*df['x3']-df['y_true']+w0+w1*df['x1']+w2*df['x2']))\n",
    "\n",
    "    w0 = w0 - learning_rate * w0_derivative\n",
    "    w1 = w1 - learning_rate * w1_derivative\n",
    "    w2 = w2 - learning_rate * w2_derivative\n",
    "    w3 = w3 - learning_rate * w3_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will plot the loss function values for each training iteration. \n",
    "# As we can see, the difference between the actual and predicted target value decreases gradually.\n",
    "plt.plot(loss)\n",
    "plt.title(f'MSE Loss: {loss[-1]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a7a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w0, w1, w2, w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af534555",
   "metadata": {},
   "source": [
    "### Finaly\n",
    "The gradient descent algorithm is a crutical component of many machine learning algorithms, including linear regression, logistic regression, and neural networks. It works by iteratively adjusting the parameters until the cost is minimized, effectively finding the optimal values for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f61a53",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTING\n",
    "Now we are using 1000 observations and dividing the samples into two: training and testing. Some of the steps will be new from what was before. I will describe them in as much detail as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e71a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=3, noise=.01, random_state=42)\n",
    "\n",
    "# Divide sample on train and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Represent data in a table for easier visualization.\n",
    "train = pd.DataFrame(X_train, columns=['x1', 'x2', 'x3'])\n",
    "train['y_true'] = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40971df6",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Let's make a simple constant prediction of the target variable by taking the mean of the true target values. Then we will calculate the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ff371",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['y_pred'] = np.mean(train['y_true'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c782847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the loss function\n",
    "mse = mean_squared_error(train['y_true'], train['y_pred'])\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c73ce3",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "And so, our task is to minimize the value of the loss function. To solve this problem, we will find the value of the derivative for each observation, that is, the residual. Let's find the residuals, the difference between the actual and predicted target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['residuals'] = train['y_true'] - train['y_pred']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb8085",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Let's create a simple decision tree, for clarity we use the minimum depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(random_state=42, max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea5030",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "For each leaf, the DecisionTreeRegressor minimizes the loss function.  Algorithm searches for such a predict value that the sum of derivatives (residuals) is minimal. Train the tree to predict residuals.\n",
    "\n",
    "This value will be mean value of residuals in the leaf. DecisionTreeRegressor will ised them as a predictor for each observation. \n",
    "\n",
    "Due to the fact that we use a tree with max_depth = 1, we will have only two leaves, respectively, only two mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(train[['x1', 'x2', 'x3']], train['residuals'])\n",
    "train['output (res_pred)'] = tree.predict(train[['x1', 'x2', 'x3']])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4072ac5",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "It's time for boosting. Let's improve our model's prediction. Just like last time, we'll gradually train the model, adjusting the prediction value with the learning rate.\n",
    "\n",
    "But now for prediction we use desicion tree instead of linear regression function and we predict residuals instead of target value.\n",
    "\n",
    "We will improve the accuracy by adding to the target variable the value of the residual multiplied by the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['y_pred'] +=  learning_rate * train['output (res_pred)']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70eb93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(train['y_true'], train['y_pred'])\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98932d6b",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "Not bad. We were able to improve the prediction quality. Let's repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9cc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "\n",
    "# Arrays for storing the values of the loss function and trees. \n",
    "# The trees will be needed later for predicting target values in the test sample.\n",
    "loss = []\n",
    "trees = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(n):\n",
    "    train['residuals'] = train['y_true'] - train['y_pred']\n",
    "    tree = DecisionTreeRegressor(random_state=42, max_depth=1)\n",
    "    tree.fit(train[['x1', 'x2', 'x3']], train['residuals'])\n",
    "    train['output (res_pred)'] = tree.predict(train[['x1', 'x2', 'x3']])\n",
    "    train['y_pred'] +=  learning_rate * train['output (res_pred)']\n",
    "    loss.append(mean_squared_error(train['y_true'], train['y_pred']))\n",
    "    trees.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.title(f'MSE Loss {loss[-1]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cb3ac",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "Let's repeat the actions on the data that the model did not see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(X_test, columns=['x1', 'x2', 'x3'])\n",
    "test['y_true'] = y_test\n",
    "test['y_pred'] = y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "\n",
    "for tree in trees:\n",
    "    test['output (res_pred)'] = tree.predict(test[['x1', 'x2', 'x3']])\n",
    "    test['y_pred'] +=  learning_rate * test['output (res_pred)']\n",
    "    loss.append(mean_squared_error(test['y_true'], test['y_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.title(f'MSE Loss {loss[-1]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df560853",
   "metadata": {},
   "source": [
    "# LET'S PRACTICE\n",
    "We will practice on the popular dataset House Prices. First of all, it is important for us to understand the work of the GBDT, so we will don't focus on the steps with EDA, data preprocessing and feature engenering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test = pd.read_csv('/kaggle//input/house-prices-advanced-regression-techniques/test.csv')\n",
    "test_id = test['Id'].copy()\n",
    "sample_sub = pd.read_csv('/kaggle//input/house-prices-advanced-regression-techniques/sample_submission.csv')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7063f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.concat([train, test])\n",
    "train_test = train_test.reset_index(drop=True)\n",
    "train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in ['PoolQC', 'Alley', 'MiscFeature', 'FireplaceQu', 'Fence']:\n",
    "    train_test[col_name] = train_test[col_name].fillna('None')\n",
    "\n",
    "for col_name in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageYrBlt',\\\n",
    "                 'BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2', 'BsmtExposure']:\n",
    "    train_test[col_name] = train_test[col_name].fillna('No')\n",
    "\n",
    "for col_name in ['MasVnrArea', 'BsmtFinSF2', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\\\n",
    "                 'BsmtUnfSF', 'BsmtFinSF1', 'GarageArea', 'LotFrontage']:\n",
    "    train_test[col_name] = train_test[col_name].fillna(0)\n",
    "\n",
    "for col_name, value in zip(['MSZoning', 'Utilities', 'Functional', 'Exterior1st',\\\n",
    "                            'KitchenQual', 'Electrical', 'SaleType', 'Exterior2nd', 'MasVnrType', 'GarageCars'],\\\n",
    "                           ['RL', 'AllPub', 'Typ', 'VinylSd', 'TA', 'SBrkr', 'WD', 'VinylSd', 'None', 2]):\n",
    "\ttrain_test[col_name] = train_test[col_name].fillna(value)\n",
    "\n",
    "train_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = train_test.select_dtypes(include = ['float64', 'int64']).columns.drop('SalePrice')\n",
    "categorical_features = train_test.select_dtypes(include = 'object').columns\n",
    "predictors = list(numerical_features) + list(categorical_features)\n",
    "std = StandardScaler()\n",
    "target = 'SalePrice'\n",
    "\n",
    "\n",
    "\n",
    "def encoder(df, categorical_cols, numerical_cols, target):\n",
    "    y = df[target]\n",
    "    df[categorical_cols] = df[categorical_cols].astype('str')\n",
    "    df_lbl = df[categorical_cols].apply(LabelEncoder().fit_transform)\n",
    "    num_cols_std_scaled = std.fit_transform(df[numerical_cols])\n",
    "    df_num = pd.DataFrame(num_cols_std_scaled, columns=numerical_cols)\n",
    "    df = pd.concat([df_lbl, df_num], axis=1)\n",
    "    df = pd.concat([df, y], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_test = encoder(train_test, categorical_features, numerical_features, target)\n",
    "\n",
    "train = train_test[train_test[target].notnull()].copy()\n",
    "test = train_test[train_test[target].isna()].copy()\n",
    "test = test.drop([target], axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2aed0",
   "metadata": {},
   "source": [
    "### LET'S GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426cebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Predictions'] = train[target].mean()\n",
    "train[[target, 'Predictions']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Residuals'] = train[target] - train['Predictions']\n",
    "train[[target, 'Predictions', 'Residuals']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d896a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(np.log(train['Predictions']), np.log(train[target]), squared=False) \n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(random_state=42, max_depth=4, min_samples_split=13, max_features=0.74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(train[predictors], train['Residuals'])\n",
    "train['Output'] = tree.predict(train[predictors])\n",
    "train[[target, 'Predictions', 'Residuals', 'Output']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc57f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Predictions'] +=  learning_rate * train['Output']\n",
    "train[[target, 'Predictions']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(np.log(train['Predictions']), np.log(train[target]), squared=False) \n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "trees = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435e543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(n)):\n",
    "    train['Residuals'] = train[target] - train['Predictions']\n",
    "    train_sample, val_sample = train_test_split(train, train_size=0.8, random_state=42)\n",
    "    tree = DecisionTreeRegressor(random_state=42, max_depth=4, min_samples_split=13, max_features=0.74)\n",
    "    tree.fit(train_sample[predictors], train_sample['Residuals'])\n",
    "    train_sample['Output'] = tree.predict(train_sample[predictors])\n",
    "    train_sample['Predictions'] +=  learning_rate * train_sample['Output']\n",
    "    val_sample['Output'] = tree.predict(val_sample[predictors])\n",
    "    val_sample['Predictions'] +=  learning_rate * val_sample['Output']\n",
    "    train = pd.concat([train_sample, val_sample])\n",
    "    loss_train.append(mean_squared_error(np.log(train['Predictions']), np.log(train[target]), squared=False))\n",
    "    loss_val.append(mean_squared_error(np.log(val_sample['Predictions']), np.log(val_sample[target]), squared=False))\n",
    "    trees.append(tree)\n",
    "\n",
    "loss = pd.DataFrame({'Train': loss_train, 'Val': loss_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd4a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(np.log(train['Predictions']), np.log(train[target]), squared=False) \n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de6fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[target, 'Predictions']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Predictions'] = train[target].mean()\n",
    "for tree in trees:\n",
    "    test['Output'] = tree.predict(test[predictors])\n",
    "    test['Predictions'] +=  learning_rate * test['Output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532832e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(columns=sample_sub.columns)\n",
    "prediction.iloc[:, 0] = test_id\n",
    "# prediction.iloc[:, 1] = np.expm1(test['Predictions'].reset_index(drop=True))\n",
    "prediction.iloc[:, 1] = test['Predictions'].reset_index(drop=True)\n",
    "prediction.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab8e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe088282",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
