{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134a8d17",
   "metadata": {},
   "source": [
    "[Next notebook](https://www.kaggle.com/keremt/06-inference-multi-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289d662",
   "metadata": {},
   "source": [
    "[Next notebook](https://www.kaggle.com/keremt/06-inference-multi-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c75cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d870046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100be28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8faf516",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embspath = Path(\"/kaggle/input/rsnaperawembs256v2//\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "embspath = Path(\"/kaggle/input/rsnaperawembs256v2//\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embspath.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "embspath.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85416767",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_df = pd.read_csv(embspath/'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_df = pd.read_csv(embspath/'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e8be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make sure to normalize views for pe: left-right-central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf977eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make sure to normalize views for pe: left-right-central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec70edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67646653",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = torch.cat([torch.load(embspath/f'train_embs-{o}.pkl') for o in [0,1,2,'final']])\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = torch.cat([torch.load(embspath/f'train_embs-{o}.pkl') for o in [0,1,2,'final']])\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding embedding and idx\n",
    "embs_len = len(embs)\n",
    "pad_emb = torch.zeros_like(embs[:1])\n",
    "embs = torch.cat([embs, pad_emb])\n",
    "input_pad_idx = embs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding embedding and idx\n",
    "embs_len = len(embs)\n",
    "pad_emb = torch.zeros_like(embs[:1])\n",
    "embs = torch.cat([embs, pad_emb])\n",
    "input_pad_idx = embs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd29475",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs[input_pad_idx], embs.shape, input_pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs[input_pad_idx], embs.shape, input_pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = train_feat_df.groupby(\"StudyInstanceUID\").apply(len)\n",
    "seq_lens.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34360577",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = train_feat_df.groupby(\"StudyInstanceUID\").apply(len)\n",
    "seq_lens.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493dfcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(seq_lens < 300).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e112d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(seq_lens < 300).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76006b4",
   "metadata": {},
   "source": [
    "### Read metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1beeb",
   "metadata": {},
   "source": [
    "### Read metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = Path(\"/kaggle/input/rsnastrpemetadata/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2be2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = Path(\"/kaggle/input/rsnastrpemetadata/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadf = pd.concat([pd.read_parquet(metadata_path/f\"train_metadf_part{i}.pqt\") for i in range(1,3)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadf = pd.concat([pd.read_parquet(metadata_path/f\"train_metadf_part{i}.pqt\") for i in range(1,3)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df894380",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_feats = train_metadf[['StudyInstanceUID', 'SOPInstanceUID', 'ImagePositionPatient2', 'img_min', 'img_max', 'img_mean', 'img_std', 'img_pct_window']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_feats = train_metadf[['StudyInstanceUID', 'SOPInstanceUID', 'ImagePositionPatient2', 'img_min', 'img_max', 'img_mean', 'img_std', 'img_pct_window']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fe41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaler(o): return (o - min(o))/(max(o) - min(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaler(o): return (o - min(o))/(max(o) - min(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ad0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pos = train_meta_feats.groupby('StudyInstanceUID')['ImagePositionPatient2'].apply(minmax_scaler)\n",
    "train_meta_feats['scaled_position'] = scaled_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pos = train_meta_feats.groupby('StudyInstanceUID')['ImagePositionPatient2'].apply(minmax_scaler)\n",
    "train_meta_feats['scaled_position'] = scaled_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ef94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_feats.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_feats.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b465698",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_feats.sort_values('ImagePositionPatient2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110abb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_feats.sort_values('ImagePositionPatient2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee6d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feats = ['img_min', 'img_max', 'img_mean', 'img_std', 'scaled_position']\n",
    "mean_std = {}\n",
    "for f in meta_feats:\n",
    "    mean,std = train_meta_feats[f].mean(), train_meta_feats[f].std()\n",
    "    train_meta_feats[f] = (train_meta_feats[f] - mean) / std\n",
    "    mean_std[f] = (mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704231aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feats = ['img_min', 'img_max', 'img_mean', 'img_std', 'scaled_position']\n",
    "mean_std = {}\n",
    "for f in meta_feats:\n",
    "    mean,std = train_meta_feats[f].mean(), train_meta_feats[f].std()\n",
    "    train_meta_feats[f] = (train_meta_feats[f] - mean) / std\n",
    "    mean_std[f] = (mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673bcba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feats_dict = dict(zip(train_meta_feats['SOPInstanceUID'], train_meta_feats[meta_feats].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feats_dict = dict(zip(train_meta_feats['SOPInstanceUID'], train_meta_feats[meta_feats].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feats_dict['cc96a7a2e72c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127fd222",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feats_dict['cc96a7a2e72c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f458575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meta_feats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46fee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meta_feats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce90dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embs = np.vstack([meta_feats_dict[o] for o in train_feat_df['SOPInstanceUID'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embs = np.vstack([meta_feats_dict[o] for o in train_feat_df['SOPInstanceUID'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32550d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79dfcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embs = tensor(np.vstack((meta_embs, np.zeros((1,meta_embs.shape[1])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02217ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embs = tensor(np.vstack((meta_embs, np.zeros((1,meta_embs.shape[1])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516d8e3",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49321543",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use saved pids\n",
    "unique_pids = train_feat_df.StudyInstanceUID.unique()\n",
    "n = len(unique_pids)\n",
    "nvalid = int(n*0.05); nvalid, n\n",
    "unique_pids = np.random.permutation(unique_pids)\n",
    "train_pids = unique_pids[nvalid:]\n",
    "valid_pids = unique_pids[:nvalid]\n",
    "len(train_pids), len(valid_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use saved pids\n",
    "unique_pids = train_feat_df.StudyInstanceUID.unique()\n",
    "n = len(unique_pids)\n",
    "nvalid = int(n*0.05); nvalid, n\n",
    "unique_pids = np.random.permutation(unique_pids)\n",
    "train_pids = unique_pids[nvalid:]\n",
    "valid_pids = unique_pids[:nvalid]\n",
    "len(train_pids), len(valid_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08af581",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = defaultdict(list)\n",
    "for i, (_, row) in enumerate(train_feat_df.iterrows()):\n",
    "    fn = Path(row['fname'])\n",
    "    slice_no = int(fn.stem.split(\"_\")[0])\n",
    "    sid = fn.parent.parent.name\n",
    "    files_dict[sid].append(({**dict(row), **{\"slice_no\":slice_no, \"embs_idx\":i}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = defaultdict(list)\n",
    "for i, (_, row) in enumerate(train_feat_df.iterrows()):\n",
    "    fn = Path(row['fname'])\n",
    "    slice_no = int(fn.stem.split(\"_\")[0])\n",
    "    sid = fn.parent.parent.name\n",
    "    files_dict[sid].append(({**dict(row), **{\"slice_no\":slice_no, \"embs_idx\":i}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab587c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f577539",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_targets = L(['pe_present_on_image'])\n",
    "exam_targets = L([\n",
    "#           'positive_exam_for_pe'\n",
    "            'negative_exam_for_pe',\n",
    "            'indeterminate',\n",
    "\n",
    "            'rv_lv_ratio_gte_1',\n",
    "            'rv_lv_ratio_lt_1',\n",
    "    # none\n",
    "\n",
    "            'leftsided_pe',\n",
    "            'rightsided_pe',\n",
    "            'central_pe',\n",
    "\n",
    "            'chronic_pe',\n",
    "            'acute_and_chronic_pe',           \n",
    "            # neither chronic or acute_and_chronic\n",
    "          \n",
    "    \n",
    "    \n",
    "#             'qa_motion',\n",
    "#             'qa_contrast',\n",
    "#             'flow_artifact',\n",
    "#             'true_filling_defect_not_pe',\n",
    "             ]); exam_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_targets = L(['pe_present_on_image'])\n",
    "exam_targets = L([\n",
    "#           'positive_exam_for_pe'\n",
    "            'negative_exam_for_pe',\n",
    "            'indeterminate',\n",
    "\n",
    "            'rv_lv_ratio_gte_1',\n",
    "            'rv_lv_ratio_lt_1',\n",
    "    # none\n",
    "\n",
    "            'leftsided_pe',\n",
    "            'rightsided_pe',\n",
    "            'central_pe',\n",
    "\n",
    "            'chronic_pe',\n",
    "            'acute_and_chronic_pe',           \n",
    "            # neither chronic or acute_and_chronic\n",
    "          \n",
    "    \n",
    "    \n",
    "#             'qa_motion',\n",
    "#             'qa_contrast',\n",
    "#             'flow_artifact',\n",
    "#             'true_filling_defect_not_pe',\n",
    "             ]); exam_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca827c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_pid = np.random.choice(train_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efcb989",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_pid = np.random.choice(train_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c0456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(pid):\n",
    "    o = files_dict[pid]    \n",
    "    l = sorted(o, key=lambda x: x['slice_no']) \n",
    "    return tensor([o['embs_idx'] for o in l])\n",
    "\n",
    "def get_img_y(pid):\n",
    "    o = files_dict[pid]    \n",
    "    l = sorted(o, key=lambda x: x['slice_no']) \n",
    "    img_y = [o['pe_present_on_image'] for o in l]\n",
    "    exam_y = [max(img_y)] + [o[0][t] for t in exam_targets]\n",
    "    return tensor(img_y)\n",
    "\n",
    "def get_exam_y(pid):\n",
    "    \"\"\"\n",
    "    'POSITIVE','negative_exam_for_pe','indeterminate',\n",
    "    'rv_lv_ratio_gte_1','rv_lv_ratio_lt_1', 'NEITHER'\n",
    "    'leftsided_pe','rightsided_pe','central_pe',\n",
    "    'chronic_pe','acute_and_chronic_pe','NEITHER'\n",
    "    \"\"\"\n",
    "    o = files_dict[pid]    \n",
    "    l = sorted(o, key=lambda x: x['slice_no']) \n",
    "    img_y = [o['pe_present_on_image'] for o in l]\n",
    "    \n",
    "    exam_y = [max(img_y)] + [o[0][t] for t in exam_targets]\n",
    "    \n",
    "    none_chro_acute = [exam_y[-1] == exam_y[-2]]\n",
    "    exam_y += none_chro_acute\n",
    "    \n",
    "    none_rv_lv = [exam_y[3] == exam_y[4]]\n",
    "    exam_y = exam_y[:4] + none_rv_lv + exam_y[4:]\n",
    "    \n",
    "    \n",
    "    return tensor(exam_y)\n",
    "\n",
    "# before_batch: after collecting samples before collating\n",
    "targ_pad_idx = 666\n",
    "def SequenceBlock():       return  TransformBlock(type_tfms=[get_x], dl_type=SortedDL, dls_kwargs={'before_batch':\n",
    "                                                       [partial(pad_input, pad_idx=input_pad_idx),\n",
    "                                                        partial(pad_input, pad_idx=targ_pad_idx, pad_fields=1)]})\n",
    "def SequenceTargetBlock(): return TransformBlock(type_tfms=[get_img_y])\n",
    "def TargetBlock():         return TransformBlock(type_tfms=[get_exam_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e995382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(pid):\n",
    "    o = files_dict[pid]    \n",
    "    l = sorted(o, key=lambda x: x['slice_no']) \n",
    "    return tensor([o['embs_idx'] for o in l])\n",
    "\n",
    "def get_img_y(pid):\n",
    "    o = files_dict[pid]    \n",
    "    l = sorted(o, key=lambda x: x['slice_no']) \n",
    "    img_y = [o['pe_present_on_image'] for o in l]\n",
    "    exam_y = [max(img_y)] + [o[0][t] for t in exam_targets]\n",
    "    return tensor(img_y)\n",
    "\n",
    "def get_exam_y(pid):\n",
    "    \"\"\"\n",
    "    'POSITIVE','negative_exam_for_pe','indeterminate',\n",
    "    'rv_lv_ratio_gte_1','rv_lv_ratio_lt_1', 'NEITHER'\n",
    "    'leftsided_pe','rightsided_pe','central_pe',\n",
    "    'chronic_pe','acute_and_chronic_pe','NEITHER'\n",
    "    \"\"\"\n",
    "    o = files_dict[pid]    \n",
    "    l = sorted(o, key=lambda x: x['slice_no']) \n",
    "    img_y = [o['pe_present_on_image'] for o in l]\n",
    "    \n",
    "    exam_y = [max(img_y)] + [o[0][t] for t in exam_targets]\n",
    "    \n",
    "    none_chro_acute = [exam_y[-1] == exam_y[-2]]\n",
    "    exam_y += none_chro_acute\n",
    "    \n",
    "    none_rv_lv = [exam_y[3] == exam_y[4]]\n",
    "    exam_y = exam_y[:4] + none_rv_lv + exam_y[4:]\n",
    "    \n",
    "    \n",
    "    return tensor(exam_y)\n",
    "\n",
    "# before_batch: after collecting samples before collating\n",
    "targ_pad_idx = 666\n",
    "def SequenceBlock():       return  TransformBlock(type_tfms=[get_x], dl_type=SortedDL, dls_kwargs={'before_batch':\n",
    "                                                       [partial(pad_input, pad_idx=input_pad_idx),\n",
    "                                                        partial(pad_input, pad_idx=targ_pad_idx, pad_fields=1)]})\n",
    "def SequenceTargetBlock(): return TransformBlock(type_tfms=[get_img_y])\n",
    "def TargetBlock():         return TransformBlock(type_tfms=[get_exam_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b61709",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBlock(blocks=(SequenceBlock,SequenceTargetBlock,TargetBlock), \n",
    "                 n_inp=1, \n",
    "                 splitter=FuncSplitter(lambda o: o in valid_pids))\n",
    "dls = data.dataloaders(list(train_pids)+list(valid_pids), bs=128)\n",
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBlock(blocks=(SequenceBlock,SequenceTargetBlock,TargetBlock), \n",
    "                 n_inp=1, \n",
    "                 splitter=FuncSplitter(lambda o: o in valid_pids))\n",
    "dls = data.dataloaders(list(train_pids)+list(valid_pids), bs=128)\n",
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b597c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(dls, nn.Linear(10,10), loss_func=noop)\n",
    "learner._split(b)\n",
    "len(learner.xb), len(learner.yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(dls, nn.Linear(10,10), loss_func=noop)\n",
    "learner._split(b)\n",
    "len(learner.xb), len(learner.yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.xb[0].shape, learner.yb[0].shape, learner.yb[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c63153",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.xb[0].shape, learner.yb[0].shape, learner.yb[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs[learner.xb[0]].shape, meta_embs[learner.xb[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs[learner.xb[0]].shape, meta_embs[learner.xb[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([embs[learner.xb[0]], meta_embs[learner.xb[0]]], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([embs[learner.xb[0]], meta_embs[learner.xb[0]]], -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17f99b",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c072c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64684a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99fdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb89398",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSequenceClassifier(Module):\n",
    "    \"dim: input sequence feature dim\"\n",
    "    def __init__(self, input_pad_idx=input_pad_idx, dim=1024):\n",
    "        \n",
    "        store_attr('input_pad_idx')\n",
    "        self.lstm1 = nn.LSTM(dim+5, dim//16, bidirectional=True)\n",
    "#         self.lstm1 = nn.LSTM(dim+5, dim//8, bidirectional=True)\n",
    "#         self.lstm2 = nn.LSTM(dim//4, dim//16, bidirectional=True)\n",
    "        \n",
    "        # image level preds\n",
    "        self.seq_cls_head = nn.Linear(dim//8, 1)\n",
    "    \n",
    "        \n",
    "        # positive, negative, indeterminate\n",
    "        self.pe_head = nn.Linear(dim//4, 3) # softmax\n",
    "        # rv / lv >=,  < 1 or neither\n",
    "        self.rv_lv_head = nn.Linear(dim//4, 3) # softmax\n",
    "        # l,r,c pe\n",
    "        self.pe_position_head = nn.Linear(dim//4, 3) # sigmoid\n",
    "        # chronic, ac-chr or neither\n",
    "        self.chronic_pe_head = nn.Linear(dim//4, 3) # softmax\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # get mask from non-pad idxs and then features\n",
    "        mask = x != self.input_pad_idx\n",
    "        x = torch.cat([embs[x], meta_embs[x]], dim=-1).to(device)\n",
    "        \n",
    "        # sequence outs\n",
    "        x, _ = self.lstm1(x) \n",
    "#         x, _ = self.lstm2(x)\n",
    "        seq_cls_out = self.seq_cls_head(x).squeeze(-1)\n",
    "        \n",
    "        \n",
    "        #masked concat pool\n",
    "        pooled_x = []\n",
    "        for i in range(x.size(0)):\n",
    "            xi = x[i, mask[i], :]\n",
    "            pooled_x.append(torch.cat([xi.mean(0), xi.max(0).values]).unsqueeze(0))\n",
    "        pooled_x = torch.cat(pooled_x)\n",
    "        \n",
    "\n",
    "        # 'POSITIVE','negative_exam_for_pe','indeterminate'\n",
    "        out1 = self.pe_head(pooled_x)\n",
    "\n",
    "        # 'rv_lv_ratio_gte_1','rv_lv_ratio_lt_1', 'NEITHER'\n",
    "        out2 = self.rv_lv_head(pooled_x)\n",
    "\n",
    "        # 'leftsided_pe','rightsided_pe','central_pe',\n",
    "        out3 = self.pe_position_head(pooled_x)\n",
    "\n",
    "        # 'chronic_pe','acute_and_chronic_pe','NEITHER'\n",
    "        out4 = self.chronic_pe_head(pooled_x)\n",
    "\n",
    "        return (seq_cls_out, out1, out2, out3, out4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80569bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSequenceClassifier(Module):\n",
    "    \"dim: input sequence feature dim\"\n",
    "    def __init__(self, input_pad_idx=input_pad_idx, dim=1024):\n",
    "        \n",
    "        store_attr('input_pad_idx')\n",
    "        self.lstm1 = nn.LSTM(dim+5, dim//16, bidirectional=True)\n",
    "#         self.lstm1 = nn.LSTM(dim+5, dim//8, bidirectional=True)\n",
    "#         self.lstm2 = nn.LSTM(dim//4, dim//16, bidirectional=True)\n",
    "        \n",
    "        # image level preds\n",
    "        self.seq_cls_head = nn.Linear(dim//8, 1)\n",
    "    \n",
    "        \n",
    "        # positive, negative, indeterminate\n",
    "        self.pe_head = nn.Linear(dim//4, 3) # softmax\n",
    "        # rv / lv >=,  < 1 or neither\n",
    "        self.rv_lv_head = nn.Linear(dim//4, 3) # softmax\n",
    "        # l,r,c pe\n",
    "        self.pe_position_head = nn.Linear(dim//4, 3) # sigmoid\n",
    "        # chronic, ac-chr or neither\n",
    "        self.chronic_pe_head = nn.Linear(dim//4, 3) # softmax\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # get mask from non-pad idxs and then features\n",
    "        mask = x != self.input_pad_idx\n",
    "        x = torch.cat([embs[x], meta_embs[x]], dim=-1).to(device)\n",
    "        \n",
    "        # sequence outs\n",
    "        x, _ = self.lstm1(x) \n",
    "#         x, _ = self.lstm2(x)\n",
    "        seq_cls_out = self.seq_cls_head(x).squeeze(-1)\n",
    "        \n",
    "        \n",
    "        #masked concat pool\n",
    "        pooled_x = []\n",
    "        for i in range(x.size(0)):\n",
    "            xi = x[i, mask[i], :]\n",
    "            pooled_x.append(torch.cat([xi.mean(0), xi.max(0).values]).unsqueeze(0))\n",
    "        pooled_x = torch.cat(pooled_x)\n",
    "        \n",
    "\n",
    "        # 'POSITIVE','negative_exam_for_pe','indeterminate'\n",
    "        out1 = self.pe_head(pooled_x)\n",
    "\n",
    "        # 'rv_lv_ratio_gte_1','rv_lv_ratio_lt_1', 'NEITHER'\n",
    "        out2 = self.rv_lv_head(pooled_x)\n",
    "\n",
    "        # 'leftsided_pe','rightsided_pe','central_pe',\n",
    "        out3 = self.pe_position_head(pooled_x)\n",
    "\n",
    "        # 'chronic_pe','acute_and_chronic_pe','NEITHER'\n",
    "        out4 = self.chronic_pe_head(pooled_x)\n",
    "\n",
    "        return (seq_cls_out, out1, out2, out3, out4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62102b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiHeadedSequenceClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c885f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiHeadedSequenceClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb54bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs = model(*learner.xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs = model(*learner.xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLoss(Module):\n",
    "    \n",
    "    def __init__(self, targ_pad_idx=666):\n",
    "        store_attr(\"targ_pad_idx\")\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.ce_loss = CrossEntropyLossFlat()\n",
    "\n",
    "    def forward(self, inp, yb0, yb1):\n",
    "\n",
    "        seq_cls_out, out1, out2, out3, out4 = inp\n",
    "\n",
    "        mask = yb0 != self.targ_pad_idx\n",
    "        loss0 = self.bce_loss(seq_cls_out[mask], yb0[mask].float())\n",
    "\n",
    "\n",
    "        # loss 1 p/n/i\n",
    "        loss1 = self.ce_loss(out1, torch.where(yb1[:,:3])[1])\n",
    "\n",
    "        # loss 2 rv/lv/neither\n",
    "        loss2 = self.ce_loss(out2, torch.where(yb1[:,3:6])[1])\n",
    "\n",
    "        # loss 3 L/R/C\n",
    "        loss3 = self.bce_loss(out3, yb1[:,6:9].float())\n",
    "\n",
    "        # loss 3 chro/acute/neither\n",
    "        loss4 = self.ce_loss(out4, torch.where(yb1[:,9:])[1])\n",
    "\n",
    "        \n",
    "        return (loss0 + loss1 + loss2 + loss3 + loss4) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6412bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLoss(Module):\n",
    "    \n",
    "    def __init__(self, targ_pad_idx=666):\n",
    "        store_attr(\"targ_pad_idx\")\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.ce_loss = CrossEntropyLossFlat()\n",
    "\n",
    "    def forward(self, inp, yb0, yb1):\n",
    "\n",
    "        seq_cls_out, out1, out2, out3, out4 = inp\n",
    "\n",
    "        mask = yb0 != self.targ_pad_idx\n",
    "        loss0 = self.bce_loss(seq_cls_out[mask], yb0[mask].float())\n",
    "\n",
    "\n",
    "        # loss 1 p/n/i\n",
    "        loss1 = self.ce_loss(out1, torch.where(yb1[:,:3])[1])\n",
    "\n",
    "        # loss 2 rv/lv/neither\n",
    "        loss2 = self.ce_loss(out2, torch.where(yb1[:,3:6])[1])\n",
    "\n",
    "        # loss 3 L/R/C\n",
    "        loss3 = self.bce_loss(out3, yb1[:,6:9].float())\n",
    "\n",
    "        # loss 3 chro/acute/neither\n",
    "        loss4 = self.ce_loss(out4, torch.where(yb1[:,9:])[1])\n",
    "\n",
    "        \n",
    "        return (loss0 + loss1 + loss2 + loss3 + loss4) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd92a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = MultiLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65783d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = MultiLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_func(outs, *learner.yb); loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_func(outs, *learner.yb); loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5388aa",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4eff01",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'POSITIVE','negative_exam_for_pe','indeterminate',\n",
    "# 'rv_lv_ratio_gte_1','rv_lv_ratio_lt_1', 'NEITHER'\n",
    "# 'leftsided_pe','rightsided_pe','central_pe',\n",
    "# 'chronic_pe','acute_and_chronic_pe','NEITHER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e3165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'POSITIVE','negative_exam_for_pe','indeterminate',\n",
    "# 'rv_lv_ratio_gte_1','rv_lv_ratio_lt_1', 'NEITHER'\n",
    "# 'leftsided_pe','rightsided_pe','central_pe',\n",
    "# 'chronic_pe','acute_and_chronic_pe','NEITHER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc3942",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = BCEWithLogitsLossFlat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec81ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = BCEWithLogitsLossFlat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591118b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_cls_out, out1, out2, out3, out4 = outs\n",
    "# yb0, yb1 = learner.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_cls_out, out1, out2, out3, out4 = outs\n",
    "# yb0, yb1 = learner.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pe_wgt = 0.0736196319\n",
    "indeterminate_wgt = 0.09202453988\n",
    "\n",
    "rv_lv_gte_1_wgt = 0.2346625767\n",
    "rv_lv_lt_1_wgt = 0.0782208589\n",
    "\n",
    "left_pe_wgt = 0.06257668712\n",
    "right_pe_wgt = 0.06257668712\n",
    "central_pe_wgt = 0.1877300613\n",
    "\n",
    "chronic_wgt = 0.1042944785\n",
    "acute_chronic_wgt = 0.1042944785\n",
    "\n",
    "bce_loss = BCEWithLogitsLossFlat()\n",
    "\n",
    "def metric(preds, yb0, yb1):\n",
    "    \n",
    "    seq_cls_out, out1, out2, out3, out4 = preds\n",
    "    \n",
    "    bs = out1.size(0)\n",
    "    \n",
    "    out1 = F.softmax(out1, 1)\n",
    "    out2 = F.softmax(out2, 1)\n",
    "    out3 = torch.sigmoid(out3)\n",
    "    out4 = F.softmax(out4, 1)\n",
    "    \n",
    "    neg_pe_loss = F.binary_cross_entropy(out1[:,1], yb1[:,1].float())\n",
    "    indeterminate_loss = F.binary_cross_entropy(out1[:,2], yb1[:,2].float())\n",
    "\n",
    "    rv_lv_gte_1_loss = F.binary_cross_entropy(out2[:,0], yb1[:,3].float())\n",
    "    rv_lv_lt_1_loss = F.binary_cross_entropy(out2[:,1], yb1[:,4].float())\n",
    "\n",
    "    left_pe_wgt = F.binary_cross_entropy(out3[:,0], yb1[:,6].float())\n",
    "    right_pe_wgt = F.binary_cross_entropy(out3[:,0], yb1[:,7].float())\n",
    "    central_pe_wgt = F.binary_cross_entropy(out3[:,0], yb1[:,8].float())\n",
    "\n",
    "    chronic_loss = F.binary_cross_entropy(out4[:,0], yb1[:,9].float())\n",
    "    acute_chronic_loss = F.binary_cross_entropy(out4[:,1], yb1[:,10].float())\n",
    "\n",
    "    \n",
    "    tot_exam_loss = 0\n",
    "    tot_exam_wgts = 0\n",
    "\n",
    "    tot_exam_loss += neg_pe_loss*bs*neg_pe_wgt\n",
    "    tot_exam_loss += indeterminate_loss*bs*indeterminate_wgt\n",
    "    tot_exam_loss += rv_lv_gte_1_loss*bs*rv_lv_gte_1_wgt\n",
    "    tot_exam_loss += rv_lv_lt_1_loss*bs*rv_lv_lt_1_wgt\n",
    "    tot_exam_loss += left_pe_wgt*bs*left_pe_wgt\n",
    "    tot_exam_loss += right_pe_wgt*bs*right_pe_wgt\n",
    "    tot_exam_loss += central_pe_wgt*bs*central_pe_wgt\n",
    "    tot_exam_loss += chronic_loss*bs*chronic_wgt\n",
    "    tot_exam_loss += acute_chronic_loss*bs*acute_chronic_wgt\n",
    "\n",
    "    tot_exam_wgts += bs*neg_pe_wgt\n",
    "    tot_exam_wgts += bs*indeterminate_wgt\n",
    "    tot_exam_wgts += bs*rv_lv_gte_1_wgt\n",
    "    tot_exam_wgts += bs*rv_lv_lt_1_wgt\n",
    "    tot_exam_wgts += bs*left_pe_wgt\n",
    "    tot_exam_wgts += bs*right_pe_wgt\n",
    "    tot_exam_wgts += bs*central_pe_wgt\n",
    "    tot_exam_wgts += bs*chronic_wgt\n",
    "    tot_exam_wgts += bs*acute_chronic_wgt\n",
    "\n",
    "    tot_exam_loss, tot_exam_wgts = tot_exam_loss.item(), tot_exam_wgts.item()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Image-level weighted log loss (single batch)\n",
    "    w_img = 0.07361963\n",
    "    tot_img_loss = 0\n",
    "    tot_img_wgts = 0\n",
    "    for img_preds, img_targs in zip(seq_cls_out, yb0):\n",
    "        mask = img_targs != targ_pad_idx\n",
    "        img_preds = img_preds[mask]\n",
    "        img_targs = img_targs[mask]\n",
    "\n",
    "        n_imgs = sum(mask)\n",
    "\n",
    "        qi = img_targs.float().mean()    \n",
    "        img_loss = bce_loss(img_preds, img_targs)    \n",
    "        wgt = w_img*qi\n",
    "        tot_img_wgts += (wgt).item()*n_imgs\n",
    "        tot_img_loss += (wgt*img_loss).item()*n_imgs\n",
    "\n",
    "    \n",
    "    return (tot_exam_loss + tot_img_loss) / (tot_exam_wgts + tot_img_wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pe_wgt = 0.0736196319\n",
    "indeterminate_wgt = 0.09202453988\n",
    "\n",
    "rv_lv_gte_1_wgt = 0.2346625767\n",
    "rv_lv_lt_1_wgt = 0.0782208589\n",
    "\n",
    "left_pe_wgt = 0.06257668712\n",
    "right_pe_wgt = 0.06257668712\n",
    "central_pe_wgt = 0.1877300613\n",
    "\n",
    "chronic_wgt = 0.1042944785\n",
    "acute_chronic_wgt = 0.1042944785\n",
    "\n",
    "bce_loss = BCEWithLogitsLossFlat()\n",
    "\n",
    "def metric(preds, yb0, yb1):\n",
    "    \n",
    "    seq_cls_out, out1, out2, out3, out4 = preds\n",
    "    \n",
    "    bs = out1.size(0)\n",
    "    \n",
    "    out1 = F.softmax(out1, 1)\n",
    "    out2 = F.softmax(out2, 1)\n",
    "    out3 = torch.sigmoid(out3)\n",
    "    out4 = F.softmax(out4, 1)\n",
    "    \n",
    "    neg_pe_loss = F.binary_cross_entropy(out1[:,1], yb1[:,1].float())\n",
    "    indeterminate_loss = F.binary_cross_entropy(out1[:,2], yb1[:,2].float())\n",
    "\n",
    "    rv_lv_gte_1_loss = F.binary_cross_entropy(out2[:,0], yb1[:,3].float())\n",
    "    rv_lv_lt_1_loss = F.binary_cross_entropy(out2[:,1], yb1[:,4].float())\n",
    "\n",
    "    left_pe_wgt = F.binary_cross_entropy(out3[:,0], yb1[:,6].float())\n",
    "    right_pe_wgt = F.binary_cross_entropy(out3[:,0], yb1[:,7].float())\n",
    "    central_pe_wgt = F.binary_cross_entropy(out3[:,0], yb1[:,8].float())\n",
    "\n",
    "    chronic_loss = F.binary_cross_entropy(out4[:,0], yb1[:,9].float())\n",
    "    acute_chronic_loss = F.binary_cross_entropy(out4[:,1], yb1[:,10].float())\n",
    "\n",
    "    \n",
    "    tot_exam_loss = 0\n",
    "    tot_exam_wgts = 0\n",
    "\n",
    "    tot_exam_loss += neg_pe_loss*bs*neg_pe_wgt\n",
    "    tot_exam_loss += indeterminate_loss*bs*indeterminate_wgt\n",
    "    tot_exam_loss += rv_lv_gte_1_loss*bs*rv_lv_gte_1_wgt\n",
    "    tot_exam_loss += rv_lv_lt_1_loss*bs*rv_lv_lt_1_wgt\n",
    "    tot_exam_loss += left_pe_wgt*bs*left_pe_wgt\n",
    "    tot_exam_loss += right_pe_wgt*bs*right_pe_wgt\n",
    "    tot_exam_loss += central_pe_wgt*bs*central_pe_wgt\n",
    "    tot_exam_loss += chronic_loss*bs*chronic_wgt\n",
    "    tot_exam_loss += acute_chronic_loss*bs*acute_chronic_wgt\n",
    "\n",
    "    tot_exam_wgts += bs*neg_pe_wgt\n",
    "    tot_exam_wgts += bs*indeterminate_wgt\n",
    "    tot_exam_wgts += bs*rv_lv_gte_1_wgt\n",
    "    tot_exam_wgts += bs*rv_lv_lt_1_wgt\n",
    "    tot_exam_wgts += bs*left_pe_wgt\n",
    "    tot_exam_wgts += bs*right_pe_wgt\n",
    "    tot_exam_wgts += bs*central_pe_wgt\n",
    "    tot_exam_wgts += bs*chronic_wgt\n",
    "    tot_exam_wgts += bs*acute_chronic_wgt\n",
    "\n",
    "    tot_exam_loss, tot_exam_wgts = tot_exam_loss.item(), tot_exam_wgts.item()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Image-level weighted log loss (single batch)\n",
    "    w_img = 0.07361963\n",
    "    tot_img_loss = 0\n",
    "    tot_img_wgts = 0\n",
    "    for img_preds, img_targs in zip(seq_cls_out, yb0):\n",
    "        mask = img_targs != targ_pad_idx\n",
    "        img_preds = img_preds[mask]\n",
    "        img_targs = img_targs[mask]\n",
    "\n",
    "        n_imgs = sum(mask)\n",
    "\n",
    "        qi = img_targs.float().mean()    \n",
    "        img_loss = bce_loss(img_preds, img_targs)    \n",
    "        wgt = w_img*qi\n",
    "        tot_img_wgts += (wgt).item()*n_imgs\n",
    "        tot_img_loss += (wgt*img_loss).item()*n_imgs\n",
    "\n",
    "    \n",
    "    return (tot_exam_loss + tot_img_loss) / (tot_exam_wgts + tot_img_wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad574d9",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75c1eb",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf18a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBlock(blocks=(SequenceBlock,SequenceTargetBlock,TargetBlock), \n",
    "                 n_inp=1, splitter=FuncSplitter(lambda o: o in valid_pids))\n",
    "dls = data.dataloaders(list(train_pids)+list(valid_pids), bs=64)\n",
    "model = MultiHeadedSequenceClassifier(dim=1024)\n",
    "loss_func = MultiLoss()\n",
    "learner = Learner(dls, model, loss_func=loss_func, metrics=[metric], cbs=[SaveModelCallback(fname=\"best_seqmodel\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBlock(blocks=(SequenceBlock,SequenceTargetBlock,TargetBlock), \n",
    "                 n_inp=1, splitter=FuncSplitter(lambda o: o in valid_pids))\n",
    "dls = data.dataloaders(list(train_pids)+list(valid_pids), bs=64)\n",
    "model = MultiHeadedSequenceClassifier(dim=1024)\n",
    "loss_func = MultiLoss()\n",
    "learner = Learner(dls, model, loss_func=loss_func, metrics=[metric], cbs=[SaveModelCallback(fname=\"best_seqmodel\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fd543",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598fab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_flat_cos(20, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e661f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_flat_cos(20, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(\"init_seq_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ef4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(\"init_seq_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6824b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export(\"best_seqmodel_export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f91803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export(\"best_seqmodel_export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b47fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcfd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
