{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fc9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6a915",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f30df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "data.dropna(subset=['keyword'], inplace=True)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f66142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(strategy='most_frequent')\n",
    "data['location']=imputer.fit_transform(data[['location']])\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ff638",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:,1:-1]\n",
    "Y=data.iloc[:,-1]\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd8d09",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4453fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[:10,2]\n",
    "for i in range(10):\n",
    "    print(X.iloc[i,2])\n",
    "    print()\n",
    "\n",
    "#remove tags\n",
    "#remove links\n",
    "#remove stopwords\n",
    "#remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cfaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "stop=stopwords.words('english')\n",
    "stop.remove('not')\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii','ignore').decode('ascii')\n",
    "def hasNumber(string):\n",
    "    return any(char.isdigit() for char in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ed7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "clean_text=[]\n",
    "for tweet,keyword,location in X.loc[:,[\"text\",\"keyword\",\"location\"]].values:\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) #remove links\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) #remove usernames\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet) #remove additional whitespaces\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #replace #word with word\n",
    "    tweet = tweet.strip('\\'\"') #trim tweet  \n",
    "    \n",
    "    words=\"\"\n",
    "    for word in tweet.split():\n",
    "        if(word.lower() not in stop):\n",
    "            word=deEmojify(word)\n",
    "            words+=(word.lower())+\" \"\n",
    "    words+keyword+\" \"+location\n",
    "    clean_text.append(words)\n",
    "print(clean_text[:10])\n",
    "X['text']=clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64e0e4",
   "metadata": {},
   "source": [
    "# Prepareing Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39058883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "vector=TfidfVectorizer(analyzer='word',max_features=5000,max_df=0.8)\n",
    "#vector = CountVectorizer(max_features=3000)\n",
    "X=vector.fit_transform(X[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# clf=SVC(kernel='linear')\n",
    "# clf.fit(X,Y)\n",
    "# clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b669ddc0",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann=tf.keras.Sequential()\n",
    "ann.add(tf.keras.layers.Dense(units=10,activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=10,activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=10,activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))\n",
    "ann.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06793aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dense = X.toarray() \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_dense=sc.fit_transform(X_dense)\n",
    "\n",
    "ann.fit(X_dense,Y,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfba3fd",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f888bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=data=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.isnull().sum())\n",
    "test['keyword'].fillna(\"\", inplace=True)\n",
    "test['location']=imputer.transform(test[['location']])\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd80e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabcf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "clean_text=[]\n",
    "for tweet,keyword,location in test.loc[:,[\"text\",\"keyword\",\"location\"]].values:\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) #remove links\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) #remove usernames\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet) #remove additional whitespaces\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #replace #word with word\n",
    "    tweet = tweet.strip('\\'\"') #trim tweet  \n",
    "    \n",
    "    words=\"\"\n",
    "    for word in tweet.split():\n",
    "        if(word.lower() not in stop):\n",
    "            word=deEmojify(word)\n",
    "            words+=(word.lower())+\" \"\n",
    "    words+keyword+\" \"+location\n",
    "    clean_text.append(words)\n",
    "print(clean_text[:10])\n",
    "test['text']=clean_text\n",
    "id=test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=vector.transform(test[\"text\"])\n",
    "test_dense=test.toarray()\n",
    "\n",
    "test_dense=sc.transform(test_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions=clf.predict(test)\n",
    "predictions = ann.predict(test_dense)\n",
    "predictions=predictions>0.5\n",
    "predictions = predictions.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'id': id, 'target': predictions.flatten()})\n",
    "submission_df.to_csv('/kaggle/working/output.csv',index=False)\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75999617",
   "metadata": {},
   "outputs": [],
   "source": [
    "review=\"so give me reasons to fill the hole and merge the space between , let the floods cross distance in your eyes\"\n",
    "vectorized=vector.transform([review])\n",
    "vectorized=vectorized.toarray()\n",
    "predicted=ann.predict(vectorized)\n",
    "print(predicted>0.5)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
