{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce2801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ee6fc",
   "metadata": {},
   "source": [
    "#My Fair Lady \"Just You Wait\" At the beginning everything sounds nice.\n",
    "\n",
    "\"Just you wait, Rishi Sunak, just you wait\n",
    "\n",
    "You'll be sorry but your tears will be too late\n",
    "\n",
    "You'll be broke and I'll have money\n",
    "\n",
    "Will I help you, don't be funny\n",
    "\n",
    "Just you wait, Rishi Sunak, just you wait\n",
    "\n",
    "Just you wait, Rishi Sunak 'till you're sick\n",
    "\n",
    "And you screams to fetch a doctor double quick\n",
    "\n",
    "I'll be off a second later\n",
    "\n",
    "And go straight to the theater\n",
    "\n",
    "Oh oh oh, Rishi Sunak, just you wait\n",
    "\n",
    "Ooh, Rishi Sunak\"\n",
    "\n",
    "Lyrics by  Alan Jay Lerner / Frederick Loewe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12814e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.style as style\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbac3a5",
   "metadata": {},
   "source": [
    "#Reading Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('../input/latest-elected-uk-prime-minister-rishi-sunak/uk_pm.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9484cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d1cf2",
   "metadata": {},
   "source": [
    "#Or you can have a csv version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/latest-elected-uk-prime-minister-rishi-sunak/uk_pm.csv\", delimiter=',', encoding='utf8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165bc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "print(len(train[train['likecount'] < 500]), 'tweets with less than 500 dislikes')\n",
    "print(len(train[train['likecount'] > 500]), 'tweets with more than 500 dislikes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a12131",
   "metadata": {},
   "source": [
    "#Tweet with more comments???? I'm making my Wine Reviews (aka drinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ed0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "# video with the most comments\n",
    "\n",
    "train[train['likecount'] == train['likecount'].max()]['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf5424",
   "metadata": {},
   "source": [
    "#We don't have DISLIKES yet? Just wait Prime Minister! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "def remove_line_breaks(text):\n",
    "    text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n",
    "    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "    '''Escape all the characters in pattern except ASCII letters and numbers'''\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_zero_punctuation = []\n",
    "    for token in tokens:\n",
    "        if not re_replacements.match(token):\n",
    "            token = re_punctuation.sub(\" \", token)\n",
    "        tokens_zero_punctuation.append(token)\n",
    "    return ' '.join(tokens_zero_punctuation)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def lowercase(text):\n",
    "    text_low = [token.lower() for token in word_tokenize(text)]\n",
    "    return ' '.join(text_low)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    text = \" \".join([word for word in word_tokens if word not in stop])\n",
    "    return text\n",
    "\n",
    "#remobe one character words\n",
    "def remove_one_character_words(text):\n",
    "    '''Remove words from dataset that contain only 1 character'''\n",
    "    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n",
    "    return ' '.join(text_high_use)   \n",
    "    \n",
    "#%%\n",
    "# Stemming with 'Snowball stemmer\" package\n",
    "def stem(text):\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n",
    "    return ' '.join(text_stemmed)\n",
    "\n",
    "def lemma(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n",
    "    return ' '.join(text_lemma)\n",
    "\n",
    "\n",
    "#break sentences to individual word list\n",
    "def sentence_word(text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    return word_tokens\n",
    "#break paragraphs to sentence token \n",
    "def paragraph_sentence(text):\n",
    "    sent_token = nltk.sent_tokenize(text)\n",
    "    return sent_token    \n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Return a list of words in a text.\"\"\"\n",
    "    return re.findall(r'\\w+', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    no_nums = re.sub(r'\\d+', '', text)\n",
    "    return ''.join(no_nums)\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    _steps = [\n",
    "    remove_line_breaks,\n",
    "    remove_one_character_words,\n",
    "    remove_special_characters,\n",
    "    lowercase,\n",
    "    remove_punctuation,\n",
    "    remove_stopwords,\n",
    "    stem,\n",
    "    remove_numbers\n",
    "]\n",
    "    for step in _steps:\n",
    "        text=step(text)\n",
    "    return text   \n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ac3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/55557004/getting-attributeerror-float-object-has-no-attribute-replace-error-while\n",
    "#To avoid with tqdm AttributeError: 'float' object has no attribute\n",
    "\n",
    "train[\"text\"] = train[\"text\"].astype(str)\n",
    "train[\"text\"] = [x.replace(':',' ') for x in train[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71efb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_text'] = pd.Series([clean_text(i) for i in tqdm(train['text'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a969500",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = train[\"clean_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63989b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "ls = []\n",
    "\n",
    "for i in words:\n",
    "    ls.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705bbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "# The wordcloud \n",
    "plt.figure(figsize=(16,13))\n",
    "wc = WordCloud(background_color=\"lightblue\", colormap='Set2', max_words=1000, max_font_size= 200,  width=1600, height=800)\n",
    "wc.generate(\" \".join(ls))\n",
    "plt.title(\"Most discussed terms\", fontsize=20)\n",
    "plt.imshow(wc.recolor( colormap= 'Set2' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed82619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "most_pop = train.sort_values('likecount', ascending =False)[['text', 'likecount']].head(12)\n",
    "\n",
    "most_pop['target1'] = most_pop['likecount']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d4aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "plt.figure(figsize = (30,35))\n",
    "\n",
    "sns.barplot(data = most_pop, y = 'text', x = 'target1', color = 'c')\n",
    "plt.xticks(fontsize=50, rotation=0)\n",
    "plt.yticks(fontsize=50, rotation=0)\n",
    "plt.xlabel('Votes in Thousands', fontsize = 21)\n",
    "plt.ylabel('')\n",
    "plt.title('Most popular tweets', fontsize = 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a793f54",
   "metadata": {},
   "source": [
    "#I can't even read anything. Just: \"Congratulations Rishi Sunak.\" Just Wait PM Sunak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8999ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3899678",
   "metadata": {},
   "source": [
    "#At the beginning, everything is so nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748ac8d",
   "metadata": {},
   "source": [
    "#I ran the snippet below to avoid error on the next snippet (after that one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12171da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f36855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "doc_sample = train['text'].iloc[1]\n",
    "print('original document: ')\n",
    "\n",
    "words = []\n",
    "\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "    \n",
    "    \n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac83866",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9aecfd",
   "metadata": {},
   "source": [
    "#Above it was suppose to be \"clean_text\"  Though I got an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "words = []\n",
    "\n",
    "for i in train['text']:\n",
    "        words.append(i.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(words)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in words]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c572a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c702b",
   "metadata": {},
   "source": [
    "#What's Obama doing here??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03388e",
   "metadata": {},
   "source": [
    "#TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d296dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                       num_topics=10,\n",
    "                                       id2word=dictionary,\n",
    "                                       passes=2,\n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b23d1d",
   "metadata": {},
   "source": [
    "#Show the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63014fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n",
    "                                             num_topics=10,\n",
    "                                             id2word=dictionary,\n",
    "                                             passes=2,\n",
    "                                             workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3cc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = round(train.corr(method='pearson'),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(corrmat.iloc[1:2,:25], vmax=1.0, vmin=-1.0, square=True, annot=True, cmap='summer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bloxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bloxs import B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown\n",
    "\n",
    "display_markdown('''#Warmest congratulations \n",
    "''', raw=True)\n",
    "B([\n",
    "    B(\"üëçüëãüëã\", \"Warmest congratulations @RishiSunak\"),\n",
    "    B(\"üëÄüôè‚òùÔ∏è\", \"Looking forward to work together on global issues, and implementing Roadmap 2030\"),\n",
    "    B(\"üôè‚úîÔ∏èüòÉ\", \"Special Diwali wishes to the 'living bridge' of UK Indians\"),\n",
    "    B(\"üéâ‚ù§Ô∏èüëç\", \"As we transform our historic ties into a modern partnership\")\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b428f1cb",
   "metadata": {},
   "source": [
    "#Acknowledgements:\n",
    "\n",
    "Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n",
    "\n",
    "Bloxs by MLJAR - Maintainer: PPlonski (Piotr)- https://pypi.org/project/bloxs/\n",
    "\n",
    "Kalilur Rahman https://www.kaggle.com/code/kalilurrahman/kaggle-2022-mlds-analysis-summary"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
