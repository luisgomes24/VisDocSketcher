{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5071ec7b",
   "metadata": {},
   "source": [
    "# OVERVIEW OF DATASET\n",
    "**Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in these requires the availability of contextual tweets.**\n",
    "\n",
    "**To overcome the limitations related to noise in Twitter datasets, this News Headlines dataset for Sarcasm Detection is collected from two news website. TheOnion aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). We collect real (and non-sarcastic) news headlines from HuffPost.**\n",
    "\n",
    "**The dataset consists about 28000 text data points where each data category belongs to 2 category - Sarcastic or Not Sarcastic**\n",
    "\n",
    "**We will use two models for making predictions - Word2Vec and GloVe Embeddings. We will then compare their results and see which performs better**\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0f97c3",
   "metadata": {},
   "source": [
    "# LOADING THE NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458de5b9",
   "metadata": {},
   "source": [
    "# LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a614bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634ea90",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35877a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # Checking for NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['article_link'] # Deleting this column as it is of no use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "sns.countplot(df.is_sarcastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcaea1e",
   "metadata": {},
   "source": [
    "**SO, WE CAN SEE THAT THE DATASET IS BALANCED**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85854b77",
   "metadata": {},
   "source": [
    "**WHAT ARE STOPWORDS?**\n",
    "\n",
    "**Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f2defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bcab61",
   "metadata": {},
   "source": [
    "**BASIC DATA CLEANING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df['headline']=df['headline'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc584a8",
   "metadata": {},
   "source": [
    "**WORDCLOUD FOR TEXT THAT IS NOT SARCASTIC (LABEL - 0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091c164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) # Text that is Not Sarcastic\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 0].headline))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5436c4a",
   "metadata": {},
   "source": [
    "**WORDCLOUD FOR SARCASTIC TEXT (LABEL - 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) # Text that is Sarcastic\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 1].headline))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839f7c1",
   "metadata": {},
   "source": [
    "**Number of characters in texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c332ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "text_len=df[df['is_sarcastic']==1]['headline'].str.len()\n",
    "ax1.hist(text_len,color='red')\n",
    "ax1.set_title('Sarcastic text')\n",
    "text_len=df[df['is_sarcastic']==0]['headline'].str.len()\n",
    "ax2.hist(text_len,color='green')\n",
    "ax2.set_title('Not Sarcastic text')\n",
    "fig.suptitle('Characters in texts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb4cf1",
   "metadata": {},
   "source": [
    "**Number of words in each text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "text_len=df[df['is_sarcastic']==1]['headline'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(text_len,color='red')\n",
    "ax1.set_title('Sarcastic text')\n",
    "text_len=df[df['is_sarcastic']==0]['headline'].str.split().map(lambda x: len(x))\n",
    "ax2.hist(text_len,color='green')\n",
    "ax2.set_title('Not Sarcastic text')\n",
    "fig.suptitle('Words in texts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ac962",
   "metadata": {},
   "source": [
    "**Average word length in a text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\n",
    "word=df[df['is_sarcastic']==1]['headline'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\n",
    "ax1.set_title('Sarcastic text')\n",
    "word=df[df['is_sarcastic']==0]['headline'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\n",
    "ax2.set_title('Not Sarcastic text')\n",
    "fig.suptitle('Average word length in each text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66593b1c",
   "metadata": {},
   "source": [
    "# Introduction to Word Embedding and Word2Vec\n",
    "**Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "What are word embeddings exactly? Loosely speaking, they are vector representations of a particular word. Having said this, what follows is how do we generate them? More importantly, how do they capture the context?\n",
    "Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d479a",
   "metadata": {},
   "source": [
    "# Why do we need them?\n",
    "**Consider the following similar sentences: Have a good day and Have a great day. They hardly have different meaning. If we construct an exhaustive vocabulary (let’s call it V), it would have V = {Have, a, good, great, day}.\n",
    "Now, let us create a one-hot encoded vector for each of these words in V. Length of our one-hot encoded vector would be equal to the size of V (=5). We would have a vector of zeros except for the element at the index representing the corresponding word in the vocabulary. That particular element would be one. The encodings below would explain this better.\n",
    "Have = [1,0,0,0,0] ; a=[1,0,0,0,0] ; good=[0,0,1,0,0] ; great=[0,0,0,1,0] ; day=[0,0,0,0,1] (represents transpose)\n",
    "If we try to visualize these encodings, we can think of a 5 dimensional space, where each word occupies one of the dimensions and has nothing to do with the rest (no projection along the other dimensions). This means ‘good’ and ‘great’ are as different as ‘day’ and ‘have’, which is not true.**\n",
    "**Our objective is to have words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0.**\n",
    "![image.png](attachment:image.png)\n",
    "**Here comes the idea of generating distributed representations. Intuitively, we introduce some dependence of one word on the other words. The words in context of this word would get a greater share of this dependence. In one hot encoding representations, all the words are independent of each other, as mentioned earlier.**\n",
    "\n",
    "**Source Credits : https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4307a9",
   "metadata": {},
   "source": [
    "**Converting text to format acceptable by gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46997d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in df.headline.values:\n",
    "    words.append(i.split())\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c36368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#Dimension of vectors we are generating\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "#Creating Word Vectors by Word2Vec Method (takes time...)\n",
    "w2v_model = gensim.models.Word2Vec(sentences = words , size=EMBEDDING_DIM , window = 5 , min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab size\n",
    "len(w2v_model.wv.vocab)\n",
    "#We have now represented each of 38071 words by a 100dim vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82904834",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=35000)\n",
    "tokenizer.fit_on_texts(words)\n",
    "tokenized_train = tokenizer.texts_to_sequences(words)\n",
    "x = sequence.pad_sequences(tokenized_train, maxlen = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12757a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "# Embedding Layer creates one more vector for \"UNKNOWN\" words, or padded words (0s). This Vector is filled with zeros.\n",
    "# Thus our vocab size inceeases by 1\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create weight matrix from word2vec gensim model\n",
    "def get_weight_matrix(model, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = model[word]\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0af59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\n",
    "embedding_vectors = get_weight_matrix(w2v_model, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad62f9a",
   "metadata": {},
   "source": [
    "# TRAINING WORD2VEC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7fd49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=20, trainable=True))\n",
    "#LSTM \n",
    "model.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.3 , dropout = 0.3,return_sequences = True)))\n",
    "model.add(Bidirectional(GRU(units=32 , recurrent_dropout = 0.1 , dropout = 0.1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "del embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, df.is_sarcastic , test_size = 0.3 , random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102176bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 128 , validation_data = (x_test,y_test) , epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100)\n",
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c12331",
   "metadata": {},
   "source": [
    "# ANALYSIS AFTER TRAINING OF WORD2VEC MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(3)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['acc']\n",
    "train_loss = history.history['loss']\n",
    "val_acc = history.history['val_acc']\n",
    "val_loss = history.history['val_loss']\n",
    "fig.set_size_inches(20,10)\n",
    "\n",
    "ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
    "ax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\n",
    "ax[0].set_title('Training & Testing Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\n",
    "ax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\n",
    "ax[1].set_title('Training & Testing Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec4ed24",
   "metadata": {},
   "source": [
    "**SEEMS LIKE THE MODEL IS OVERFITTING AND NOT PERFORMING WELL ON THE TEST DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6865cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(x_test)\n",
    "pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35acd518",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f1212",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(cm , index = ['Not Sarcastic','Sarcastic'] , columns = ['Not Sarcastic','Sarcastic'])\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Not Sarcastic','Sarcastic'] , yticklabels = ['Not Sarcastic','Sarcastic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e971140",
   "metadata": {},
   "source": [
    "# Introduction to GloVe\n",
    "**GloVe method is built on an important idea, You can derive semantic relationships between words from the co-occurrence matrix. Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.**\n",
    "![image.png](attachment:image.png)\n",
    "**The co-occurrence matrix for the sentence “the cat sat on the mat” with a window size of 1. As you probably noticed it is a symmetric matrix. How do we get a metric that measures semantic similarity between words from this? For that, you will need three words at a time. Let me concretely lay down this statement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40451108",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "**The behavior of P_ik/P_jk for various words Consider the entity P_ik/P_jk where P_ik = X_ik/X_i Here P_ik denotes the probability of seeing word i and k together, which is computed by dividing the number of times i and k appeared together (X_ik) by the total number of times word i appeared in the corpus (X_i). You can see that given two words, i.e. ice and steam, if the third word k (also called the “probe word”), is very similar to ice but irrelevant to steam (e.g. k=solid), P_ik/P_jk will be very high (>1), is very similar to steam but irrelevant to ice (e.g. k=gas), P_ik/P_jk will be very small (<1), is related or unrelated to either words, then P_ik/P_jk will be close to 1 So, if we can find a way to incorporate P_ik/P_jk to computing word vectors we will be achieving the goal of using global statistics when learning word vectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbee3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(df.headline,df.is_sarcastic, test_size = 0.3 , random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 35000\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db8285",
   "metadata": {},
   "source": [
    "**Tokenizing Text -> Repsesenting each word by a number**\n",
    "\n",
    "**Mapping of orginal word to number is preserved in word_index property of tokenizer**\n",
    "\n",
    "**Tokenized applies basic processing like changing it to lower case, explicitely setting that as False**\n",
    "\n",
    "**Lets keep all news to 200, add padding to news with less than 200 words and truncating long ones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c81d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf855e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../input/glove-twitter/glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "#change below line if computing normal stats is too slow\n",
    "embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2571127",
   "metadata": {},
   "source": [
    "**BASIC MODEL PARAMETERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 2\n",
    "embed_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de6ad2",
   "metadata": {},
   "source": [
    "# TRAINING GLOVE EMBEDDINGS MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(nb_words, output_dim=embed_size, weights=[embedding_matrix], input_length=200, trainable=True))\n",
    "#LSTM \n",
    "model.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.5 , dropout = 0.5)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfb308",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ab994",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c9634",
   "metadata": {},
   "source": [
    "**THE ACCURACY IMPROVED FROM 79% TO 83%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ebe5f",
   "metadata": {},
   "source": [
    "# ANALYSIS AFTER TRAINING OF GLOVE EMBEDDINGS MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test)\n",
    "pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde06232",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred, target_names = ['Not Sarcastic','Sarcastic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cce0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf01e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(cm , index = ['Not Sarcastic','Sarcastic'] , columns = ['Not Sarcastic','Sarcastic'])\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Not Sarcastic','Sarcastic'] , yticklabels = ['Not Sarcastic','Sarcastic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b71e1",
   "metadata": {},
   "source": [
    "**PLS UPVOTE THIS NOTEBOOK IF YOU LIKE IT. THANKS FOR YOUR TIME!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d76aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
