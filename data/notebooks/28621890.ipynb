{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0464e1b5",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ec0a0",
   "metadata": {},
   "source": [
    "# Competition goal\n",
    "\n",
    "* Classifiy the etiology (origin) of a blood clot shown in a tissue slice of a patient that has experienced an acute ischemic stroke. \n",
    "* It's a **binary classification task** with labels CE (cardioembolic) or LAA (Large Artery Atherosclerosis)\n",
    "* There are supplemental slides with a either an **unknown etiology or an etiology other than CE or LAA**! \n",
    "\n",
    "Aha! ... :-O Do you know the difference between CE and LAA or what is meant by an acute ischemic stroke? ... Neither do I! ;-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeded3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<iframe width=\"800\" height=\"444\" src=\"https://www.youtube.com/embed/abxcrAvw-O0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651897a",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Prepare to start](#prepare)\n",
    "2. [Exploratory analysis - EDA](#eda)\n",
    "    * [What do we know about the training data?](#train_data)\n",
    "3. [The nightmare of overfitting](#nightmare)\n",
    "4. [Tell me what do you see?](#explain)\n",
    "    * [Choosing a model](#choose_model)\n",
    "5. [Explaining predictions with LIME](#lime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b6ec7",
   "metadata": {},
   "source": [
    "# Prepare to start <a class=\"anchor\" id=\"prepare\"></a>\n",
    "\n",
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d83124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from lime import lime_image\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6aa3e",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eae57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/mayo-clinic-strip-ai/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5991c8",
   "metadata": {},
   "source": [
    "# Exploratory analysis <a class=\"anchor\" id=\"eda\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d27db5",
   "metadata": {},
   "source": [
    "## What do we know about our training data? <a class=\"anchor\" id=\"train_data\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c43b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7f221",
   "metadata": {},
   "source": [
    "* The **image_id** consists of two different parts: *{patient_id}_{image_number}*\n",
    "* The **center_id** defines the medical center or institute where the image was taken\n",
    "* **patient_id** is the id of the patient\n",
    "* **image_num** is a counter for slices of a patient\n",
    "* **label** is our target\n",
    "    * CE: cardioembolic\n",
    "    * LAA: Large artery atherosclerosis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6dea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.patient_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234f6f0",
   "metadata": {},
   "source": [
    "632 patients in train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(train.groupby(\"patient_id\").image_num.size(), palette=\"Greens_r\")\n",
    "plt.xlabel(\"Number of images per patient\")\n",
    "plt.title(\"Max image number per patient in train\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79369e2",
   "metadata": {},
   "source": [
    "We can clearly see that most patients only have one slice in the training data and only a few show more than one. Maybe these are hard cases where it's more difficult to see the etiology?! Or does it depend on the medical center?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"patient_id\").center_id.nunique().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0886ff",
   "metadata": {},
   "source": [
    "Ok, all patients belong to one center. There is no patient that shows images that were taken in different medial centers. How many do we have and how many patients do we have per center?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"center_id\").patient_id.size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e66e8",
   "metadata": {},
   "source": [
    "Definitely not balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e141f",
   "metadata": {},
   "source": [
    "Let's take a look at the target distribution! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db995239",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train.label, palette=\"Reds_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c13a80",
   "metadata": {},
   "source": [
    "Ok, it's an imbalanced classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c510f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.label.value_counts() / train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f59e04",
   "metadata": {},
   "source": [
    "Ok we know that we have some patients with more than one slice. Is it possible they show more than one label per patient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a912840",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"patient_id\").label.nunique().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9742f",
   "metadata": {},
   "source": [
    "No! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/mayo-clinic-strip-ai/test.csv\")\n",
    "test[\"image_path\"] = test[\"image_id\"].apply(lambda x: os.path.join(\"../input/mayo-clinic-strip-ai/test/\", x+\".tif\"))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc969b9a",
   "metadata": {},
   "source": [
    "## The nightmare of overfitting ;-) <a class=\"anchor\" id=\"nightmare\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a4b23",
   "metadata": {},
   "source": [
    "Can't wait anymore! Let's start to discover the images! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ded74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"../input/mayo-clinic-strip-ai/train/006388_0.tif\"\n",
    "img = Image.open(example)\n",
    "img.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c2232",
   "metadata": {},
   "source": [
    "Uff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eedd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = img.size[0]\n",
    "x2 = img.size[1]\n",
    "sc = x2/x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1/10\n",
    "resized_img = img.resize((int(x1*factor), int(sc*x1*factor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59826ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ab7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img.rotate(90, expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5a08f",
   "metadata": {},
   "source": [
    "Oh this will be a nightmare for validation.... such a complex feature space... I can't see the blood clot, do you? How should we prevent our model from overfitting? Seems hopeless. Now, imagine that medical centers might also use different methods or protocols to obtain these slices... ohoh! The number of patients given such a high feature space is low and the public test data seems to be quite small as well. Hmm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78bbdd",
   "metadata": {},
   "source": [
    "# Tell me, what do you see? <a class=\"anchor\" id=\"explain\"></a>\n",
    "\n",
    "Ok, so far I can't imagine that any model will be able to see something useful in this complex feature space... but let's better try to understand and explain what our models \"see\". For this purpose I like to use and learn more about some machine learning explainability tools for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3742a6a",
   "metadata": {},
   "source": [
    "## What is meant by Machine Learning Explainability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a3eb8",
   "metadata": {},
   "source": [
    "## Choosing a model <a class=\"anchor\" id=\"choose_model\"></a>\n",
    "\n",
    "\n",
    "Let's pick one of the best scoring public models so far with the hope that it has found an interesting and useful signal. At the moment it's this one:\n",
    "\n",
    "* https://www.kaggle.com/code/realneuralnetwork/cnn-strip-ai-inference/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fcfabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df \n",
    "        self.train = 'label' in df.columns\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        paths = [\"../input/jpg-images-strip-ai/test/\", \"../input/jpg-images-strip-ai/train/\"]\n",
    "        try:\n",
    "            image = cv2.imread(paths[self.train] + self.df.iloc[index].image_id + \".jpg\")\n",
    "        except:\n",
    "            image = np.zeros((512,512,3), np.uint8)\n",
    "        label = 0\n",
    "        try:\n",
    "            if len(image.shape) == 5:\n",
    "                image = image.squeeze().transpose(1, 2, 0)\n",
    "            image = cv2.resize(image, (512, 512)).transpose(2, 0, 1)\n",
    "        except:\n",
    "            image = np.zeros((3, 512, 512))\n",
    "        if(self.train):\n",
    "            label = {\"CE\" : 0, \"LAA\": 1}[self.df.iloc[index].label]\n",
    "        patient_id = self.df.iloc[index].patient_id\n",
    "        return image, label, patient_id\n",
    "    \n",
    "    \n",
    "def predict(model, dataloader):\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    dataloader = dataloader\n",
    "    outputs = []\n",
    "    s = nn.Softmax(dim=1)\n",
    "    ids = []\n",
    "    for item in tqdm(dataloader, leave=False):\n",
    "        patient_id = item[2][0]\n",
    "        try:\n",
    "            images = item[0].cuda().float()\n",
    "            ids.append(patient_id)\n",
    "            output = model(images)\n",
    "            outputs.append(s(output.cpu()[:,:2])[0].detach().numpy())\n",
    "        except:\n",
    "            ids.append(patient_id)\n",
    "            outputs.append(s(torch.tensor([[1, 1]]).float())[0].detach().numpy())\n",
    "    return np.array(outputs), ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load('../input/strip-ai-models-to-explain/kabir_ivan_cnn_strip_ai_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "test_loader = DataLoader(\n",
    "    ImgDataset(test.iloc[0:4]), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "anss, ids = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707eeeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "anss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95deb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = pd.DataFrame({\"CE\" : anss[:,0], \"LAA\" : anss[:,1], \"id\" : ids}).groupby(\"id\").mean()\n",
    "submission = pd.read_csv(\"../input/mayo-clinic-strip-ai/sample_submission.csv\")\n",
    "submission.CE = prob.CE.to_list()\n",
    "submission.LAA = prob.LAA.to_list()\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f9841",
   "metadata": {},
   "source": [
    "# Explaining predictions with LIME <a class=\"anchor\" id=\"lime\"></a>\n",
    "\n",
    "LIME stands for Local Interpretable Model-Agnoistic Explanations. You can read the paper published 2016 [here](https://arxiv.org/pdf/1602.04938.pdf). I know that there exist newer tools but I think it can be a good starting point to get started with the topic. ;-) Here is a great video to introduce LIME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<iframe width=\"708\" height=\"400\" src=\"https://www.youtube.com/embed/hUnRCxnydCc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ee651",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "Let's collect a few key messages of the video and the paper:\n",
    "\n",
    "* LIME explains the predictions of any classifer (model) by treating it as a black box.\n",
    "* It trains an interpretable model locally around the prediction.\n",
    "* It helps to decide whether to trust a prediction or a model.\n",
    "* We need to keep in mind that the performance measured by your evaluation metric might not suite to the performance of our model on real-world data (pulic/private test data here) or even with good performance it can happen that your predictions doesn't make sense. \n",
    "* The explanation we get with LIME has to be simple enough for a human to understand regardless how many features were used by a model to make its prediction.\n",
    "* For an image classifier an interpretable representation can be a binary vector that indicates a presence of absence of a contiguous patch of similar pixels (super-pixel).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b79932",
   "metadata": {},
   "source": [
    "Ok let's start! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(images):\n",
    "    s = nn.Softmax(dim=1)\n",
    "    images = images.transpose((0,3,1,2))\n",
    "    images = torch.tensor(images).cuda().float()\n",
    "    outputs = model(images)\n",
    "    preds = s(outputs.cpu()[:,:2]).detach().numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "test_loader = DataLoader(\n",
    "    ImgDataset(test.iloc[0:2]), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=1\n",
    ")\n",
    "test_iterator = iter(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159b639",
   "metadata": {},
   "source": [
    "As we have only two classes we can set top_labels with the value 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "model.eval()\n",
    "batch = next(test_iterator)\n",
    "patient_ids = batch[2]\n",
    "image = batch[0].cpu().detach()\n",
    "lm_image = image.squeeze().numpy().transpose((1,2,0)) \n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "\n",
    "explanation = explainer.explain_instance(lm_image, \n",
    "                                     batch_predict, \n",
    "                                     top_labels=2, \n",
    "                                     hide_color=0, batch_size=1)\n",
    "output = model(image.cuda().float())\n",
    "s = nn.Softmax(dim=1)\n",
    "probas = s(output[:,:2])[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f13e6",
   "metadata": {},
   "source": [
    "We can see that the probability of the first label (CE) is higher than for the second one (LAA). To understand which parts of the image caused the model to make this prediction, we can take a look at the superpixels for the top_label (CE in this case). The tissues of these images are very detailed and I expect that the signals of the origin of the stroke are very tiny. For this reason I like to choose a high number of superpixels given by the num_features attribute. We only want to see which superpixels contribute to the top_label prediction (CE) and hide the rest of the image. And this is what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=100, hide_rest=True)\n",
    "img_boundry1 = mark_boundaries(temp/255.0, mask)\n",
    "\n",
    "\n",
    "images = batch[0].cpu().detach().numpy()\n",
    "print(images.shape)\n",
    "images = images.transpose((0,2,3,1))\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
    "ax[0].imshow(images[0])\n",
    "\n",
    "ax[0].set_title(\"Patient {}\".format(patient_ids[0]))\n",
    "ax[1].imshow(img_boundry1)\n",
    "\n",
    "for n in range(2):\n",
    "    ax[n].axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e2655",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "* We can't trust our model! Do you see it? We can clearly see that empty superpixels contributed to the top_label prediction. This doesn't make sense! \n",
    "* There are also a few regions that belong to tissue but so far it's not clear why. It could be the color or the way the image was streched or distorted during preprocessing... who knows?! \n",
    "\n",
    "In my opinion we should try to understand the problem better. Can we really see differenes in these tissues given the origin of the blood clot after an acute ischemic stroke? How do medical experts perform this task? Do they have more information available when they need to make a choice?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
