{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d785c3",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd8a1b",
   "metadata": {},
   "source": [
    "**In my earlier discussion post titled [DRN — Dilated Residual Networks (Image Classification & Semantic Segmentation)](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/111546) ** a kaggler name Qishen Ha (@haqishen) said \"If you want to try out dilated convolution, Deeplabv3 is a good sample to learn\" :) so i decided to make this kernel for my fellow kaggle mates and also self teaching deeplabv3 a little bit :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026ac0f",
   "metadata": {},
   "source": [
    "**Note : i am completely new in deep learning world and has very little knowledge about this field,if you find any implementation error or bug please let me know in the comment box and it will be highly appreciated**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834ea25",
   "metadata": {},
   "source": [
    "#  Algorithms for Image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34b7ab",
   "metadata": {},
   "source": [
    "Image segmentation is a long standing computer Vision problem. Quite a few algorithms have been designed to solve this task, such as the Watershed algorithm, Image thresholding , K-means clustering, Graph partitioning methods, etc.\n",
    "\n",
    "Many deep learning architectures (like fully connected networks for image segmentation) have also been proposed, but Google’s DeepLab model has given the best results till date. That’s why we’ll focus on using DeepLab.DeepLab uses atrous convolution with rates 6, 12 and 18."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b1fc1",
   "metadata": {},
   "source": [
    "# Why using DEEPLABV3-RESNET101?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815839c",
   "metadata": {},
   "source": [
    "*For the semantic segmentation model, GluonCV-Torch mainly supports pre-trained FCN, PSPNet and DeepLab-V3. DeepLab-V3 is a very common open source model, which has very good effect on semantic segmentation tasks. The pre-training effects of these three models in the Pascal VOC dataset are shown below, where Pascal VOC contains 20 categories of images:*\n",
    "\n",
    "![](http://www.programmersought.com/images/427/d258324e597cccec479b706fa7a2a04b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0635d",
   "metadata": {},
   "source": [
    "**The following shows the effects of three semantic segmentation models in the ADE20K dataset, where ADE20K is a scene resolution dataset published by MIT that contains a variety of scenarios, including people, backgrounds, and objects.**\n",
    "\n",
    "![](http://www.programmersought.com/images/928/522173f7eab9d4d27b9bbdd0b833fde8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5237c14",
   "metadata": {},
   "source": [
    "*  Source of informations above : http://www.programmersought.com/article/5710352893/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52278d4",
   "metadata": {},
   "source": [
    "*  <font size=\"4\" color=\"green\">Google's Deeplabv3 uses dilated convolution,lets first talk little bit about dilated convolutions!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960791cb",
   "metadata": {},
   "source": [
    "# Dilated Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daa6697",
   "metadata": {},
   "source": [
    " equation of DilatedNet:\n",
    " \n",
    "![](https://miro.medium.com/proxy/1*mlHFvK6H_wMCyURSZNZWGQ.png) \n",
    "\n",
    "\n",
    "The left one is the standard convolution. The right one is the dilated convolution. We can see that at the summation, it is s+lt=p that we will skip some points during convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c593c40",
   "metadata": {},
   "source": [
    "1. When l=1, it is standard convolution\n",
    "\n",
    "2. When l>1, it is dilated convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa7fbe",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1185/1*btockft7dtKyzwXqfq70_w.gif)\n",
    "                  **Standard Convolution (l=1) (Left) Dilated Convolution (l=2) (Right)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35555a25",
   "metadata": {},
   "source": [
    "* The above illustrate an example of dilated convolution when l=2. We can see that the receptive field is larger compared with the standard one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9feefd",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/proxy/1*tnDNIyPePgHvb8JIx8SbqA.png)\n",
    "\n",
    "**l=1 (left), l=2 (Middle), l=4 (Right)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298e559",
   "metadata": {},
   "source": [
    " The above figure shows more examples about the receptive field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f3321",
   "metadata": {},
   "source": [
    "# Reasons of Dilated Convolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a242337",
   "metadata": {},
   "source": [
    "1. It is found that with small output feature map obtained at the end of the network, the accuracy is reduced in semantic segmentation.\n",
    "\n",
    "2. In FCN, it also shows that when 32× upsampling is needed, we can only get a very rough segmentation results. Thus, a larger output feature map is desired.\n",
    "\n",
    "3. A naive approach is to simply remove subsampling (striding) steps in the network in order to increase the resolution of feature map. However, this also reduces the receptive field which severely reduces the amount of context. such reduction in receptive field is an unacceptable price to pay for higher resolution.\n",
    "\n",
    "4. For this reason, dilated convolutions are used to increase the receptive field of the higher layers, compensating for the reduction in receptive field induced by removing subsampling.\n",
    "\n",
    "5. And it is found that using dilated convolution can also help for image classification task in this paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96273b49",
   "metadata": {},
   "source": [
    "## Dilated Residual Networks (DRN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd329e7",
   "metadata": {},
   "source": [
    "1. In the paper(mentioned below), it uses d as dilation factor.\n",
    "\n",
    "2. When d=1, it is standard convolution.\n",
    "\n",
    "3. When d>1, it is dilated convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dcb20",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/794/1*-67TMJkhBO3sTtzAg2oUHg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0040f92",
   "metadata": {},
   "source": [
    "SOURCES : \n",
    "1. https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5\n",
    "2. https://towardsdatascience.com/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de15ad",
   "metadata": {},
   "source": [
    "I  highly recommend you to check this link : [Semantic Segmentation: Introduction to the Deep Learning Technique Behind Google Pixel’s Camera!](https://www.analyticsvidhya.com/blog/2019/02/tutorial-semantic-segmentation-google-deeplab/)\n",
    "i have used some of the contents from that link,definitely a great resource for understanding deeplabv3 well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6e6af",
   "metadata": {},
   "source": [
    "<font size=\"6\" color=\"red\">Please UPVOTE if you find this kernel Helpful!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a414f85",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db3d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import torch.distributed as dist\n",
    "import errno\n",
    "from fastai import metrics\n",
    "\n",
    "import cv2\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch.utils.data\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2\n",
    "import pdb\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from matplotlib import pyplot as plt\n",
    "from albumentations import (HorizontalFlip,VerticalFlip,Cutout,SmallestMaxSize,\n",
    "                            ToGray, ShiftScaleRotate, Blur,Normalize, Resize, Compose, GaussNoise)\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.data as utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01436194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(f'Python version: {platform.python_version()}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea511f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=43):\n",
    "    '''\n",
    "      Make PyTorch deterministic.\n",
    "    '''    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "IS_DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d889027",
   "metadata": {},
   "source": [
    "**Utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02554195",
   "metadata": {},
   "source": [
    "**Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    " \n",
    "    def forward(self, logits, targets):\n",
    "        ''' fastai.metrics.dice uses argmax() which is not differentiable, so it \n",
    "          can NOT be used in training, however it can be used in prediction.\n",
    "          see https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L53\n",
    "        '''\n",
    "        N = targets.size(0)\n",
    "        preds = torch.sigmoid(logits)\n",
    "        #preds = logits.argmax(dim=1) # do NOT use argmax in training, because it is NOT differentiable\n",
    "        # https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/keras/backend.py#L96\n",
    "        EPSILON = 1e-7\n",
    " \n",
    "        preds_flat = preds.view(N, -1)\n",
    "        targets_flat = targets.view(N, -1)\n",
    " \n",
    "        intersection = (preds_flat * targets_flat).sum()#.float()\n",
    "        union = (preds_flat + targets_flat).sum()#.float()\n",
    "        \n",
    "        loss = (2.0 * intersection + EPSILON) / (union + EPSILON)\n",
    "        loss = 1 - loss / N\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e075a",
   "metadata": {},
   "source": [
    "**Function for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    loss_func = DiceLoss() #nn.BCEWithLogitsLoss() \n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    lossf=None\n",
    "    inner_tq = tqdm(data_loader, total=len(data_loader), leave=False, desc= f'Iteration {epoch}')\n",
    "    for images, masks in inner_tq:\n",
    "        y_preds = model(images.to(device))\n",
    "        y_preds = y_preds['out'][:, 1, :, :] #\n",
    "\n",
    "        loss = loss_func(y_preds, masks.to(device))\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        if lossf:\n",
    "            lossf = 0.98*lossf+0.02*loss.item()\n",
    "        else:\n",
    "            lossf = loss.item()\n",
    "        inner_tq.set_postfix(loss = lossf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fdda9",
   "metadata": {},
   "source": [
    "**Mask Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116fc750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle2mask(rle, imgshape):\n",
    "    width = imgshape[0]\n",
    "    height= imgshape[1]\n",
    "    \n",
    "    mask= np.zeros( width*height ).astype(np.uint8)\n",
    "    \n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]\n",
    "    lengths = array[1::2]\n",
    "\n",
    "    current_position = 0\n",
    "    for index, start in enumerate(starts):\n",
    "        mask[int(start):int(start+lengths[index])] = 1\n",
    "        current_position += lengths[index]\n",
    "        \n",
    "    return np.flipud( np.rot90( mask.reshape(height, width), k=1 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2dba3d",
   "metadata": {},
   "source": [
    "**Steel Dataset paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5626d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('../input/severstal-steel-defect-detection/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13049e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '../input/severstal-steel-defect-detection/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ecf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv(path + 'train.csv')\n",
    "print(len(tr))\n",
    "tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01939518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tr[tr['EncodedPixels'].notnull()].reset_index(drop=True)\n",
    "\n",
    "#df_train1 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '1')].reset_index(drop=True)\n",
    "#df_train2 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '2')].reset_index(drop=True)\n",
    "#df_train3 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '3')].reset_index(drop=True)\n",
    "#df_train4 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '4')].reset_index(drop=True)\n",
    "\n",
    "#df_train = tr[tr['EncodedPixels']].reset_index(drop=True)\n",
    "#df_train = tr\n",
    "print(len(df_train))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029978f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb90cb",
   "metadata": {},
   "source": [
    "**Steel DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c52431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageData(Dataset):\n",
    "    def __init__(self, df, transform, subset=\"train\"):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.subset = subset\n",
    "        \n",
    "        if self.subset == \"train\":\n",
    "            self.data_path = path + 'train_images/'\n",
    "        elif self.subset == \"test\":\n",
    "            self.data_path = path + 'test_images/'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):                      \n",
    "        fn = self.df['ImageId_ClassId'].iloc[index].split('_')[0]         \n",
    "        img = Image.open(self.data_path + fn)\n",
    "        img = self.transform(img)\n",
    "\n",
    "        if self.subset == 'train': \n",
    "            mask = rle2mask(self.df['EncodedPixels'].iloc[index], (256, 1600))\n",
    "            mask = transforms.ToPILImage()(mask)            \n",
    "            mask = self.transform(mask)\n",
    "            return img, mask\n",
    "        else: \n",
    "            mask = None\n",
    "            return img  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transf = transforms.Compose([\n",
    "                                  transforms.Scale((256, 1600)),\n",
    "                                  #HorizontalFlip(p=0.5),\n",
    "                                  #VerticalFlip(p = 0.5),\n",
    "                                  #Blur(),\n",
    "                                  #Cutout(),\n",
    "                                  #ShiftScaleRotate(),\n",
    "                                  #GaussNoise(),\n",
    "                                  #ToGray(),\n",
    "                                  transforms.ToTensor()])\n",
    "train_data = ImageData(df = df_train, transform = data_transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88582d57",
   "metadata": {},
   "source": [
    "# Understanding the DeepLab Model Architecture\n",
    "\n",
    "DeepLab V3 uses ImageNet’s pretrained Resnet-101 with atrous convolutions as its main feature extractor. In the modified ResNet model, the last ResNet block uses atrous convolutions with different dilation rates. It uses Atrous Spatial Pyramid Pooling and bilinear upsampling for the decoder module on top of the modified ResNet block.\n",
    "\n",
    "DeepLab V3+ uses Aligned Xception as its main feature extractor, with the following modifications:\n",
    "\n",
    "1. All max pooling operations are replaced by depthwise separable convolution with striding\n",
    "2. Extra batch normalization and ReLU activation are added after each 3 x 3 depthwise convolution\n",
    "3. Depth of the model is increased without changing the entry flow network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47d0fd",
   "metadata": {},
   "source": [
    "![](https://i0.wp.com/s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/01/semantic_8.png?resize=649%2C333&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e723a",
   "metadata": {},
   "source": [
    "**[DeepLabV3 model with a ResNet-101 backbone](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, num_classes=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft.to(device)\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "if NUM_GPUS > 1:\n",
    "    model_ft = torch.nn.DataParallel(model_ft)\n",
    "_ = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fa11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=4, shuffle=True, num_workers=NUM_GPUS,drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b76715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an optimizer\n",
    "params = [p for p in model_ft.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d29782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=5,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model_ft, optimizer, data_loader, device, epoch)\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ba1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "model_ft.to(torch.device('cuda'))\n",
    "#assert model_ft.training == False\n",
    "\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151bebc7",
   "metadata": {},
   "source": [
    "<font size=\"6\" color=\"green\">Please UPVOTE if you find this kernel Helpful!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'deeplabv3Resnet101.pth')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a4b2a",
   "metadata": {},
   "source": [
    "# References \n",
    "* [mask-rcnn with augmentation and multiple masks](https://www.kaggle.com/abhishek/mask-rcnn-with-augmentation-and-multiple-masks)\n",
    "\n",
    "* [SIIM-DeepLabV3](https://www.kaggle.com/soulmachine/siim-deeplabv3)\n",
    "\n",
    "* [PyTorch Starter (U-Net ResNet)](https://www.kaggle.com/ateplyuk/pytorch-starter-u-net-resnet)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
