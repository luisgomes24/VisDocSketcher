{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e8a828",
   "metadata": {},
   "source": [
    "# Description\n",
    "This kernel performs inference for [Grapheme Fasta2i starter](https://www.kaggle.com/sheriytm/grapheme-fastai2-starter) kernel. Check it for more training details. The image preprocessing pipline is provided [here](https://www.kaggle.com/iafoss/image-preprocessing-128x128).\n",
    "\n",
    "Please upvote the original [training](https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964), [inference](https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-inference) kernels and [my Fasai2 dataset](https://www.kaggle.com/sheriytm/fastai2). Thank you @lafoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69770c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "import os\n",
    "from mish_activation import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cbcda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "SIZE = 128\n",
    "bs = 128\n",
    "stats = (0.0692, 0.2051)\n",
    "arch = models.densenet121\n",
    "MODEL = '../input/bengali-fastai2-cutmix-mixup-resized-128x128-input/model_0.pth'\n",
    "#MODEL = \"../input/grapheme-fastai2-starter/model_0.pth\"\n",
    "#MODEL = \"../input/bengali-seresnext-models/model_0_metric_tot_0968549.pth\"\n",
    "#MODEL = \"../input/bengali-seresnext-models/model_0_cutmix_02_25_2020.pth\"\n",
    "nworkers = 2\n",
    "\n",
    "TEST = ['/kaggle/input/bengaliai-cv19/test_image_data_0.parquet',\n",
    "        '/kaggle/input/bengaliai-cv19/test_image_data_1.parquet',\n",
    "        '/kaggle/input/bengaliai-cv19/test_image_data_2.parquet',\n",
    "        '/kaggle/input/bengaliai-cv19/test_image_data_3.parquet']\n",
    "\n",
    "LABELS = '../input/bengaliai-cv19/train.csv'\n",
    "\n",
    "df = pd.read_csv(LABELS)\n",
    "nunique = list(df.nunique())[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23612585",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e1cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, nc, n, ps=0.5):\n",
    "        super().__init__()\n",
    "        layers = [AdaptiveConcatPool2d(), Mish(), Flatten()] + \\\n",
    "            bn_drop_lin(nc*2, 512, True, ps, Mish()) + \\\n",
    "            bn_drop_lin(512, n, True, ps)\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self._init_weight()\n",
    "        \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1.0)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "#change the first conv to accept 1 chanel input\n",
    "class Dnet_1ch(nn.Module):\n",
    "    def __init__(self, arch=arch, n=nunique, pre=True, ps=0.5):\n",
    "        super().__init__()\n",
    "        m = arch(True) if pre else arch()\n",
    "        \n",
    "        conv = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        w = (m.features.conv0.weight.sum(1)).unsqueeze(1)\n",
    "        conv.weight = nn.Parameter(w)\n",
    "        \n",
    "        self.layer0 = nn.Sequential(conv, m.features.norm0, nn.ReLU(inplace=True))\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
    "            m.features.denseblock1)\n",
    "        self.layer2 = nn.Sequential(m.features.transition1,m.features.denseblock2)\n",
    "        self.layer3 = nn.Sequential(m.features.transition2,m.features.denseblock3)\n",
    "        self.layer4 = nn.Sequential(m.features.transition3,m.features.denseblock4,\n",
    "                                    m.features.norm5)\n",
    "        \n",
    "        nc = self.layer4[-1].weight.shape[0]\n",
    "        self.head1 = Head(nc,n[0])\n",
    "        self.head2 = Head(nc,n[1])\n",
    "        self.head3 = Head(nc,n[2])\n",
    "        #to_Mish(self.layer0), to_Mish(self.layer1), to_Mish(self.layer2)\n",
    "        #to_Mish(self.layer3), to_Mish(self.layer4)\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x1 = self.head1(x)\n",
    "        x2 = self.head2(x)\n",
    "        x3 = self.head3(x)\n",
    "        \n",
    "        return x1,x2,x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dnet_1ch(pre=False).cuda()\n",
    "model.load_state_dict(torch.load(MODEL, map_location=torch.device('cpu')));\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a025f0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08649bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "\n",
    "def bbox(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def crop_resize(img0, size=SIZE, pad=16):\n",
    "    #crop a box around pixels large than the threshold \n",
    "    #some images contain line at the sides\n",
    "    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n",
    "    #cropping may cut too much, so we need to add it back\n",
    "    xmin = xmin - 13 if (xmin > 13) else 0\n",
    "    ymin = ymin - 10 if (ymin > 10) else 0\n",
    "    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n",
    "    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n",
    "    img = img0[ymin:ymax,xmin:xmax]\n",
    "    #remove lo intensity pixels as noise\n",
    "    img[img < 28] = 0\n",
    "    lx, ly = xmax-xmin,ymax-ymin\n",
    "    l = max(lx,ly) + pad\n",
    "    #make sure that the aspect ratio is kept in rescaling\n",
    "    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n",
    "    return cv2.resize(img,(size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed112a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphemeDataset(Dataset):\n",
    "    def __init__(self, fname):\n",
    "        self.df = pd.read_parquet(fname)\n",
    "        self.data = 255 - self.df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.df.iloc[idx,0]\n",
    "        #normalize each image by its max val\n",
    "        img = (self.data[idx]*(255.0/self.data[idx].max())).astype(np.uint8)\n",
    "        img = crop_resize(img)\n",
    "        img = (img.astype(np.float32)/255.0 - stats[0])/stats[1]\n",
    "        return img, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe60898",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a20c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id,target = [],[]\n",
    "for fname in TEST:\n",
    "    ds = GraphemeDataset(fname)\n",
    "    dl = DataLoader(ds, batch_size=bs, num_workers=nworkers, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm(dl):\n",
    "            x = x.unsqueeze(1).cuda()\n",
    "            p1,p2,p3 = model(x)\n",
    "            p1 = p1.argmax(-1).view(-1).cpu()\n",
    "            p2 = p2.argmax(-1).view(-1).cpu()\n",
    "            p3 = p3.argmax(-1).view(-1).cpu()\n",
    "            for idx,name in enumerate(y):\n",
    "                row_id += [f'{name}_grapheme_root',f'{name}_vowel_diacritic',\n",
    "                           f'{name}_consonant_diacritic']\n",
    "                target += [p1[idx].item(),p2[idx].item(),p3[idx].item()]\n",
    "                \n",
    "sub_df = pd.DataFrame({'row_id': row_id, 'target': target})\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda52cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
