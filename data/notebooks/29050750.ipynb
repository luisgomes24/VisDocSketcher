{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab73232",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<h1><center><strong>COMPARISON OF TREE \n",
    "    BASED MODELS üìä</strong></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5044a64",
   "metadata": {},
   "source": [
    "#### If this helped in your learning, then please **UPVOTE** ‚Äì as they are the source of motivation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a9f4d",
   "metadata": {},
   "source": [
    "There are Many Tree Based Algorithms but we discuss the most usable ones in Kaggle Competition Now a days.\n",
    "I picked two Important Algorithms:\n",
    "\n",
    "#### **XGBOOST and Gradient Boosting Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc25d54",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"imports\"></a>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 30px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 1px; background-color: #ffffff; color: #03045e;\" id=\"imports\">&nbsp;&nbsp;XGBOOST MODEL&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a89bb4a",
   "metadata": {},
   "source": [
    "### **What is XGBoost and why is it so popular?**\n",
    "\n",
    "Let me introduce you to the hottest Machine Learning library in the ML community ‚Äî XGBoost. In recent years, it has been the main driving force behind the algorithms that win massive ML competitions. Its speed and performance are unparalleled and it consistently outperforms any other algorithms aimed at supervised learning tasks.\n",
    "\n",
    "The library is parallelizable which means the core algorithm can run on clusters of GPUs or even across a network of computers. This makes it feasible to solve ML tasks by training on hundreds of millions of training examples with high performance.\n",
    "\n",
    "**Classification task:** a supervised machine learning task in which one should predict if an instance is in some category by studying the instance‚Äôs features. For example, by looking at the body measurements, patient history, and glucose levels of a person, you can predict whether a person belongs to the ‚ÄòHas diabetes‚Äô or ‚ÄòDoes not have diabetes‚Äô group.\n",
    "\n",
    "**Binary classification:** One type of classification where the target instance can only belong to either one of two classes. For example, predicting whether an email is a spam or not, whether a customer purchases some product or not, etc.\n",
    "\n",
    "**Multi-class classification:** Another type of classification problem where the target can belong to one of many categories. For example, predicting the species of a bird, guessing someone's bloody type, etc.\n",
    "\n",
    "\n",
    "Unlike many other algorithms, XGBoost is an ensemble learning algorithm meaning that it combines the results of many models, called base learners to make a prediction.\n",
    "\n",
    "Just like in Random Forests, XGBoost uses Decision Trees as base learners:\n",
    "\n",
    "![](https://miro.medium.com/proxy/1*0e-3tgPVUJOhTfe2vRNjqg.png)\n",
    "\n",
    "An example of a decision tree can be seen above. In each decision node (circles), there is a single question that is being asked with only two possible answers. At the bottom of each tree, there is a single decision (rectangles). In the above tree, the first question is whether it is sunny or not. If yes, you immediately decide that it is not going to rain. If otherwise, you continue to ask more binary (yes/no) questions that ultimately will lead to some decision at the last ‚Äúleaf‚Äù (rectangle).\n",
    "\n",
    "Individual decision trees are low-bias, high-variance models. They are incredibly good at finding the relationships in any type of training data but struggle to generalize well on unseen data.\n",
    "\n",
    "However, the trees used by XGBoost are a bit different than traditional decision trees. They are called CART trees (Classification and Regression trees) and instead of containing a single decision in each ‚Äúleaf‚Äù node, they contain real-value scores of whether an instance belongs to a group. After the tree reaches max depth, the decision can be made by converting the scores into categories using a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42b237",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 30px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 1px; background-color: #ffffff; color: #03045e;\" id=\"imports\">&nbsp;&nbsp;GRADIENT BOOSTING ALGORITHM&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0839b5",
   "metadata": {},
   "source": [
    "*Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. (Wikipedia definition)*\n",
    "\n",
    "The objective of any supervised learning algorithm is to define a loss function and minimize it. Let‚Äôs see how maths work out for Gradient Boosting algorithm. Say we have mean squared error (MSE) as loss defined as:\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*fHenn7NVqcWvw25D3-zRiQ.png)\n",
    "\n",
    "\n",
    "We want our predictions, such that our loss function (MSE) is minimum. By using gradient descent and updating our predictions based on a learning rate, we can find the values where MSE is minimum.\n",
    "\n",
    "\n",
    "**I will dive into the maths of the algorithm, Above I just shows you the main points.**\n",
    "\n",
    "#### **Main Points:**\n",
    "\n",
    "1. We first model data with simple models and analyze data for errors.\n",
    "2. These errors signify data points that are difficult to fit by a simple model.\n",
    "3. Then for later models, we particularly focus on those hard to fit data to get them right.\n",
    "4. In the end, we combine all the predictors by giving some weights to each predictor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407c3ea",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 1px; background-color: #ffffff; color: #03045e;\" id=\"imports\">&nbsp;&nbsp;IMPORT LIBRARIES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['LotFrontage'].fillna(train_df['LotFrontage'].median(),inplace=True)\n",
    "train_df['MasVnrArea'].fillna(train_df['MasVnrArea'].median(),inplace=True)\n",
    "\n",
    "# Log Transformation: Applied on High Variance columns\n",
    "\n",
    "\n",
    "train_df['LotArea'] = np.log(train_df['LotArea'])\n",
    "\n",
    "# Dropped Skewed Columns\n",
    "\n",
    "skewed_cols = ['LowQualFinSF','3SsnPorch','PoolArea','MiscVal','GarageYrBlt']\n",
    "train_df.drop(skewed_cols,axis=1,inplace=True)\n",
    "\n",
    "# Finding Outliers \n",
    "for col in train_df.columns:\n",
    "    \n",
    "    if train_df[col].dtype !='object':\n",
    "        first_quartile = train_df[col].quantile(0.25) \n",
    "        third_quartile = train_df[col].quantile(0.75)\n",
    "\n",
    "        IQR = third_quartile - first_quartile\n",
    "        out = third_quartile + 3*IQR \n",
    "        train_df.drop(train_df[train_df[col] > out].index,axis=0,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "train_df.drop(['Id','GarageCars'],axis=1,inplace=True)\n",
    "\n",
    "# Categorical : \n",
    "\n",
    "\n",
    "drop_col = ['Alley','PoolQC','MiscFeature','Fence']\n",
    "train_df.drop(drop_col,axis=1,inplace=True)\n",
    "\n",
    "train_df['MasVnrType'].fillna(train_df['MasVnrType'].mode()[0],inplace=True)\n",
    "train_df['BsmtQual'].fillna(train_df['BsmtQual'].mode()[0],inplace=True)\n",
    "train_df['BsmtCond'].fillna(train_df['BsmtCond'].mode()[0],inplace=True)\n",
    "train_df['BsmtExposure'].fillna(train_df['BsmtExposure'].mode()[0],inplace=True)\n",
    "train_df['BsmtFinType1'].fillna(train_df['BsmtFinType1'].mode()[0],inplace=True)\n",
    "train_df['BsmtFinType2'].fillna(train_df['BsmtFinType2'].mode()[0],inplace=True)\n",
    "train_df['FireplaceQu'].fillna(train_df['FireplaceQu'].mode()[0],inplace=True)\n",
    "train_df['GarageType'].fillna(train_df['GarageType'].mode()[0],inplace=True)\n",
    "train_df['GarageFinish'].fillna(train_df['GarageFinish'].mode()[0],inplace=True)\n",
    "train_df['GarageQual'].fillna(train_df['GarageQual'].mode()[0],inplace=True)\n",
    "train_df['GarageCond'].fillna(train_df['GarageCond'].mode()[0],inplace=True)\n",
    "train_df['Electrical'].fillna(train_df['Electrical'].mode()[0],inplace=True)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        train_df[col] = le.fit_transform(train_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46780cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['LotFrontage'].fillna(test_df['LotFrontage'].median(),inplace=True)\n",
    "test_df['MasVnrArea'].fillna(test_df['MasVnrArea'].median(),inplace=True)\n",
    "\n",
    "\n",
    "# Log Transformation: Applied on High Variance columns\n",
    "\n",
    "test_df['LotArea'] = np.log(test_df['LotArea'])\n",
    "\n",
    "# Dropped Skewed Columns\n",
    "\n",
    "skewed_cols = ['LowQualFinSF','3SsnPorch','PoolArea','MiscVal','GarageYrBlt']\n",
    "test_df.drop(skewed_cols,axis=1,inplace=True)\n",
    "\n",
    "test_df.drop(['Id','GarageCars'],axis=1,inplace=True)\n",
    "\n",
    "# Categorical : \n",
    "\n",
    "\n",
    "drop_col = ['Alley','PoolQC','MiscFeature','Fence']\n",
    "test_df.drop(drop_col,axis=1,inplace=True)\n",
    "\n",
    "test_df['MasVnrType'].fillna(test_df['MasVnrType'].mode()[0],inplace=True)\n",
    "test_df['BsmtQual'].fillna(test_df['BsmtQual'].mode()[0],inplace=True)\n",
    "test_df['BsmtCond'].fillna(test_df['BsmtCond'].mode()[0],inplace=True)\n",
    "test_df['BsmtExposure'].fillna(test_df['BsmtExposure'].mode()[0],inplace=True)\n",
    "test_df['BsmtFinType1'].fillna(test_df['BsmtFinType1'].mode()[0],inplace=True)\n",
    "test_df['BsmtFinType2'].fillna(test_df['BsmtFinType2'].mode()[0],inplace=True)\n",
    "test_df['FireplaceQu'].fillna(test_df['FireplaceQu'].mode()[0],inplace=True)\n",
    "test_df['GarageType'].fillna(test_df['GarageType'].mode()[0],inplace=True)\n",
    "test_df['GarageFinish'].fillna(test_df['GarageFinish'].mode()[0],inplace=True)\n",
    "test_df['GarageQual'].fillna(test_df['GarageQual'].mode()[0],inplace=True)\n",
    "test_df['GarageCond'].fillna(test_df['GarageCond'].mode()[0],inplace=True)\n",
    "test_df['Electrical'].fillna(test_df['Electrical'].mode()[0],inplace=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in test_df:\n",
    "    if test_df[col].dtype == 'object':\n",
    "        test_df[col] = le.fit_transform(test_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = train_df.drop(['SalePrice'],axis=1)\n",
    "target = train_df.SalePrice\n",
    "x_train,x_test,y_train,y_test = train_test_split(input,target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774ac5c",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "<a id=\"imports\"></a>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 1px; background-color: #ffffff; color: #03045e;\" id=\"imports\">XGBRegressor Model with these hyperparameters&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBRegressor(subsample=0.1,\n",
    "                      n_estimators=700,\n",
    "                      min_child_weight= 2,\n",
    "                      max_depth=4,\n",
    "                      learning_rate=0.2,\n",
    "                      col_sample_bytree=1,\n",
    "                      booster='gblinear',\n",
    "                      alpha=22)\n",
    "xg.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aec7d0",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 1px; background-color: #ffffff; color: #03045e;\" id=\"imports\">RandomForestRegressor with these Hyperparameters&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babab5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmf = RandomForestRegressor(n_estimators=500,min_samples_split=3,max_depth=5)\n",
    "rmf.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5856d2",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>\n",
    "\n",
    "<h1 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 1px; background-color: #ffffff; color: #03045e;\" id=\"imports\">GradientBoostingRegressor with these Hyperparameters&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60866ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_m = GradientBoostingRegressor(learning_rate=0.5,n_estimators=900,max_depth=11,min_samples_split=8)\n",
    "gb_m.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa23243",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "acc.append(xg.score(x_test,y_test))\n",
    "acc.append(rmf.score(x_test,y_test))\n",
    "acc.append(gb_m.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620cb3f3",
   "metadata": {},
   "source": [
    "**we tested all three Best Models and know we create a DataFrame to frame these model's absolute error to find out the least errored Model, which will be the best one for this House Price Prediction Problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83becc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plott = pd.DataFrame()\n",
    "plott['Model'] = ['XGBRegressor', 'RandomForestRegressor' , 'GradientBoostingRegressor']\n",
    "arr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f28c97",
   "metadata": {},
   "source": [
    "# Calculating the Absolute Error for all these three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xg.predict(x_test)\n",
    "arr.append(mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rmf.predict(x_test)\n",
    "arr.append(mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d216b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gb_m.predict(x_test)\n",
    "arr.append(mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24738688",
   "metadata": {},
   "outputs": [],
   "source": [
    "plott['Absolute Error'] = arr\n",
    "plott['Accuracy Score'] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1768942",
   "metadata": {},
   "source": [
    "**Now we can see by visualization that the least error Model is XGBRegressor + also the Model's accuracy is more than other two.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a826f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(28,25))\n",
    "plt.subplot(5,4,1)\n",
    "plt.title('Absolute Error Measurement')\n",
    "plt.ylabel('Error')\n",
    "plt.bar('Model','Absolute Error',data=plott,color='maroon')\n",
    "plt.subplot(5,4,2)\n",
    "plt.title('Accuracy Score Measurement')\n",
    "plt.ylabel('Score')\n",
    "plt.bar('Model','Accuracy Score',data=plott,color='darkorchid')\n",
    "plt.legend()\n",
    "fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b54dce0",
   "metadata": {},
   "source": [
    "**So we can clearly say that we use XGBRegressor with these hyperparameters will give better result than ensembles.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
