{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bfe3ff",
   "metadata": {},
   "source": [
    "**This is a competition notebook with the goal of obtaining the highest possible rank on the public leaderboard.**\n",
    "\n",
    "Marko Marfat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5378728",
   "metadata": {},
   "source": [
    "**Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12884e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/dapprojekt22/train.csv')\n",
    "test_data = pd.read_csv('../input/dapprojekt22/test.csv')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f980bbf0",
   "metadata": {},
   "source": [
    "**Removing data with NA values**\n",
    "\n",
    "Fortunately, in this dataset, the only NA values are in columns which only contain NA values. This way, it's simple to decide what to do with NA values - they'll simply be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15500e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(axis=1)\n",
    "test_data = test_data.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89acdc",
   "metadata": {},
   "source": [
    "**Removing data with a uniform distribution**\n",
    "\n",
    "All data with uniform (constant) distribution will be removed as it serves no purpose. Only identifiers will be kept as they can be later used to for joining dataframes and similar operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6af542",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_uniform = ['PCT_AST_HOME', 'PCT_AST_AWAY', 'PCT_BLKA_HOME', 'PCT_BLKA_AWAY', 'PCT_BLK_HOME', 'PCT_BLK_AWAY', 'PCT_DREB_HOME', 'PCT_DREB_AWAY', 'PCT_FG3A_HOME', \n",
    "                  'PCT_FG3A_AWAY', 'PCT_FG3M_HOME', 'PCT_FG3M_AWAY', 'PCT_FGA_HOME', 'PCT_FGA_AWAY', 'PCT_FGM_HOME', 'PCT_FGM_AWAY', 'PCT_FTA_HOME', 'PCT_FTA_AWAY', \n",
    "                  'PCT_FTM_HOME', 'PCT_FTM_AWAY', 'PCT_OREB_HOME', 'PCT_OREB_AWAY', 'PCT_PFD_HOME', 'PCT_PFD_AWAY', 'PCT_PF_HOME', 'PCT_PF_AWAY', 'PCT_PTS_HOME', \n",
    "                  'PCT_PTS_AWAY', 'PCT_REB_HOME', 'PCT_REB_AWAY', 'PCT_STL_HOME', 'PCT_STL_AWAY', 'PCT_TOV_HOME', 'PCT_TOV_AWAY']\n",
    "\n",
    "train_data = train_data.loc[:, ~train_data.columns.isin(remove_uniform)]\n",
    "test_data = test_data.loc[:, ~test_data.columns.isin(remove_uniform)]\n",
    "train_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aef299",
   "metadata": {},
   "source": [
    "**Adding a new column to signify which team won the current match**\n",
    "\n",
    "This is done so that the correlation between the statistics and the outcome of the match can be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data['PTS_HOME'] > train_data['PTS_AWAY'], 'CURRENT_WINNER'] = 0\n",
    "train_data.loc[train_data['PTS_HOME'] < train_data['PTS_AWAY'], 'CURRENT_WINNER'] = 1\n",
    "\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa55cb",
   "metadata": {},
   "source": [
    "**Determining the correlation**\n",
    "\n",
    "Tracking down the 50 variables which have the highest magnitude of correlation with the target variable. This is done to get an idea of which stats have the highest predictability of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20151e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "top50_corr_var = np.abs(train_data.iloc[:, :-1].corrwith(train_data['CURRENT_WINNER'])).sort_values(ascending=False).head(50)\n",
    "top50_corr_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bdbabd",
   "metadata": {},
   "source": [
    "**Reducing the dataset to only these 50 variables**\n",
    "\n",
    "**Note:** Variables `TEAM_ABBREVIATION_HOME`, `TEAM_ABBREVIATION_AWAY`, `NEXT_HOME`, `NEXT_AWAY` and `NEXT_WINNER` will also be included as they are needed for learning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506967bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vars = list(top50_corr_var.index)\n",
    "new_vars.extend(['TEAM_ABBREVIATION_HOME', 'TEAM_ABBREVIATION_AWAY', 'NEXT_HOME', 'NEXT_AWAY', 'NEXT_WINNER'])\n",
    "\n",
    "train_data = train_data[new_vars]\n",
    "\n",
    "new_vars.remove('NEXT_WINNER')\n",
    "new_vars.append('id')\n",
    "\n",
    "test_data = test_data[new_vars]\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0d951",
   "metadata": {},
   "source": [
    "**Transforming the data**\n",
    "\n",
    "Next step is to transform the data so that a meaningful classifier can be made. Data will be transformed so that each team which is playing the next match will have their average stats from their last 5 games. These stats will be split into the home and away categories since the performances of team depends if they play in a comfortable environment (@ home) or a new environment (@ away).\n",
    "\n",
    "First step is determining how many matches did each team play prior to their next match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828be717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_counter(row, test):\n",
    "    next_home = row['NEXT_HOME']\n",
    "    next_away = row['NEXT_HOME']\n",
    "    \n",
    "    past_matches = train_data.loc[0:row.name]\n",
    "    \n",
    "    if test:\n",
    "        past_matches = test_data.loc[0:row.name]\n",
    "        \n",
    "    # HAH -> HOME AT HOME -> How many matches did the 'NEXT_HOME' team play at home so far?\n",
    "    # HAA -> HOME AT AWAY -> How many matches did the 'NEXT_HOME' team play at away so far?\n",
    "    # AAH -> AWAY AT HOME -> How many matches did the 'NEXT_AWAY' team play at home so far?\n",
    "    # AAA -> AWAY AT AWAY -> How many matches did the 'NEXT_AWAY' team play at away so far?\n",
    "        \n",
    "    row['COUNT_HAH'] = len(past_matches.loc[past_matches.TEAM_ABBREVIATION_HOME == next_home])\n",
    "    row['COUNT_HAA'] = len(past_matches.loc[past_matches.TEAM_ABBREVIATION_AWAY == next_home])\n",
    "    row['COUNT_AAH'] = len(past_matches.loc[past_matches.TEAM_ABBREVIATION_HOME == next_away])\n",
    "    row['COUNT_AAA'] = len(past_matches.loc[past_matches.TEAM_ABBREVIATION_AWAY == next_away])\n",
    "    \n",
    "    return row\n",
    "    \n",
    "train_data = train_data.apply(match_counter, test=False, axis=1)\n",
    "test_data = test_data.apply(match_counter, test=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ea454",
   "metadata": {},
   "source": [
    "Columns with total games played for NEXT_HOME and NEXT_AWAY will also be added because they may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee4cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['COUNT_HT'] = train_data['COUNT_HAH'] + train_data['COUNT_HAA']\n",
    "train_data['COUNT_AT'] = train_data['COUNT_AAH'] + train_data['COUNT_AAA']\n",
    "\n",
    "test_data['COUNT_HT'] = test_data['COUNT_HAH'] + test_data['COUNT_HAA']\n",
    "test_data['COUNT_AT'] = test_data['COUNT_AAH'] + test_data['COUNT_AAA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[150:160]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f84e60",
   "metadata": {},
   "source": [
    "Now it's time to transform the actual stats into the explained format.\n",
    "\n",
    "First, it's necessary to calculate all the means of the features (stats) since those will be used when there isn't enough information about the match (e.g. not enough games played)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb26015",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_means = dict()\n",
    "test_feature_means = dict()\n",
    "\n",
    "def calculate_feature_means(column, test):\n",
    "    if not test:\n",
    "        train_feature_means[f\"{column.name}\"] = np.mean(column)\n",
    "    else:\n",
    "        test_feature_means[f\"{column.name}\"] = np.mean(column)\n",
    "    \n",
    "train_data.iloc[:, :-11].apply(calculate_feature_means, test=False, axis=0);\n",
    "test_data.iloc[:, :-11].apply(calculate_feature_means, test=True, axis=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d450f9b",
   "metadata": {},
   "source": [
    "Next step is calculating stats of the teams which are playing the next game. The requirement was looking at the last 5 games of each team. If there aren't 5 games to look at, the means from the previous step will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_vars = new_vars[:-5]\n",
    "\n",
    "def calculate_stats(row, test):\n",
    "    next_home = row['NEXT_HOME']\n",
    "    next_away = row['NEXT_AWAY']\n",
    "    \n",
    "    matches_HAH = row['COUNT_HAH']\n",
    "    matches_HAA = row['COUNT_HAA']\n",
    "    matches_AAH = row['COUNT_AAH']\n",
    "    matches_AAA = row['COUNT_AAA']\n",
    "    \n",
    "    past_matches = train_data.loc[0:row.name]\n",
    "    mean_values = train_feature_means\n",
    "    \n",
    "    \n",
    "    if test:\n",
    "        past_matches = test_data.loc[0:row.name]\n",
    "        mean_values = test_feature_means\n",
    "    \n",
    "    \n",
    "    if matches_HAH >= 5:\n",
    "        last_5_games = past_matches[past_matches.TEAM_ABBREVIATION_HOME == next_home].iloc[-5:]\n",
    "        \n",
    "        for stat in stats_vars:\n",
    "            if stat.endswith('HOME'):\n",
    "                row[stat + '_HAH'] = last_5_games[stat].mean()\n",
    "            \n",
    "    else:\n",
    "        for stat in stats_vars:\n",
    "            if stat.endswith('HOME'):\n",
    "                row[stat + '_HAH'] = mean_values[stat]\n",
    "        \n",
    "    if matches_HAA >= 5:\n",
    "        last_5_games = past_matches[past_matches.TEAM_ABBREVIATION_AWAY == next_home].iloc[-5:]\n",
    "        \n",
    "        for stat in stats_vars:\n",
    "            if stat.endswith('AWAY'):\n",
    "                row[stat + '_HAA'] = last_5_games[stat].mean()\n",
    "            \n",
    "    else:\n",
    "        for stat in stats_vars:\n",
    "            if stat.endswith('AWAY'):\n",
    "                row[stat + '_HAA'] = mean_values[stat]\n",
    "            \n",
    "    if matches_AAH >= 5:\n",
    "        last_5_games = past_matches[past_matches.TEAM_ABBREVIATION_HOME == next_away].iloc[-5:]\n",
    "        \n",
    "        for stat in stats_vars:\n",
    "            if stat.endswith('HOME'):\n",
    "                row[stat + '_AAH'] = last_5_games[stat].mean()\n",
    "        \n",
    "    else:\n",
    "        for stat in stats_vars:\n",
    "            if stat.endswith('HOME'):\n",
    "                row[stat + '_AAH'] = mean_values[stat]\n",
    "        \n",
    "    if matches_AAA >= 5:\n",
    "        last_5_games = past_matches[past_matches.TEAM_ABBREVIATION_AWAY == next_away].iloc[-5:]\n",
    "        \n",
    "        for stat in stats_vars:\n",
    "            if stat.endswith('AWAY'):\n",
    "                row[stat + '_AAA'] = last_5_games[stat].mean()\n",
    "    else:\n",
    "        for stat in stats_vars:\n",
    "            if stat.endswith('AWAY'):\n",
    "                row[stat + '_AAA'] = mean_values[stat]\n",
    "        \n",
    "    return row\n",
    "    \n",
    "\n",
    "train_data = train_data.apply(calculate_stats, test=False, axis=1)\n",
    "test_data = test_data.apply(calculate_stats, test=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.iloc[:, 54:]\n",
    "test_data = test_data.iloc[:, 54:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebda7e",
   "metadata": {},
   "source": [
    "Now we have the dataset with all the necessary transformations to start training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7905d4c",
   "metadata": {},
   "source": [
    "**Normalising the data**\n",
    "\n",
    "Next step before training the models is data normalisation. Standard scaling will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aac0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing COUNT columns as they aren't needed anymore\n",
    "count_cols = ['COUNT_HAH', 'COUNT_HAA', 'COUNT_AAH', 'COUNT_AAA', 'COUNT_HT', 'COUNT_AT']\n",
    "train_data = train_data.loc[:, ~train_data.columns.isin(count_cols)]\n",
    "test_data = test_data.loc[:, ~test_data.columns.isin(count_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:, 1:] = StandardScaler().fit_transform(train_data.iloc[:, 1:])\n",
    "test_data.iloc[:, 1:] = StandardScaler().fit_transform(test_data.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ecdbc",
   "metadata": {},
   "source": [
    "**Cross-validation function**\n",
    "\n",
    "Let's test some models using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6dd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving NEXT_WINNER and id to the end of the dataset\n",
    "cols_train = train_data.columns.tolist()\n",
    "cols_test = test_data.columns.tolist()\n",
    "\n",
    "cols_train = cols_train[1:] + cols_train[:1]\n",
    "cols_test = cols_test[1:] + cols_test[:1]\n",
    "\n",
    "train_data = train_data[cols_train]\n",
    "test_data = test_data[cols_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286be912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_compare(data):\n",
    "    models = [GaussianNB(), LogisticRegression(), RandomForestClassifier(), ExtraTreesClassifier(), XGBClassifier()]\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1:].squeeze(axis=1).ravel()\n",
    "    \n",
    "    table = pd.DataFrame(columns = [\"Algorithm\", \"Fold 1\", \"Fold 2\", \"Fold 3\", \"Fold 4\", \"Fold 5\", \"Average\"])\n",
    "    \n",
    "    for model in models:\n",
    "        acc = cross_val_score(model, X, y, scoring='accuracy', cv = tss, n_jobs = -1)\n",
    "        row = {'Algorithm': type(model).__name__, 'Fold 1': acc[0], 'Fold 2': acc[1], 'Fold 3': acc[2], 'Fold 4': acc[3], 'Fold 5': acc[4], 'Average': np.mean(acc)}\n",
    "        table = table.append(row, ignore_index=True)\n",
    "    \n",
    "    table.set_index('Algorithm', inplace=True)\n",
    "    display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_compare(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit(n_splits=5)\n",
    "accuracies = []\n",
    "\n",
    "for i in range(int(100 / 5)):\n",
    "    selector = SelectKBest(score_func=f_classif, k=int((i+1)*5)).fit(train_data.iloc[:, :-1], train_data.iloc[:, -1:].squeeze(axis=1).ravel())\n",
    "    cols = selector.get_support(indices=True)\n",
    "    \n",
    "    new_featureset = train_data.iloc[:,cols]\n",
    "    \n",
    "    X, y = new_featureset, train_data.iloc[:, -1:].squeeze(axis=1).ravel()\n",
    "    accuracies.append(cross_val_score(ExtraTreesClassifier(), X, y, scoring='accuracy', cv = tss, n_jobs = -1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a61b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accs = [np.mean(x) for x in accuracies]\n",
    "\n",
    "plt.plot(all_accs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8beca8",
   "metadata": {},
   "source": [
    "It seems that using ~40 variables is the best for this model.\n",
    "\n",
    "Let's see how the models perform after eliminating other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=f_classif, k=int(40)).fit(train_data.iloc[:, :-1], train_data.iloc[:, -1:].squeeze(axis=1).ravel())\n",
    "cols = selector.get_support(indices=True)\n",
    "                       \n",
    "new_featureset = train_data.iloc[:,cols]  \n",
    "train_data_40 = new_featureset\n",
    "train_data_40['NEXT_WINNER'] = train_data.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_40 = test_data.iloc[:, cols]\n",
    "test_data_40['id'] = test_data.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_compare(train_data_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fea073",
   "metadata": {},
   "source": [
    "Let's try using RandomForest with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c869790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "#max_features = ['auto', 'sqrt']\n",
    "#max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "#max_depth.append(None)\n",
    "#min_samples_split = [2, 5, 10]\n",
    "#min_samples_leaf = [1, 2, 4]\n",
    "#bootstrap = [True, False]\n",
    "#random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "#\n",
    "#rf_classifier = RandomForestClassifier()\n",
    "#rf_random = RandomizedSearchCV(estimator = rf_classifier, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "#rf_random.fit(train_data_40.loc[:, train_data_40.columns != 'NEXT_WINNER'], train_data_40['NEXT_WINNER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799aee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_random.best_params_\n",
    "\n",
    "# best params {'n_estimators': 1000,\n",
    "# 'min_samples_split': 5,\n",
    "# 'min_samples_leaf': 2,\n",
    "# 'max_features': 'sqrt',\n",
    "# 'max_depth': 10,\n",
    "# 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97faa26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', max_depth=10, bootstrap=True)\n",
    "model.fit(train_data_40.loc[:, train_data_40.columns != 'NEXT_WINNER'], train_data_40['NEXT_WINNER'])\n",
    "\n",
    "predictions = model.predict(test_data_40.loc[:, test_data_40.columns != 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_data.loc[:,test_data.columns.isin(('id', ))]\n",
    "submission.loc[:, 'NEXT_WINNER'] = predictions\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index = None)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
