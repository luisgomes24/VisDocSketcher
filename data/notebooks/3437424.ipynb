{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22118b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# For reproducibility.\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "print(os.listdir(\"../input/glove-global-vectors-for-word-representation\"))\n",
    "print(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25b3fd",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "we load the dataset and apply some transformations to use it in a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "df_test = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\",encoding=\"latin-1\")\n",
    "print(\"Test shape:\", df_test.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.rename(columns=({\"comment_text\":\"Reviews\"}))\n",
    "df_train = df_train.rename(columns=({\"target\":\"Label\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895159d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.rename(columns=({\"comment_text\":\"Reviews\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8582730",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[:10000]\n",
    "df_test = df_test[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51f386",
   "metadata": {},
   "source": [
    "In addition to the datasets, we load a pretrained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE =  '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    embeddings = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_embeddings(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b87565",
   "metadata": {},
   "source": [
    "# Preprocessings\n",
    "\n",
    "After loading the datasets, we will preprocess the datasets. In this time, we will apply following techniques:\n",
    "\n",
    "* Nigation handling\n",
    "* Replacing numbers\n",
    "* Tokenization\n",
    "* Zero padding\n",
    "\n",
    "Negation handling is the process of converting negation abbreviation to a canonical format. For example, \"aren't\" is converted to \"are not\". It is helpful for sentiment analysis.\n",
    "\n",
    "Replacing numbers is the process of converting numbers to a specific character. For example, \"1924\" and \"123\" are converted to \"0\". It is helpful for reducing the vocabulary size.\n",
    "\n",
    "Tokenization is the process of taking a text or set of texts and breaking it up into its individual words. In this step, we will tokenize text with the help of splitting text by space or punctuation marks.\n",
    "\n",
    "Zero padding is the process of pad \"0\" to the dataset for the purpose of ensuring that all sentences has the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d80f4",
   "metadata": {},
   "source": [
    "## Negation handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21155d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Reviews = df_train.Reviews.str.replace(\"n't\", 'not')\n",
    "df_test.Reviews = df_test.Reviews.str.replace(\"n't\", 'not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6279e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_count=len(df_train['Label'].value_counts())\n",
    "target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f154c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e2e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Reviews'] = df_train['Reviews'].astype(str)\n",
    "df_test['Reviews'] = df_test['Reviews'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c5bce",
   "metadata": {},
   "source": [
    "## Replacing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Reviews = df_train.Reviews.apply(lambda x: re.sub(r'[0-9]+', '0', x))\n",
    "df_test.Reviews = df_test.Reviews.apply(lambda x: re.sub(r'[0-9]+', '0', x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60453c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train['Reviews'].values\n",
    "x_test  = df_test['Reviews'].values\n",
    "y_train = df_train['Label'].values\n",
    "x = np.r_[x_train, x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc2f58",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True, filters='\\n\\t')\n",
    "tokenizer.fit_on_texts(x)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test  = tokenizer.texts_to_sequences(x_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 is for zero padding.\n",
    "print('vocabulary size: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f730a47",
   "metadata": {},
   "source": [
    "## Zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fa6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = len(max((s for s in np.r_[x_train, x_test]), key=len))\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen, padding='post')\n",
    "print('maxlen: {}'.format(maxlen))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad80566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_embeddings(embeddings, word_index, vocab_size, dim=300):\n",
    "    embedding_matrix = np.zeros([vocab_size, dim])\n",
    "    for word, i in word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        vector = embeddings.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[i] = vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_size = 200\n",
    "embedding_matrix = filter_embeddings(embeddings, tokenizer.word_index,\n",
    "                                     vocab_size, embedding_size)\n",
    "print('OOV: {}'.format(len(set(tokenizer.word_index) - set(embeddings))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950efef6",
   "metadata": {},
   "source": [
    "# Building a model\n",
    "\n",
    "In this time, we will use attention based LSTM model. First of all, we should define the attention layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Keras Layer that implements an Attention mechanism for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    :param kwargs:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(Attention())\n",
    "    \"\"\"\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ad32d",
   "metadata": {},
   "source": [
    "After defining the attention layer, we will define the entire model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a167b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maxlen, vocab_size, embedding_size, embedding_matrix):\n",
    "    input_words = Input((maxlen, ))\n",
    "    x_words = Embedding(vocab_size,\n",
    "                        embedding_size,\n",
    "                        weights=[embedding_matrix],\n",
    "                        mask_zero=True,\n",
    "                        trainable=False)(input_words)\n",
    "    x_words = SpatialDropout1D(0.3)(x_words)\n",
    "    x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n",
    "    x = Attention(maxlen)(x_words)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    pred = Dense(target_count, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_words, outputs=pred)\n",
    "    return model\n",
    "\n",
    "model = build_model(maxlen, vocab_size, embedding_size, embedding_matrix)\n",
    "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507c9b84",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'model.h5'\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=15, verbose=1,\n",
    "                    batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a331d",
   "metadata": {},
   "source": [
    "# Making a submission file\n",
    "\n",
    "After training the model, we make a submission file by predicting for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "y_pred = y_pred.argmax(axis=1).astype(int)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26024b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['prediction'] = y_pred\n",
    "df_test[['id', 'prediction']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
