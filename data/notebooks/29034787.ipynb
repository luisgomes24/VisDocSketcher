{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11760c",
   "metadata": {},
   "source": [
    "###### X_MLP/W10_L6(last lec.)-dip-iitm: [AdaBoost and GradientBoost Regressor on California Housing](https://www.youtube.com/watch?v=yJjCDkjNNaM&list=PLyGVjd4KQp13HB4vo0f_ztrpR8dPrDNKP&index=76)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3d6e8",
   "metadata": {},
   "source": [
    "# AdaBoost and GradientBoost Regressor on California Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d79ee3",
   "metadata": {},
   "source": [
    "In this notebook, we will apply ensemble tecniques regression problem in california housing datasets.\n",
    "\n",
    "We will make use of \n",
    "- AdaBoost regressor\n",
    "- Gradient boosting regressor\n",
    "- XGBoost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import AdaBoostRegressor,\\\n",
    "    GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import cross_validate,\\\n",
    "    train_test_split, RandomizedSearchCV, ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(306)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176c174",
   "metadata": {},
   "source": [
    "Lets use`ShuffleSplit` as cv with 10 splits and 20% examples set aside as test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e1756",
   "metadata": {},
   "source": [
    "Let's download the data and split it into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "labels*=100\n",
    "\n",
    "# train_test split\n",
    "com_train_features, test_features, com_train_labels, test_labels = \\\n",
    "    train_test_split(features, labels, random_state=42)\n",
    "\n",
    "\n",
    "train_features, dev_features, train_labels, dev_labels =\\\n",
    "    train_test_split(com_train_features, com_train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3706d6",
   "metadata": {},
   "source": [
    "## Training different regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1445021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressor(estimator, X_train, y_train, cv, name):\n",
    "    cv_results = cross_validate(estimator, \n",
    "                               X_train, \n",
    "                               y_train,\n",
    "                               cv=cv,\n",
    "                               scoring=\"neg_mean_absolute_error\",\n",
    "                               return_train_score=True,\n",
    "                               return_estimator=True)\n",
    "    \n",
    "    cv_train_error = -1 * cv_results['train_score']\n",
    "    cv_test_error = -1 * cv_results['test_score']\n",
    "    \n",
    "    print(f\"On an average, {name} makes an error of \"\n",
    "          f\"{cv_train_error.mean():.3f}k +/- {cv_train_error.std():.3f}k on the training set.\")\n",
    "    print(f\"On an average, {name} makes an error of\"\n",
    "          f\"{cv_test_error.mean():.3f}k +/- {cv_test_error.std():.3f}k on teh test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title AdaBoostRegressor\n",
    "train_regressor(\n",
    "    AdaBoostRegressor(), com_train_features,\n",
    "    com_train_labels, cv, 'AdaBoostRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GradientBoostingRegressor\n",
    "train_regressor(\n",
    "    GradientBoostingRegressor(), com_train_features,\n",
    "    com_train_labels, cv, 'GradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b33a2",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ea6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72873d8",
   "metadata": {},
   "source": [
    "Extreme gradient boosting(XGBoost) is the latest boosting technique. It is more regularized form of gradient boosting. With regularization, it is able to achieve better generalization performance than gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_regressor = XGBRegressor(objective='reg:squarederror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed66b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_regressor(\n",
    "    xgb_regressor, com_train_features,\n",
    "    com_train_labels, cv, 'XGBoostRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
