{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d764c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/tabular-playground-series-feb-2022/train.csv\", index_col='row_id')\n",
    "test = pd.read_csv(\"/kaggle/input/tabular-playground-series-feb-2022/test.csv\", index_col='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dd = train.drop_duplicates().reset_index()\n",
    "train_wt = train.value_counts().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18be35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dd,train_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ce5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "def bias(w, x, y, z):\n",
    "    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n",
    "\n",
    "import re\n",
    "hist_re = re.compile(\"A(\\d+)T(\\d+)G(\\d+)C(\\d+)\")\n",
    "biases = {col:bias(*[int(i) for i in hist_re.match(col).groups()]) for col in train.columns[:-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_i = pd.DataFrame({col: ((train_dd[col] + biases[col]) * 1000000).round().astype(int) for col in train.columns[:-1]})\n",
    "test_i = pd.DataFrame({col: ((test[col] + biases[col]) * 1000000).round().astype(int) for col in test.columns})\n",
    "train_i.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c340fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dd = train_dd.assign(gcd = np.gcd.reduce(train_i, axis=1))\n",
    "test = test.assign(gcd = np.gcd.reduce(test_i, axis=1))\n",
    "train_dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f22d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_dd[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from matplotlib import pyplot\n",
    "xy = decomposition.PCA(n_components=2,whiten=True).fit_transform(train_i)\n",
    "pyplot.scatter(xy[:,0],xy[:,1],c=le.transform(train_dd[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scale in np.sort(train_dd['gcd'].unique()):\n",
    "    # Compute the PCA\n",
    "    pca = decomposition.PCA(whiten=True, random_state=1)\n",
    "    pca.fit(train_i[train_dd['gcd'] == scale])\n",
    "\n",
    "    # Transform the data so that the components can be analyzed\n",
    "    Xt_tr = pca.transform(train_i[train_dd['gcd'] == scale])\n",
    "    Xt_te = pca.transform(test_i[test['gcd'] == scale])\n",
    "\n",
    "    # Plot a scattergram, projected to two PCA components, colored by classification target\n",
    "    pyplot.figure(figsize=(6,6))\n",
    "    pyplot.scatter(Xt_tr[:,0], Xt_tr[:,1], c=le.transform(train_dd.loc[train_dd['gcd'] == scale,\"target\"]), s=1)\n",
    "    pyplot.title(f\"{1000000 // scale} decamers ({(train_dd['gcd'] == scale).sum()} samples with gcd = {scale})\")\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545cdc4c",
   "metadata": {},
   "source": [
    "So maybe we should just train on the ones with GCD=1?\n",
    "\n",
    "Or train separate classifiers depending on the GCD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcds = train_dd[\"gcd\"].unique()\n",
    "X_split = {g:train_i.loc[train_dd[\"gcd\"] == g] for g in gcds}\n",
    "y_split = {g:train_dd.loc[train_dd[\"gcd\"] == g, \"target\"] for g in gcds}\n",
    "wt_split = {g:train_wt.loc[train_dd[\"gcd\"] == g] for g in gcds}\n",
    "test_split = {g:test_i.loc[test[\"gcd\"] == g] for g in gcds}\n",
    "X_split[10].shape, y_split[10].shape, wt_split[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db77b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline, preprocessing, ensemble, model_selection\n",
    "\n",
    "#grid_params = {\n",
    "#    'extratreesclassifier__n_estimators': [50,100,250,500,1000]\n",
    "#}\n",
    "\n",
    "#model_split = {g:model_selection.GridSearchCV(\n",
    "#    pipeline.make_pipeline(preprocessing.StandardScaler(),\n",
    "#    ensemble.ExtraTreesClassifier()), grid_params, cv=5) for g in gcds}\n",
    "\n",
    "### parameters extracted by GridSearchCV\n",
    "\n",
    "n_estimators = {\n",
    "    1:50,\n",
    "    10:100,\n",
    "    1000:1000,\n",
    "    10000:1000\n",
    "}\n",
    "model_split = {g:pipeline.make_pipeline(preprocessing.StandardScaler(),\n",
    "    ensemble.ExtraTreesClassifier(n_estimators=n_estimators[g])) for g in gcds}\n",
    "\n",
    "for g in gcds:\n",
    "    model_split[g].fit(X_split[g], y_split[g], extratreesclassifier__sample_weight=wt_split[g])\n",
    "    #print(f\"Model on GCD={g}, accuracy: {model_split[g].best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b4746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for g in gcds:\n",
    "    #model_split[g].fit(X_split[g],y_split[g],extratreesclassifier__sample_weight=wt_split[g])\n",
    "    #print(f\"Model on GCD={g}, parameters: {model_split[g].best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test[[]].assign(target=\"\")\n",
    "\n",
    "for g in gcds:\n",
    "    submission.loc[test[\"gcd\"] == g, \"target\"] = model_split[g].predict(test_i.loc[test[\"gcd\"] == g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ca503",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test[[\"row_id\"]].assign(target=ypred).set_index(\"row_id\").to_csv(\"submission.csv\")\n",
    "submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4984cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
