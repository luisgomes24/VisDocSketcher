{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time, logging, gc\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f50327",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\n",
    "test = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5154020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from https://www.kaggle.com/hijest/gaps-features-tf-lstm-resnet-like-ff/notebook?scriptVersionId=77500256&cellId=3\n",
    "def add_features(df): #CV 0.1579\n",
    "    \n",
    "    df['u_in'] = np.sqrt(2*df['u_in'])\n",
    "    \n",
    "    df['area'] = df['time_step'] * df['u_in']\n",
    "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
    "    df['state'] = np.array([1 if x>0 else 0 for x in df['u_in']]) - df['u_out']\n",
    "    \n",
    "    df['exhale'] = df.groupby('breath_id')['u_out'].cumsum()\n",
    "        \n",
    "    df['delta_time'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
    "    df['delta_u_in'] = df.groupby(df['breath_id'])['u_in'].diff().fillna(0).reset_index(level=0,drop=True)     \n",
    "            \n",
    "    df['u_in_1st_derivative'] = (df['delta_u_in'] /df['delta_time']).fillna(0)\n",
    "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
    "    \n",
    "    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1).fillna(0).reset_index(level=0,drop=True)   \n",
    "    #df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2).fillna(0).reset_index(level=0,drop=True)\n",
    "    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3).fillna(0).reset_index(level=0,drop=True)\n",
    "    df['u_in_back1'] = df.groupby('breath_id')['u_in'].shift(-1).fillna(0).reset_index(level=0,drop=True)   \n",
    "    #df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2).fillna(0).reset_index(level=0,drop=True)\n",
    "    df['u_in_back3'] = df.groupby('breath_id')['u_in'].shift(-3).fillna(0).reset_index(level=0,drop=True)\n",
    "    #df['u_in_lag6'] = df.groupby('breath_id')['u_in'].shift(6).fillna(0).reset_index(level=0,drop=True)\n",
    "    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
    "#     df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
    "    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
    "#     df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
    "    \n",
    "    df['volume_mean']=((df['u_in'] + df['u_in'].shift(1).fillna(0))/2 * df['time_step'].diff().fillna(0)).clip(0,)\n",
    "      \n",
    "    df['volume_mean_10']=df.groupby(df['breath_id'])['volume_mean'].rolling(window=10, min_periods=1).sum().reset_index(level=0,drop=True)\n",
    "    df['volume_mean_20']=df.groupby(df['breath_id'])['volume_mean'].rolling(window=20, min_periods=1).sum().reset_index(level=0,drop=True)\n",
    "       \n",
    "    df['volume_in_cumsum']=df.groupby('breath_id')['volume_mean'].cumsum()     \n",
    "        \n",
    "    df['_volume'] = df['volume_in_cumsum'] * (1 - df['u_out'])\n",
    "    df['tidal_volume']=df.groupby(df['breath_id'])['_volume'].transform('max')\n",
    "    df['volume_out'] = -(df['tidal_volume']/ np.log(df['exhale'])).replace(np.inf,0)\n",
    "    \n",
    "    df['volume_out_cumsum']=df.groupby('breath_id')['volume_out'].cumsum() \n",
    "    \n",
    "    #df['u_in_rol_q0.1'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.1).reset_index(level=0,drop=True)\n",
    "    df['u_in_rol_q0.25'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.25).reset_index(level=0,drop=True)\n",
    "    #df['u_in_rol_q0.5'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.5).reset_index(level=0,drop=True)\n",
    "    df['u_in_rol_q0.75'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.75).reset_index(level=0,drop=True)\n",
    "    #df['u_in_rol_q0.9'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.9).reset_index(level=0,drop=True)  \n",
    "\n",
    "    df['PdV'] = df['delta_time']*df['volume_mean'] \n",
    "    df['PdV'] = df.groupby(df['breath_id'])['PdV'].cumsum()\n",
    "    df['PdV'] = df['PdV'] / df['C'] #см вд столба\n",
    "    \n",
    "    df['PdV-'] = df['delta_time']*df['volume_out']\n",
    "    df['PdV-'] = df.groupby(df['breath_id'])['PdV-'].cumsum()\n",
    "    df['PdV-'] = df['PdV-'] / df['C'] #см вд столба\n",
    "    \n",
    "    df['dP'] = df['PdV'] + (df['volume_mean'] * df['R']) /1000 #см вд столба\n",
    "    \n",
    "    df['RC'] = (df['R']*df['C']).astype(str)\n",
    "    df['R'] = df['R'].astype(str)\n",
    "    df['C'] = df['C'].astype(str)\n",
    "    df = pd.get_dummies(df)\n",
    "\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#\n",
    "train = add_features(train)\n",
    "test = add_features(test)\n",
    "#-------------cell------------------\n",
    "targets = train[['pressure']].to_numpy().reshape(-1, 80)\n",
    "train.drop(['id','breath_id','time_step','pressure'], axis=1, inplace=True)\n",
    "test = test.drop(['id','breath_id','time_step'], axis=1)\n",
    "#-------------cell------------------\n",
    "COLS = train.columns.to_numpy()\n",
    "#-------------cell------------------\n",
    "# train = reduce_mem_usage(train)\n",
    "# test = reduce_mem_usage(test)\n",
    "#-------------cell------------------\n",
    "RS = RobustScaler()\n",
    "train = RS.fit_transform(train)\n",
    "test = RS.transform(test)\n",
    "#-------------cell------------------\n",
    "train = train.reshape(-1, 80, train.shape[-1])\n",
    "test = test.reshape(-1, 80, train.shape[-1])\n",
    "#-------------cell------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdcd3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect hardware, return appropriate distribution strategy\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)\n",
    "#tf.get_logger().setLevel(logging.ERROR)\n",
    "try: # detect TPU\n",
    "    tpu = None\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError: # detect GPU(s) and enable mixed precision\n",
    "    strategy = tf.distribute.MirroredStrategy() # works on GPU and multi-GPU\n",
    "    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    tf.config.optimizer.set_jit(True) # XLA compilation\n",
    "    tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "    print('Mixed precision enabled')\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"loss\"])\n",
    "    plt.plot(hist.history[\"val_loss\"])\n",
    "    plt.title(\"model performance\")\n",
    "    plt.ylabel(\"mean_absolute_error\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.layers import MultiHeadAttention\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras \n",
    "class Time2Vec(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=1):\n",
    "        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n",
    "        self.k = kernel_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # trend\n",
    "        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
    "        # periodic\n",
    "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
    "        super(Time2Vec, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        bias = self.wb * inputs + self.bb\n",
    "        dp = K.dot(inputs, self.wa) + self.ba\n",
    "        wgts = K.sin(dp) # or K.cos(.)\n",
    "\n",
    "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
    "        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
    "        return ret\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]*(self.k + 1))\n",
    "#\n",
    "class AttentionBlock(tf.keras.Model):\n",
    "    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        if ff_dim is None:\n",
    "            ff_dim = head_size\n",
    "\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=head_size, dropout=dropout)\n",
    "        self.attention_dropout = keras.layers.Dropout(dropout)\n",
    "        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n",
    "        # self.ff_conv2 at build()\n",
    "        self.ff_dropout = keras.layers.Dropout(dropout)\n",
    "        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.attention([inputs, inputs])\n",
    "        x = self.attention_dropout(x)\n",
    "        x = self.attention_norm(inputs + x)\n",
    "\n",
    "        x = self.ff_conv1(x)\n",
    "        x = self.ff_conv2(x)\n",
    "        x = self.ff_dropout(x)\n",
    "\n",
    "        x = self.ff_norm(inputs + x)\n",
    "        return x\n",
    "#\n",
    "class ModelTrunk(tf.keras.Model):\n",
    "    def __init__(self, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
    "        if ff_dim is None:\n",
    "            ff_dim = head_size\n",
    "        self.dropout = dropout\n",
    "        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n",
    "        #self.pooling=tf.keras.layers.AveragePooling1D(pool_size=4,data_format='channels_first')\n",
    "        self.final_layer = Sequential([\n",
    "                Bidirectional(LSTM(256, return_sequences=True)),\n",
    "                Bidirectional(LSTM(256, return_sequences=True)),\n",
    "                Dense(128, activation='selu'),\n",
    "                Dense(1)])\n",
    "    def call(self, inputs):\n",
    "        time_embedding = tf.keras.layers.TimeDistributed(self.time2vec)(inputs)\n",
    "        #time_embedding=inputs\n",
    "        #print(time_embedding.shape)\n",
    "        x = K.concatenate([inputs, time_embedding], -1)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer(x)\n",
    "        x=self.final_layer(x)\n",
    "        return K.reshape(x, (-1, x.shape[1] * x.shape[2])) # flat vector of features out\n",
    "t2v=ModelTrunk(ff_dim=256,num_heads=4,num_layers=2,)\n",
    "x=tf.random.uniform((64,80,68))\n",
    "x=t2v(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr():\n",
    "    initial_learning_rate=0.001\n",
    "    decay_rate=0.9#1e-5\n",
    "    batch_size=512\n",
    "    train_len=60360\n",
    "    epoch_iter=int(train_len/batch_size)\n",
    "    decay_steps=5#200*epoch_iter\n",
    "    def decayed_learning_rate(step):\n",
    "        return initial_learning_rate * pow(decay_rate , (step / decay_steps))\n",
    "    lr_lst=[]\n",
    "    for i in range(300):\n",
    "        lr=decayed_learning_rate(i)\n",
    "        lr_lst.append(lr)\n",
    "    print(min(lr_lst))\n",
    "    plt.plot(lr_lst)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=126)\n",
    "\n",
    "test_preds = []\n",
    "batch_size=128\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
    "    print(f\"****** fold: {fold+1} *******\")\n",
    "    X_train, X_valid = train[train_idx], train[test_idx]\n",
    "    y_train, y_valid = targets[train_idx], targets[test_idx]\n",
    "    #scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 200*((len(train)*0.8)/batch_size), 1e-5)\n",
    "    scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 5, 0.9)\n",
    "    es = EarlyStopping(monitor='val_loss',mode='min', patience=35, verbose=1,restore_best_weights=True)\n",
    "    checkpoint_filepath = f\"folds{fold}.tf\"#f\"folds{fold}.hdf5\"\n",
    "#     sv = keras.callbacks.ModelCheckpoint(\n",
    "#             checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "#             save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "#             options=None\n",
    "#         )\n",
    "    with strategy.scope():\n",
    "        model = ModelTrunk(ff_dim=256,num_heads=8,num_layers=2,)\n",
    "        model.compile(optimizer=\"adam\",loss = \"mae\")\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=250, batch_size = batch_size,\n",
    "                        callbacks = [es,tf.keras.callbacks.LearningRateScheduler(scheduler)])\n",
    "    test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())\n",
    "    #plot_hist(history)\n",
    "    del X_train, X_valid, y_train, y_valid, model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n",
    "submission[\"pressure\"] =sum(test_preds)/len(test_preds)# test_preds[0]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a6956",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gf = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\n",
    "all_pressure = sorted(train_gf.pressure.unique())\n",
    "PRESSURE_MIN = all_pressure[0]\n",
    "PRESSURE_MAX = all_pressure[-1]\n",
    "PRESSURE_STEP = (all_pressure[1] - all_pressure[0])\n",
    "submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n",
    "submission[\"pressure\"] =np.round( (submission.pressure - PRESSURE_MIN)/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n",
    "submission.pressure = np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n",
    "pressure_unique = np.array(sorted(train_gf['pressure'].unique()))\n",
    "submission['pressure'] = submission['pressure'].map(lambda x: pressure_unique[np.abs(pressure_unique-x).argmin()])\n",
    "submission.to_csv('submission.csv', index = 0)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88558482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy, cudf, matplotlib.pyplot as plt\n",
    "# train_gf = cudf.read_csv('../input/ventilator-pressure-prediction/train.csv')\n",
    "# # plt.title('Histogram of Train Pressures',size=14)\n",
    "# # plt.hist(train_gf.sample(100_000).pressure.to_array(),bins=100)\n",
    "# # plt.show()\n",
    "# print('Max pressure =',train_gf.pressure.max(), 'Min pressure =',train_gf.pressure.min())\n",
    "# all_pressure = cupy.sort( train_gf.pressure.unique().values )\n",
    "# print('The first 25 unique pressures...')\n",
    "# PRESSURE_MIN = all_pressure[0].item()\n",
    "# PRESSURE_MAX = all_pressure[-1].item()\n",
    "# #\n",
    "# PRESSURE_STEP = ( all_pressure[1] - all_pressure[0] ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8794436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n",
    "# sub_1=pd.read_csv('../input/submission-list/submission172.csv')\n",
    "# sub_2=pd.read_csv('../input/submission-list/submission_mean_LB157.csv')\n",
    "# submission['pressure'] = (sub_1['pressure'].values*0.2)+(sub_2['pressure'].values*0.8)\n",
    "# submission[\"pressure\"]=np.round((submission.pressure-PRESSURE_MIN)/PRESSURE_STEP)*PRESSURE_STEP+PRESSURE_MIN\n",
    "# submission.pressure=np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ff915",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
