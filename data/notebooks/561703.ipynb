{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f035b93e",
   "metadata": {},
   "source": [
    "# Goal\n",
    "The goal is to use a simple model to classify x-ray images in Keras, the notebook how to use the ```flow_from_dataframe``` to deal with messier datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df = pd.read_csv('../input/Data_Entry_2017.csv')\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join('..', 'input', 'images*', '*', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "all_xray_df['Patient Age'] = all_xray_df['Patient Age'].map(lambda x: int(x[:-1]))\n",
    "all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8fdfa2",
   "metadata": {},
   "source": [
    "# Preprocessing Labels\n",
    "Here we take the labels and make them into a more clear format. The primary step is to see the distribution of findings and then to convert them to simple binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5702c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = all_xray_df['Finding Labels'].value_counts()[:15]\n",
    "fig, ax1 = plt.subplots(1,1,figsize = (12, 8))\n",
    "ax1.bar(np.arange(len(label_counts))+0.5, label_counts)\n",
    "ax1.set_xticks(np.arange(len(label_counts))+0.5)\n",
    "_ = ax1.set_xticklabels(label_counts.index, rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df['Finding Labels'] = all_xray_df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\n",
    "from itertools import chain\n",
    "all_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "all_labels = [x for x in all_labels if len(x)>0]\n",
    "print('All Labels ({}): {}'.format(len(all_labels), all_labels))\n",
    "for c_label in all_labels:\n",
    "    if len(c_label)>1: # leave out empty labels\n",
    "        all_xray_df[c_label] = all_xray_df['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)\n",
    "all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724eba4c",
   "metadata": {},
   "source": [
    "### Clean categories\n",
    "Since we have too many categories, we can prune a few out by taking the ones with only a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep at least 1000 cases\n",
    "MIN_CASES = 1000\n",
    "all_labels = [c_label for c_label in all_labels if all_xray_df[c_label].sum()>MIN_CASES]\n",
    "print('Clean Labels ({})'.format(len(all_labels)), \n",
    "      [(c_label,int(all_xray_df[c_label].sum())) for c_label in all_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the dataset is very unbiased, we can resample it to be a more reasonable collection\n",
    "# weight is 0.1 + number of findings\n",
    "sample_weights = all_xray_df['Finding Labels'].map(lambda x: len(x.split('|')) if len(x)>0 else 0).values + 4e-2\n",
    "sample_weights /= sample_weights.sum()\n",
    "all_xray_df = all_xray_df.sample(40000, weights=sample_weights)\n",
    "\n",
    "label_counts = all_xray_df['Finding Labels'].value_counts()[:15]\n",
    "fig, ax1 = plt.subplots(1,1,figsize = (12, 8))\n",
    "ax1.bar(np.arange(len(label_counts))+0.5, label_counts)\n",
    "ax1.set_xticks(np.arange(len(label_counts))+0.5)\n",
    "_ = ax1.set_xticklabels(label_counts.index, rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = 100*np.mean(all_xray_df[all_labels].values,0)\n",
    "fig, ax1 = plt.subplots(1,1,figsize = (12, 8))\n",
    "ax1.bar(np.arange(len(label_counts))+0.5, label_counts)\n",
    "ax1.set_xticks(np.arange(len(label_counts))+0.5)\n",
    "ax1.set_xticklabels(all_labels, rotation = 90)\n",
    "ax1.set_title('Adjusted Frequency of Diseases in Patient Group')\n",
    "_ = ax1.set_ylabel('Frequency (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a1bb6",
   "metadata": {},
   "source": [
    "# Prepare Training Data\n",
    "Here we split the data into training and validation sets and create a single vector (disease_vec) with the 0/1 outputs for the disease status (what the model will try and predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df['disease_vec'] = all_xray_df.apply(lambda x: [x[all_labels].values], 1).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(all_xray_df, \n",
    "                                   test_size = 0.25, \n",
    "                                   random_state = 2018,\n",
    "                                   stratify = all_xray_df['Finding Labels'].map(lambda x: x[:4]))\n",
    "print('train', train_df.shape[0], 'validation', valid_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fe88a",
   "metadata": {},
   "source": [
    "# Create Data Generators\n",
    "Here we make the data generators for loading and randomly transforming images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bec8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "IMG_SIZE = (128, 128)\n",
    "core_idg = ImageDataGenerator(samplewise_center=True, \n",
    "                              samplewise_std_normalization=True, \n",
    "                              horizontal_flip = True, \n",
    "                              vertical_flip = False, \n",
    "                              height_shift_range= 0.05, \n",
    "                              width_shift_range=0.1, \n",
    "                              rotation_range=5, \n",
    "                              shear_range = 0.1,\n",
    "                              fill_mode = 'reflect',\n",
    "                              zoom_range=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n",
    "    base_dir = os.path.dirname(in_df[path_col].values[0])\n",
    "    print('## Ignore next message from keras, values are replaced anyways')\n",
    "    df_gen = img_data_gen.flow_from_directory(base_dir, \n",
    "                                     class_mode = 'sparse',\n",
    "                                    **dflow_args)\n",
    "    df_gen.filenames = in_df[path_col].values\n",
    "    df_gen.classes = np.stack(in_df[y_col].values)\n",
    "    df_gen.samples = in_df.shape[0]\n",
    "    df_gen.n = in_df.shape[0]\n",
    "    df_gen._set_index_array()\n",
    "    df_gen.directory = '' # since we have the full path\n",
    "    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n",
    "    return df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = flow_from_dataframe(core_idg, train_df, \n",
    "                             path_col = 'path',\n",
    "                            y_col = 'disease_vec', \n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'grayscale',\n",
    "                            batch_size = 32)\n",
    "\n",
    "valid_gen = flow_from_dataframe(core_idg, valid_df, \n",
    "                             path_col = 'path',\n",
    "                            y_col = 'disease_vec', \n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'grayscale',\n",
    "                            batch_size = 256) # we can use much larger batches for evaluation\n",
    "# used a fixed dataset for evaluating the algorithm\n",
    "test_X, test_Y = next(flow_from_dataframe(core_idg, \n",
    "                               valid_df, \n",
    "                             path_col = 'path',\n",
    "                            y_col = 'disease_vec', \n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'grayscale',\n",
    "                            batch_size = 1024)) # one big batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_y = next(train_gen)\n",
    "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -1.5, vmax = 1.5)\n",
    "    c_ax.set_title(', '.join([n_class for n_class, n_score in zip(all_labels, c_y) \n",
    "                             if n_score>0.5]))\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c97758",
   "metadata": {},
   "source": [
    "# Create a simple model\n",
    "Here we make a simple model to train using MobileNet as a base and then adding a GAP layer (Flatten could also be added), dropout, and a fully-connected layer to calculate specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfabb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "base_mobilenet_model = MobileNet(input_shape =  t_x.shape[1:], \n",
    "                                 include_top = False, weights = None)\n",
    "multi_disease_model = Sequential()\n",
    "multi_disease_model.add(base_mobilenet_model)\n",
    "multi_disease_model.add(GlobalAveragePooling2D())\n",
    "multi_disease_model.add(Dropout(0.5))\n",
    "multi_disease_model.add(Dense(512))\n",
    "multi_disease_model.add(Dropout(0.5))\n",
    "multi_disease_model.add(Dense(len(all_labels), activation = 'sigmoid'))\n",
    "multi_disease_model.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
    "                           metrics = ['binary_accuracy', 'mae'])\n",
    "multi_disease_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d46d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('xray_class')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=3)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c022dd",
   "metadata": {},
   "source": [
    "# First Round\n",
    "Here we do a first round of training to get a few initial low hanging fruit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b19205",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_disease_model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch=100,\n",
    "                                  validation_data = (test_X, test_Y), \n",
    "                                  epochs = 1, \n",
    "                                  callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df57e5",
   "metadata": {},
   "source": [
    "# Check Output\n",
    "Here we see how many positive examples we have of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ff334",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_label, s_count in zip(all_labels, 100*np.mean(test_Y,0)):\n",
    "    print('%s: %2.2f%%' % (c_label, s_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b23c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = multi_disease_model.predict(test_X, batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442f352",
   "metadata": {},
   "source": [
    "# ROC Curves\n",
    "While a very oversimplified metric, we can show the ROC curve for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3fdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
    "for (idx, c_label) in enumerate(all_labels):\n",
    "    fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])\n",
    "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "fig.savefig('barely_trained_net.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0839a3",
   "metadata": {},
   "source": [
    "# Continued Training\n",
    "Now we do a much longer training process to see how the results improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b178a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_disease_model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch = 100,\n",
    "                                  validation_data =  (test_X, test_Y), \n",
    "                                  epochs = 5, \n",
    "                                  callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best weights\n",
    "multi_disease_model.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42673a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = multi_disease_model.predict(test_X, batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at how often the algorithm predicts certain diagnoses \n",
    "for c_label, p_count, t_count in zip(all_labels, \n",
    "                                     100*np.mean(pred_Y,0), \n",
    "                                     100*np.mean(test_Y,0)):\n",
    "    print('%s: Dx: %2.2f%%, PDx: %2.2f%%' % (c_label, t_count, p_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7060773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
    "for (idx, c_label) in enumerate(all_labels):\n",
    "    fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])\n",
    "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "fig.savefig('trained_net.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d226d8",
   "metadata": {},
   "source": [
    "# Show a few images and associated predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sickest_idx = np.argsort(np.sum(test_Y, 1)<1)\n",
    "fig, m_axs = plt.subplots(4, 2, figsize = (16, 32))\n",
    "for (idx, c_ax) in zip(sickest_idx, m_axs.flatten()):\n",
    "    c_ax.imshow(test_X[idx, :,:,0], cmap = 'bone')\n",
    "    stat_str = [n_class[:6] for n_class, n_score in zip(all_labels, \n",
    "                                                                  test_Y[idx]) \n",
    "                             if n_score>0.5]\n",
    "    pred_str = ['%s:%2.0f%%' % (n_class[:4], p_score*100)  for n_class, n_score, p_score in zip(all_labels, \n",
    "                                                                  test_Y[idx], pred_Y[idx]) \n",
    "                             if (n_score>0.5) or (p_score>0.5)]\n",
    "    c_ax.set_title('Dx: '+', '.join(stat_str)+'\\nPDx: '+', '.join(pred_str))\n",
    "    c_ax.axis('off')\n",
    "fig.savefig('trained_img_predictions.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
