{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "model_path = 'swin_large_patch4_window7_224' # this is an exmaple. Please see used models described in https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/300929\n",
    "img_size = 224\n",
    "\n",
    "exp_name = 'exp8'\n",
    "model_dir = f'{exp_name}_' + model_path.replace('/', '_')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 8 ### I used BATCH_SIZE = 8 if mamory allows. Otherwise, I used BATCH_SIZE = 4\n",
    "\n",
    "n_epochs = 1 ### change this to 20 for reproduction\n",
    "\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "sys.path = [\"../input/pytorch-1-10-1/\"] + sys.path\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "from timm import create_model\n",
    "from timm.data.mixup import Mixup\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "set_seed(999, reproducible=True)\n",
    "\n",
    "\n",
    "\n",
    "# Let's check what data is available to us:\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "dataset_path = Path('../input/petfinder-pawpularity-score/')\n",
    "dataset_path.ls()\n",
    "\n",
    "\n",
    "# We can see that we have our train csv file with the train image names, metadata and labels, the test csv file with test image names and metadata, the sample submission csv with the test image names, and the train and test image folders.\n",
    "# \n",
    "# Let's check the train csv file:\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(dataset_path/'train.csv')\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "# The metadata provided includes information about key visual quality and composition parameters of the photos. The Pawpularity Score is derived from the profile's page view statistics. This is the target we are aiming to predict.\n",
    "\n",
    "# Let's do some quick processing of the image filenames to make it easier to access:\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "train_df['path'] = train_df['Id'].map(lambda x:str(dataset_path/'train'/x)+'.jpg')\n",
    "train_df = train_df.drop(columns=['Id'])\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "# Okay, let's check how many images are available in the training dataset:\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "len_df = len(train_df)\n",
    "print(f\"There are {len_df} images\")\n",
    "\n",
    "\n",
    "# Let's check the distribution of the Pawpularity Score:\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "train_df['Pawpularity'].hist(figsize = (10, 5))\n",
    "print(f\"The mean Pawpularity score is {train_df['Pawpularity'].mean()}\")\n",
    "print(f\"The median Pawpularity score is {train_df['Pawpularity'].median()}\")\n",
    "print(f\"The standard deviation of the Pawpularity score is {train_df['Pawpularity'].std()}\")\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "print(f\"There are {len(train_df['Pawpularity'].unique())} unique values of Pawpularity score\")\n",
    "\n",
    "\n",
    "# Note that the Pawpularity score is an integer, so in addition to being a regression problem, it could also be treated as a 100-class classification problem. Alternatively, it can be treated as a binary classification problem if the Pawpularity Score is normalized between 0 and 1:\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "train_df['norm_score'] = train_df['Pawpularity']/100\n",
    "train_df['norm_score']\n",
    "\n",
    "\n",
    "# Let's check an example image to see what it looks like:\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "im = Image.open(train_df['path'][1])\n",
    "width, height = im.size\n",
    "print(width,height)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "im\n",
    "\n",
    "\n",
    "# ## Data loading\n",
    "# After my quick 'n dirty EDA, let's load the data into fastai as DataLoaders objects. We're using the normalized score as the label. I use some fairly basic augmentations here.\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "#if not os.path.exists('/root/.cache/torch/hub/checkpoints/'):\n",
    "#    os.makedirs('/root/.cache/torch/hub/checkpoints/')\n",
    "#get_ipython().system(\"cp '../input/swin-transformer/swin_large_patch4_window7_224_22kto1k.pth' '/root/.cache/torch/hub/checkpoints/swin_large_patch4_window7_224_22kto1k.pth'\")\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "seed=365\n",
    "set_seed(seed, reproducible=True)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms = True\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "#Sturges' rule\n",
    "num_bins = int(np.floor(1+(3.3)*(np.log2(len(train_df)))))\n",
    "# num_bins\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "train_df['bins'] = pd.cut(train_df['norm_score'], bins=num_bins, labels=False)\n",
    "train_df['bins'].hist()\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "train_df['fold'] = -1\n",
    "\n",
    "\n",
    "N_FOLDS = 5\n",
    "strat_kfold = StratifiedKFold(n_splits=N_FOLDS, random_state=seed, shuffle=True)\n",
    "for i, (_, train_index) in enumerate(strat_kfold.split(train_df.index, train_df['bins'])):\n",
    "    train_df.iloc[train_index, -1] = i\n",
    "    \n",
    "train_df['fold'] = train_df['fold'].astype('int')\n",
    "\n",
    "train_df.fold.value_counts().plot.bar()\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "train_df[train_df['fold']==0].head()\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "def petfinder_rmse(input,target):\n",
    "    return F.mse_loss(torch.clip(input.flatten(), 0.01, 1), target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "transforms = aug_transforms(do_flip=True,\n",
    "                            flip_vert=False,\n",
    "                            max_rotate=10.0,\n",
    "                            max_zoom=1.1, \n",
    "                            max_lighting=0.2,\n",
    "                            max_warp=0.2,\n",
    "                            p_affine=0.5, \n",
    "                            p_lighting=0.5, \n",
    "                            xtra_tfms=[Brightness(), Contrast(), Hue(), Saturation()])\n",
    "\n",
    "def get_data(fold):\n",
    "#     train_df_no_val = train_df.query(f'fold != {fold}')\n",
    "#     train_df_val = train_df.query(f'fold == {fold}')\n",
    "    \n",
    "#     train_df_bal = pd.concat([train_df_no_val,train_df_val.sample(frac=1).reset_index(drop=True)])\n",
    "    train_df_f = train_df.copy()\n",
    "    # add is_valid for validation fold\n",
    "    train_df_f['is_valid'] = (train_df_f['fold'] == fold)\n",
    "    \n",
    "\n",
    "    dls = ImageDataLoaders.from_df(train_df_f, #pass in train DataFrame\n",
    "#                                valid_pct=0.2, #80-20 train-validation random split\n",
    "                               valid_col='is_valid', #\n",
    "                               seed=365, #seed\n",
    "                               fn_col='path', #filename/path is in the second column of the DataFrame\n",
    "                               label_col='norm_score', #label is in the first column of the DataFrame\n",
    "                               y_block=RegressionBlock, #The type of target\n",
    "                               bs=BATCH_SIZE, #pass in batch size\n",
    "                               num_workers=8,\n",
    "                               item_tfms=Resize(img_size),\n",
    "                               batch_tfms=transforms #pass in batch_tfms\n",
    "                               )\n",
    "    \n",
    "    return dls\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "#Valid Kfolder size\n",
    "the_data = get_data(0)\n",
    "#assert (len(the_data.train) + len(the_data.valid)) == (len(train_df)//BATCH_SIZE)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "def get_learner(fold_num):\n",
    "    data = get_data(fold_num)\n",
    "    \n",
    "    model = create_model(model_path, pretrained=True, num_classes=data.c)\n",
    "    #model = torch.nn.DataParallel(model)\n",
    "    learn = Learner(data, model, model_dir=model_dir, \n",
    "                    loss_func=MSELossFlat(), \n",
    "                    metrics=petfinder_rmse,\n",
    "                    cbs=[MixUp(0.2)]).to_fp16()\n",
    "    \n",
    "    return learn\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(dataset_path/'test.csv')\n",
    "test_df.head(100)\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "test_df['Pawpularity'] = [1]*len(test_df)\n",
    "test_df['path'] = test_df['Id'].map(lambda x:str(dataset_path/'test'/x)+'.jpg')\n",
    "test_df = test_df.drop(columns=['Id'])\n",
    "train_df['norm_score'] = train_df['Pawpularity']/100\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "#train_df = train_df.append(train_df_prev, ignore_index=True).append(train_df_dog, ignore_index=True)\n",
    "#train_df = train_df.append(train_df_dog, ignore_index=True)\n",
    "\n",
    "train_df\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "try:\n",
    "    get_learner(fold_num=0).lr_find(end_lr=3e-2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "losses = []\n",
    "\n",
    "train_df['pred'] = 0\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    print(f'= {i} results')\n",
    "    \n",
    "    learn = get_learner(fold_num=i)\n",
    "\n",
    "    learn.fit_one_cycle(n_epochs, 2e-5, \n",
    "                            cbs=[\n",
    "                            SaveModelCallback(fname=f'{model_path}_{i}', \n",
    "                                                   monitor='petfinder_rmse',\n",
    "                                                   every_epoch=False,\n",
    "                                                    comp=np.less),\n",
    "                            EarlyStoppingCallback(monitor='petfinder_rmse', comp=np.less, patience=3),\n",
    "                            CSVLogger(fname=model_dir + '/hist.log', append=True)],\n",
    "                            ) \n",
    "    \n",
    "    val_preds, val_targets = learn.get_preds(1)\n",
    "    loss = petfinder_rmse(val_preds, val_targets)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    train_df.loc[train_df['fold'] == i, 'pred'] = val_preds[:, 0].cpu().numpy()\n",
    "    \n",
    "    \n",
    "    gc.collect()\n",
    "    print('RMSE', np.mean(losses))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "train_df.to_csv(model_dir + '/train_cv_score.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
