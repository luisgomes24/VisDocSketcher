{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e70cda",
   "metadata": {},
   "source": [
    "# Kaggle Workshop Walkthrough\n",
    "\n",
    "Example walkthrough for the House Price competition. This notebook shows a possible simple approach for each of the workshop proposed tasks. Consider this a simple baseline, you can to better üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a097a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce199e",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawtrain = pd.read_csv('../input/train.csv')\n",
    "rawtest = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a2bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train shape:', rawtrain.shape)\n",
    "print('Test shape:', rawtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c644dc8",
   "metadata": {},
   "source": [
    "These are the types of the columns in the dataset. `np.object` are string values for the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawtrain.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7232c7",
   "metadata": {},
   "source": [
    "# First model with selected features\n",
    "\n",
    "To make it a bit easier to do the first steps with the dataset you can use the following list with the 20 most important features (this list is the result of running a gradient boosting model and selecting the most important features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ['GrLivArea',\n",
    " 'LotArea',\n",
    " 'BsmtUnfSF',\n",
    " '1stFlrSF',\n",
    " 'TotalBsmtSF',\n",
    " 'GarageArea',\n",
    " 'BsmtFinSF1',\n",
    " 'LotFrontage',\n",
    " 'YearBuilt',\n",
    " 'Neighborhood',\n",
    " 'GarageYrBlt',\n",
    " 'OpenPorchSF',\n",
    " 'YearRemodAdd',\n",
    " 'WoodDeckSF',\n",
    " 'MoSold',\n",
    " '2ndFlrSF',\n",
    " 'OverallCond',\n",
    " 'Exterior1st',\n",
    " 'YrSold',\n",
    " 'OverallQual']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeafa90",
   "metadata": {},
   "source": [
    "Or you can just select everything if you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = [c for c in test.columns if c not in ['Id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82137185",
   "metadata": {},
   "source": [
    "This code builds a single dataframe with both `train` and `test` datasets and a new column to separate both. This can be useful when doing transformations that would need to be applied both in `train` and `test`. If you keep this approach you can use the checking code that is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rawtrain[selected].copy()\n",
    "train['is_train'] = 1\n",
    "train['SalePrice'] = rawtrain['SalePrice'].values\n",
    "train['Id'] = rawtrain['Id'].values\n",
    "\n",
    "test = rawtest[selected].copy()\n",
    "test['is_train'] = 0\n",
    "test['SalePrice'] = 1  #dummy value\n",
    "test['Id'] = rawtest['Id'].values\n",
    "\n",
    "full = pd.concat([train, test])\n",
    "\n",
    "not_features = ['Id', 'SalePrice', 'is_train']\n",
    "features = [c for c in train.columns if c not in not_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef9751",
   "metadata": {},
   "source": [
    "# Check target distribution\n",
    "\n",
    "The competition metric is based on log transformed values. That is already an hint that log transform maybe useful to make the target distribution behave more like a normal distribution.\n",
    "\n",
    "Now plot the distribution of `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cd627",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(train.SalePrice).hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.log(train.SalePrice)).hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6c23c",
   "metadata": {},
   "source": [
    "And apply the log transformation to `SalePrice` in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "full['SalePrice'] = np.log(full['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdd8c0",
   "metadata": {},
   "source": [
    "# Check missing values\n",
    "\n",
    "Do some analysis to identify the missing values. There is a proposed summary function that can be used to check missing values for the different dtypes (`np.object`, `np.float64`, `np.int64`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbdfff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df, dtype):\n",
    "    data = []\n",
    "    for c in df.select_dtypes([dtype]).columns:\n",
    "        data.append({'name': c, 'unique': df[c].nunique(), \n",
    "                     'nulls': df[c].isnull().sum(),\n",
    "                     'samples': df[c].unique()[:20] })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b670d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(full[features], np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee199ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(full[features], np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d958ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(full[features], np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3e30f",
   "metadata": {},
   "source": [
    "Now do something to replace the missing values. The best is to analyse case by case. A quick lazy approach can be to use a new label missing categoricals and zero for missing numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in full.select_dtypes([np.object]).columns:\n",
    "    full[c].fillna('__NA__', inplace=True)\n",
    "for c in full.select_dtypes([np.float64]).columns:\n",
    "    full[c].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4002d9d",
   "metadata": {},
   "source": [
    "Code to check there are no missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in full.columns:\n",
    "    assert full[c].isnull().sum() == 0, f'There are still missing values in {c}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d67ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb759fe",
   "metadata": {},
   "source": [
    "# Encode categorical\n",
    "\n",
    "Before creating the model the categorical features must be encoded into numberical values. There are many ways to do it, for example building a mapping dictionary and applying it with pandas. Or using `sklearn.preprocessing.LabelEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappers = {}\n",
    "for c in full.select_dtypes([np.object]).columns:\n",
    "    mappers[c] = {v:i for i,v in enumerate(full[c].unique())}\n",
    "    full[c] = full[c].map(mappers[c]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84259277",
   "metadata": {},
   "source": [
    "Code to check that all columns are numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f134606",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in full.columns:\n",
    "    assert is_numeric_dtype(full[c]), f'Non-numeric column {c}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55236783",
   "metadata": {},
   "source": [
    "# First model\n",
    "\n",
    "Now try to build a first predictive model. One suggestion is to use gradient boosting that typically has strong results in this kind of tabular data (in `sklearn` there is a `GradientBoostingRegress` model). If you choose to do first a Linear regression don't forget to also one-hot encode the categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156b3b4",
   "metadata": {},
   "source": [
    "Implementation of the competition metric (notice target is already log transformed so no need to do that in the metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736a7b1",
   "metadata": {},
   "source": [
    "Choose a validation strategy for your model. Simple approach is to take out a validation dataset from the train dataset (`sklearn.model_selection.train_test_split` can be used for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = full[full.is_train==1][features].values\n",
    "target = full[full.is_train==1].SalePrice.values\n",
    "Xtrain, Xvalid, ytrain, yvalid = train_test_split(train, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f4e64",
   "metadata": {},
   "source": [
    "Notice these model parameters is just a first guess. With parameter optimization the model results can be improved (for example using `sklearn.model_selection.RandomizedSearchCV`, left as a follow-up exercise). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35738c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=1500, learning_rate=0.02, max_depth=4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e67d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(Xvalid)\n",
    "rmse(yvalid, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56283f",
   "metadata": {},
   "source": [
    "Now applying the model to the test dataset, to generate the predictions to be submitted as results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = full[full.is_train==0]\n",
    "ytestpred = model.predict(test[features].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cfabf2",
   "metadata": {},
   "source": [
    "Since target was log transformed it needs to be exponentiated now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytestpred = np.exp(ytestpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = pd.DataFrame(ytestpred, index=test['Id'], columns=['SalePrice'])\n",
    "subm.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b8951",
   "metadata": {},
   "source": [
    "üéâ Great! Submission ready üí™ Now time to upload to Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dcd389",
   "metadata": {},
   "source": [
    "# Remove Outliers\n",
    "\n",
    "It is worth checking the data for outliers and try a model with some outliers removed. Suggested task is to plot a boxplot for each numerical variable. Then filter out some outliers in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3eb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = full[features].select_dtypes([np.float64, np.int64]).columns\n",
    "n_rows = math.ceil(len(cols)/2)\n",
    "fig, ax = plt.subplots(n_rows, 2, figsize=(14, n_rows*2))\n",
    "ax = ax.flatten()\n",
    "for i,c in enumerate(cols):\n",
    "    sns.boxplot(x=full[c], ax=ax[i])\n",
    "    ax[i].set_title(c)\n",
    "    ax[i].set_xlabel(\"\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f97fd",
   "metadata": {},
   "source": [
    "This code will remove some rows based on predefined limits. This is meant to be just example code, probably there is no reason to remove entries. This is a carefully cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = [('TotalBsmtSF', 4000), ('WoodDeckSF', 1400)]\n",
    "\n",
    "full['__include'] = 1 \n",
    "for c, val in limits:\n",
    "    full.loc[full[c] > val, '__include'] = 0\n",
    "\n",
    "full = full[(full.is_train==0)|(full['__include']==1)]\n",
    "\n",
    "full = full.drop('__include', axis=1)\n",
    "\n",
    "# these dates in the future are likely typos\n",
    "full['GarageYrBlt'] = np.where(full.GarageYrBlt > 2010, full.YearBuilt, full.GarageYrBlt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb65aa",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Some ideas for new features:\n",
    "\n",
    "- House age (considering the construction year and that this is a dataset from 2010)\n",
    "- What season was the house sold (winter, summer, etc)\n",
    "- Reduce the overal condition to 3 levels (good, average, bad)\n",
    "- How long ago the house was sold\n",
    "- Total area including first and second floor\n",
    "- Total area including first floor, second floor and basement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e409e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "full['Age'] = 2010 - full['YearBuilt']\n",
    "month_season_map = {12:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3}\n",
    "full['SeasonSold'] = full['MoSold'].map(month_season_map).astype(int)\n",
    "full['SimplOverallCond'] = full['OverallCond'].replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\n",
    "full['TimeSinceSold'] =  2010 - full['YrSold']\n",
    "full['TotalArea1st2nd'] = full['1stFlrSF'] + full['2ndFlrSF']\n",
    "full['TotalSF'] = full['TotalBsmtSF'] + full['1stFlrSF'] + full['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3b30e",
   "metadata": {},
   "source": [
    "# Blend 2 models\n",
    "\n",
    "Now try to make 2 different models (for example GBM and ExtraTrees or RandomForest), combine the models (for example with a weighted average) and evaluate the performance in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = full[full.is_train==1][features].values\n",
    "target = full[full.is_train==1].SalePrice.values\n",
    "Xtrain, Xvalid, ytrain, yvalid = train_test_split(train, target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=1500, learning_rate=0.02, max_depth=4, random_state=42)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xvalid)\n",
    "rmse(yvalid, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b5471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ExtraTreesRegressor(n_estimators=1500, random_state=42)\n",
    "model2.fit(Xtrain, ytrain)\n",
    "ypred2 = model2.predict(Xvalid)\n",
    "rmse(yvalid, ypred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955044ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "blendpred = 0.7*ypred + 0.3*ypred2\n",
    "rmse(yvalid, blendpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b796e3",
   "metadata": {},
   "source": [
    "If the performance improved in you CV, do another submission on Kaggle to check the value on `test`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = full[full.is_train==0]\n",
    "ytestpred = model.predict(test[features].values)\n",
    "ytestpred2 = model2.predict(test[features].values)\n",
    "blendtestpred = 0.7*ytestpred + 0.3*ytestpred2\n",
    "\n",
    "blendtestpred = np.exp(blendtestpred)\n",
    "\n",
    "subm = pd.DataFrame(blendtestpred, index=test['Id'], columns=['SalePrice'])\n",
    "subm.to_csv('submission_blend.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2b71d",
   "metadata": {},
   "source": [
    "Well Done! üèÜ Now just keep the momentum and go for the gold ü•áü•áü•áü•áüöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4ae23",
   "metadata": {},
   "source": [
    "# Extra: if you still have some time\n",
    "\n",
    "Try the following:\n",
    "\n",
    "- K-fold CV \n",
    "- Liner regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c481766",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
