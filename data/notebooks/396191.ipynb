{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19915299",
   "metadata": {},
   "source": [
    "## Titanic: Machine Learning from Disaster  \n",
    "\n",
    "[RMS Titanic](https://en.wikipedia.org/wiki/RMS_Titanic), started its journey from Southampton on 10-Apr-1912, it then called at Cherboug and then visited Queenstown. It drowned en-route New York on 15-Apr-1912. Now, we have a dataset of people onboard. Most of them died in the accident but some of them did survive. Here, I have tried to create a Neural Network that tries to predict, given the data, the chance a person survived or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ac791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from helpers.clean_titanic import load_dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3426b",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "The first step, is ofcourse loading the dataset. Since, the dataset is not big enough we will load it all into the memory itself. The function load_dataset takes in an int, *sample_size* and returns 4 parameters:   \n",
    "1. train_features (7, sample_size)  dictionary -> The Training data  \n",
    "2. train_labels (sample_size,) list -> True Training labels  \n",
    "3. test_features (7, total_size - sample_size) -> The Dev set basically  \n",
    "4. test_labels (total_size - sample_size,) -> The true labels for dev set  \n",
    "*Here all the references for total_size means the number of the rows returned by read_csv function*\n",
    "\n",
    "\n",
    "But, before we go on to load the dataset into the memory, one more thing needs to be taken care of:  \n",
    "**Imputing the missing values**   \n",
    "The features we care about are:  \n",
    "* Age: The age of the passenger at the time of accident in years  \n",
    "* Embarked: The boarding point of the passenger (S = \"Southampton\", Q = \"Queensberg\", C = \"Cherboug\")  \n",
    "* Sex: Male/Female  \n",
    "* Pclass: The class in which they were travelling (1st, 2nd or 3rd)  \n",
    "* SibSp: Number of siblings  \n",
    "* Parch: Number of parents or children  \n",
    "* Fare: The fare of the ticket  \n",
    "\n",
    "Now, let's see how we can *best guess* the missing data manually:  \n",
    "1. Embarked:  \n",
    "   Turns out, only two ladies in the dataset have their embarked missing. So, filling it as \"S\" for both seems to be a reasonable starting point.  \n",
    "1. Age:  \n",
    "   * The age can be guessed by looking at the name. Take the **median** of the ages having same **Embarked**, **Surivived** and **Pclass**, having the same name prefix e.g. **Master**.  \n",
    "   \n",
    "3. Ignored Features:  \n",
    "   The features like \"Name\", \"Ticket\" and \"Cabin\" are ignored by the algorithm, so we can simply place a placeholder at these places in order to avoid getting dropped by NA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_age(titanic_data):\n",
    "    \"\"\"\n",
    "    Input: ages_missing (dataframe) having keys (\"Survived\", \"names\", \"Embarked\", \"ages\", \"Pclass\")\n",
    "    outputs : array containing all the ages including the imputed data\n",
    "    \"\"\"\n",
    "    for i in np.squeeze(np.where(pd.isnull(titanic_data[\"Age\"]))):\n",
    "        prefix = \"\"\n",
    "        if \"Mr\" in titanic_data.Name[i]:\n",
    "            prefix = \"Mr\"\n",
    "        elif \"Mrs\" in titanic_data.Name[i]:\n",
    "            prefix = \"Mrs\"\n",
    "        elif \"Miss\" in titanic_data.Name[i]:\n",
    "            prefix = \"Miss\"\n",
    "        else:\n",
    "            prefix = \"Master\"\n",
    "        titanic_subset = titanic_data[(titanic_data.Embarked == titanic_data.Embarked[i])\n",
    "                                      & (titanic_data.Pclass == titanic_data.Pclass[i]) &\n",
    "                                     (titanic_data.Survived == titanic_data.Survived[i])&\n",
    "                                      (titanic_data.Sex == titanic_data.Sex[i] ) &\n",
    "                                      (titanic_data.Name.str.contains(prefix))\n",
    "                                     ]\n",
    "        titanic_data.Age[i] = titanic_subset.Age.median()\n",
    "    return titanic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(titanic_data):\n",
    "    titanic_data[\"Embarked\"] = titanic_data[\"Embarked\"].fillna(value = \"S\")\n",
    "    titanic_data = impute_age(titanic_data)\n",
    "    titanic_data[\"Name\"] = titanic_data[\"Name\"].fillna(value = \"\")\n",
    "    titanic_data[\"Ticket\"] = titanic_data[\"Ticket\"].fillna(value = \"\")\n",
    "    titanic_data[\"Cabin\"] = titanic_data[\"Cabin\"].fillna(value = \"\")\n",
    "    return titanic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(sample_size = 500):\n",
    "    \"\"\"\n",
    "    returns the features and labels of training and testing set indivisually\n",
    "    \"\"\"\n",
    "    #read the data\n",
    "    train_data = read_csv(\"../input/titanic/train.csv\")\n",
    "    train_data = impute_data(train_data)\n",
    "    #train_data = train_data.sample(frac = 1).reset_index(drop = True)\n",
    "    train_data = train_data.drop(axis = 1, labels = [\"Name\",\"Ticket\",\"Cabin\"]).dropna()\n",
    "    #setting data into indivisual arrays\n",
    "    embarked = train_data[\"Embarked\"].replace([\"S\",\"C\",\"Q\"], [1,2,3]).as_matrix()\n",
    "    sex = train_data[\"Sex\"].replace([\"male\",\"female\"], [0,1]).as_matrix()\n",
    "    pclass = train_data[\"Pclass\"].as_matrix()\n",
    "    age = train_data[\"Age\"].as_matrix()\n",
    "    sibsp = train_data[\"SibSp\"].as_matrix()\n",
    "    parch = train_data[\"Parch\"].as_matrix()\n",
    "    fare = train_data[\"Fare\"].as_matrix()\n",
    "    survived = train_data[\"Survived\"].tolist()\n",
    "    \n",
    "    #sample space size\n",
    "    sample_size = 500\n",
    "    #create a feature vector and labels for training set\n",
    "    train_features = {\n",
    "        \"pclass\" : pclass[:sample_size],\n",
    "        \"age\" : age[:sample_size],\n",
    "        \"sex\" : sex[:sample_size],\n",
    "        \"sibsp\" : sibsp[:sample_size],\n",
    "        \"parch\" : parch[:sample_size],\n",
    "        \"fare\" : fare[:sample_size],\n",
    "        \"embarked\":embarked[:sample_size]\n",
    "    }\n",
    "    train_labels = survived[:sample_size]\n",
    "    \n",
    "    #create a feature vector and labels for test set\n",
    "    test_features = {\n",
    "         \"pclass\" : pclass[sample_size:],\n",
    "         \"age\" : age[sample_size:],\n",
    "         \"sex\" : sex[sample_size:],\n",
    "         \"sibsp\" : sibsp[sample_size:],\n",
    "         \"parch\" : parch[sample_size:],\n",
    "         \"fare\" : fare[sample_size:],\n",
    "        \"embarked\": embarked[sample_size:]\n",
    "    }\n",
    "    \n",
    "    test_labels = survived[sample_size:]\n",
    "    return train_features, train_labels, test_features, test_labels, sample_size, train_data.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d8473",
   "metadata": {},
   "source": [
    "## Setting Up Neural Network Architecture\n",
    "\n",
    "First we need to compile the dictionary into a training feature vector to input so that the neural network can feed on that. \n",
    "Let's begin by initializing all the parameters for the set. In order to do that, we need to decide out neural network sample architecture and their dimensions.  \n",
    "\n",
    "1. Feature Vector X: There are 7 features and 500 samples (m). So out feature vector X should be (6, 500) dimensional array.  \n",
    "2. Number of hidden layers: Our architecture will have 1 hidden layer. The input layer will have 3 neurons, hidden layer will have 4 and output layer will have only 1.    \n",
    "3. So we will have:  \n",
    "  * W[1] = (3,7) ; X = (7, m) ; b[1] = (3,m)  \n",
    "  * W[2] = (4,3) ; A[1] = (3, m) ; b[2] = (4,m)  \n",
    "  * W[3] = (1,4) ; A[2] = (4, m) ; b[3] = (1, m)  \n",
    "4. Finally, the activations will be LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX   \n",
    "\n",
    "**This is how it looks at the end**\n",
    "![Title](../input/titanicnet/titanicnet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a04561",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels, test_features, test_labels, m, t = load_dataset()\n",
    "train_features = np.array(\n",
    "    [train_features[\"pclass\"],\n",
    "    train_features[\"age\"],\n",
    "    train_features[\"sex\"],\n",
    "    train_features[\"sibsp\"],\n",
    "    train_features[\"parch\"],\n",
    "    train_features[\"fare\"],\n",
    "    train_features[\"embarked\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_features = np.array(\n",
    "    [test_features[\"pclass\"],\n",
    "    test_features[\"age\"],\n",
    "    test_features[\"sex\"],\n",
    "    test_features[\"sibsp\"],\n",
    "    test_features[\"parch\"],\n",
    "    test_features[\"fare\"],\n",
    "    test_features[\"embarked\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "assert train_features.shape == (7,m)\n",
    "#assert train_labels.shape == (1,m)\n",
    "assert test_features.shape == (7, t - m)\n",
    "#assert test_labels.shape == (1, t - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(labels, C):\n",
    "    C = tf.constant(C, dtype = tf.int32)\n",
    "    one_hot_matrix = tf.one_hot(labels, depth = C, axis = 0)\n",
    "    session = tf.Session()\n",
    "    one_hot_mat = session.run(one_hot_matrix)\n",
    "    session.close()\n",
    "    return one_hot_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb28304",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = one_hot_encoder(train_labels, 2)\n",
    "test_labels = one_hot_encoder(test_labels, 2)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7869739c",
   "metadata": {},
   "source": [
    "## Initializing the variables\n",
    "\n",
    "I have chosen **Xavier Initialization** for the weights and zero initialisation of the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(dtype = tf.float32, shape = [n_x, None])\n",
    "    Y = tf.placeholder(dtype = tf.float32, shape = [n_y, None])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b40c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1)\n",
    "def initialize_network(scalea, scaleb):\n",
    "    #W1 = tf.get_variable(\"W1\", [8,7], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    W1 = tf.get_variable(\"W1\", [8,7], initializer = tf.contrib.layers.xavier_initializer(seed = 4), regularizer=tf.contrib.layers.l2_regularizer(scale=scalea))\n",
    "    b1 = tf.get_variable(\"b1\", [8,1], initializer = tf.zeros_initializer())\n",
    "    #W2 = tf.get_variable(\"W2\", [12,8], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [12,8], initializer = tf.contrib.layers.xavier_initializer(seed = 4), regularizer=tf.contrib.layers.l2_regularizer(scale=scaleb))\n",
    "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [2,12], initializer = tf.contrib.layers.xavier_initializer(seed = 4))\n",
    "    b3 = tf.get_variable(\"b3\", [2,1], initializer = tf.zeros_initializer())\n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2,\n",
    "        \"W3\": W3,\n",
    "        \"b3\": b3\n",
    "    }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49c780",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Now, the actual neural network does forward propagation on our architecture  \n",
    "LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1,X), b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)\n",
    "    \n",
    "    return Z3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc33db9",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Putting everything together into one model. The model serves two purposes:  \n",
    "1. Train the network on the training data.  \n",
    "2. Generate final parameters.  \n",
    "3. Create a metric system to analyze how well we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec579f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, scalea = 0.5, scaleb = 0.5, learning_rate = 0.005, num_iterations = 10000, print_cost = True):\n",
    "    tf.reset_default_graph()\n",
    "    X, Y = create_placeholders(7,2)\n",
    "    parameters = initialize_network(scalea, scaleb)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    costs = []\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        #writer = tf.summary.FileWriter(\"./my_graph\", graph = sess.graph)\n",
    "        each_cost = 0\n",
    "        for i in range(num_iterations):\n",
    "            _, each_cost = sess.run([optimizer, cost], feed_dict = {X: X_train, Y:Y_train})\n",
    "            if print_cost and i%5000 == 0:\n",
    "                print(\"Cost at %i is %f\"%(i, each_cost))\n",
    "            if i % 10 == 0:\n",
    "                costs.append(each_cost)\n",
    "        \n",
    "        #Metrics business\n",
    "        output = tf.argmax(Z3)\n",
    "        labels = tf.argmax(Y)\n",
    "        correct_prediction = tf.equal(output, labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "       \n",
    "        #training and testing accuracy\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y:Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        \n",
    "        #precision and recall\n",
    "        precisions = tf.metrics.precision(labels, output)\n",
    "        recalls = tf.metrics.recall(labels, output)\n",
    "        \n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        precisions = sess.run(precisions, feed_dict={X: X_test, Y: Y_test})[1]\n",
    "        recalls = sess.run(recalls, feed_dict={X: X_test, Y: Y_test})[1]\n",
    "        f1_score = 2 * precisions * recalls / (precisions + recalls)\n",
    "        \n",
    "        metrics = {\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"precision\": precisions,\n",
    "            \"recall\": recalls,\n",
    "            \"f1_score\": f1_score\n",
    "        }\n",
    "        return metrics, costs, sess.run(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, costs, parameters = model(train_features, train_labels, test_features, test_labels, scalea = 0.26, scaleb = 0.73, num_iterations=33000, learning_rate = 0.0005, print_cost = False)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.ylim(0.35, 0.8)\n",
    "plt.title(\"learning_rate = 0.0005\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99d6d1",
   "metadata": {},
   "source": [
    "## How well we did?\n",
    "\n",
    "There are various parameters on which we can see our performance.  \n",
    "**Train Accuracy** = **83.80%**  \n",
    "**Dev Accuracy** = **82.99%**  \n",
    "\n",
    "Also, the confusion matrix of our solution:  \n",
    "**Precision** = **81.53%**  \n",
    "**Recall** = **71.62%**  \n",
    "\n",
    "Finally, Our F1 score (A single score metric):  \n",
    "**F1 Score** = **76.25%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
