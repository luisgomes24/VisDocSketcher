{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d7cd70",
   "metadata": {},
   "source": [
    "👋 In this kernel I will go through one possible approach, a histogram based method, to estimate breast densities. The method has been presented in Wu et al. (2018) \\[1\\] (see also Karssemeijer 1998 \\[2\\]). Wu et al. implement the histogram as a baseline method to compare agaist their convolutional neural network-based method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737e43b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> The Breast Imaging Reporting and Data System (BI-RADS) is used to classify breast density into four categories: <b>\"A\"</b>: almost entirely fatty breast tissue, <b>\"B\"</b>: areas of dense glandular tissue and fibrous connective tissue, <b>\"C\"</b>: heterogeneously dense breast tissue with many areas of glandular tissue and fibrous connective tissue, <b>\"D\"</b>: extremely dense breast tissue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba044610",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Data source:</b> Radiological Society of North America. (2022, Nov 29). RSNA Screening Mammography Breast Cancer Detection, Version 1. Retrieved 2023 Feb 9 from [https://www.kaggle.com/competitions/rsna-breast-cancer-detection/data].\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5edfd71",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92733f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pydicom\n",
    "from pydicom.errors import InvalidDicomError\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba0a09",
   "metadata": {},
   "source": [
    "# Clone `breast_density_classifier` repository\n",
    "\n",
    "The `breast_density_classifier` is published under BSD-2-Clause license which is compatible with the Apache 2.0 open source license under which this notebook has been made public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('/kaggle/working/breast_density_classifier'):\n",
    "    !git clone https://github.com/nyukat/breast_density_classifier.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79fe145",
   "metadata": {},
   "source": [
    "## Add path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76cd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_classifier_loc = '/kaggle/working/breast_density_classifier'\n",
    "sys.path.append(density_classifier_loc)  # According to https://www.kaggle.com/general/135988#1985330"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde7319b",
   "metadata": {},
   "source": [
    "## Add `__init__.py`\n",
    "\n",
    "Let's include `__init__.py` to access the files as modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = density_classifier_loc + '__init__.py'\n",
    "if not os.path.isfile(save_path):\n",
    "    open(save_path, 'a').close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323969",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "\n",
    "In addition to the commands below, there is also a wheel (https://www.kaggle.com/code/vslaykovsky/rsna-2022-whl) which can be used in an offline setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3af3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-gdcm -q\n",
    "!pip install pylibjpeg -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5575dff",
   "metadata": {},
   "source": [
    "# Read metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = pd.read_csv(\"/kaggle/input/rsna-breast-cancer-detection/train.csv\")\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28eafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.loc[train_meta['density'] == 'C'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b6ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = '10038'  # folder name, corresponds to patient_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708a5e6",
   "metadata": {},
   "source": [
    "# Read DICOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcm_loc = pathlib.Path('/kaggle/input/rsna-breast-cancer-detection/train_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = list(dcm_loc.glob(f'{patient_id}/*.dcm'))  # manually chosen patient_id from the table above\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2be724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/code/anttiisosalo/solt-based-image-augs-rsna-bc-detection\n",
    "\n",
    "image_dict = {}\n",
    "for i, file in enumerate(filelist):\n",
    "    try:\n",
    "        ds = pydicom.dcmread(str(file))\n",
    "    except InvalidDicomError:\n",
    "        print(f'Invalid DICOM file: {str(file)}')\n",
    "        continue\n",
    "    \n",
    "    # print(ds)\n",
    "    \n",
    "    if hasattr(ds, 'SOPClassUID') and not ds.SOPClassUID == '1.2.840.10008.5.1.4.1.1.1.2':\n",
    "        print(f'File has unknown SOPClassUID: {ds.SOPClassUID}')\n",
    "        continue\n",
    "    \n",
    "    if hasattr(ds, 'PresentationIntentType') and not ds.PresentationIntentType == 'FOR PROCESSING':\n",
    "        print(f'Invalid Presentation Intent Type: {ds.PresentationIntentType}')\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        arr = ds.pixel_array.copy()  # get pixel array\n",
    "    except AttributeError:\n",
    "        print(f'AttributeError: Unable to return pixel array.')\n",
    "        continue\n",
    "    except ValueError:\n",
    "        print(f'ValueError: Unable to return pixel array.')\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        out = apply_voi_lut(arr, ds)  # apply VOI LUT (default LUT) or windowing operation\n",
    "    except AttributeError:\n",
    "        print(f'AttributeError: Unable to apply VOI LUT or windowing.')\n",
    "        continue\n",
    "    \n",
    "    if hasattr(ds, 'PhotometricInterpretation') and not (ds.PhotometricInterpretation == 'MONOCHROME1' or ds.PhotometricInterpretation == 'MONOCHROME2'):\n",
    "        print(f'Unknown photometric interpretation: {ds.PhotometricInterpretation}')\n",
    "        continue\n",
    "    elif not hasattr(ds, 'PhotometricInterpretation'):\n",
    "        print(f'Missing photometric interpretation.')\n",
    "        continue\n",
    "\n",
    "    if ds.PhotometricInterpretation == 'MONOCHROME1':  # ranges from bright to dark with ascending pixel values\n",
    "        out = out.max() - out\n",
    "    elif ds.PhotometricInterpretation == 'MONOCHROME2':  # ranges from dark to bright with ascending pixel values\n",
    "        pass\n",
    "\n",
    "    height = ds.Rows\n",
    "    width = ds.Columns\n",
    "    \n",
    "    out = out.reshape((height, width))  # (height, width)\n",
    "\n",
    "    if hasattr(ds, 'ImageLaterality'):\n",
    "        laterality = ds.ImageLaterality\n",
    "    else:\n",
    "        print(f'Unknown or invalid Laterality.')\n",
    "        continue\n",
    "    \n",
    "    patient = ds.PatientID\n",
    "    \n",
    "    try:\n",
    "        view = train_meta.loc[( train_meta['patient_id'] == int(patient) ) & ( train_meta['image_id'] == int(file.stem) ), 'view'].item()\n",
    "    except ValueError:\n",
    "        print(f'ValueError: Unable to return View.')\n",
    "        continue\n",
    "\n",
    "    bitdepth = ds.BitsAllocated\n",
    "    # bitsstored = ds.BitsStored\n",
    "\n",
    "    out = out.astype(np.float64)\n",
    "    out /= out.max()\n",
    "    out *= pow(2, bitdepth) - 1\n",
    "    out = out.astype(np.uint16)\n",
    "\n",
    "    if laterality == 'R' and view == 'MLO':\n",
    "        image_dict['R-MLO'] = out\n",
    "    elif laterality == 'L' and view == 'MLO':\n",
    "        image_dict['L-MLO'] = out\n",
    "    elif laterality == 'R' and view == 'CC':\n",
    "        image_dict['R-CC'] = out\n",
    "    elif laterality == 'L' and view == 'CC':\n",
    "        image_dict['L-CC'] = out\n",
    "    \n",
    "    # study = ds.StudyInstanceUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62042fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0914dfe",
   "metadata": {},
   "source": [
    "## Flip 'R' side images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_l_cc = image_dict['L-CC']\n",
    "view_r_cc = np.fliplr(image_dict['R-CC'])\n",
    "view_l_mlo = image_dict['L-MLO']\n",
    "view_r_mlo = np.fliplr(image_dict['R-MLO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7289f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for standard_view in [view_r_mlo, view_l_mlo, view_r_cc, view_l_cc]:\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.imshow(standard_view, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    unique_colors = len(np.unique(standard_view))\n",
    "    mi = np.min(standard_view)\n",
    "    ma = np.max(standard_view)\n",
    "    print(f'Unique colors: {unique_colors}; Min value: {mi}; Max value: {ma}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10edea",
   "metadata": {},
   "source": [
    "# Crop images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71273ca",
   "metadata": {},
   "source": [
    "Here we make use [´breast_segment´](https://github.com/olieidel/breast_segment/) script which is published under MIT license which is compatible with the Apache 2.0 open source license under which this notebook has been made public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0078cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\"\"\"\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2016 Oliver Eidel\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.exposure import equalize_hist\n",
    "from skimage.filters.rank import median\n",
    "from skimage.measure import regionprops\n",
    "from skimage.morphology import disk\n",
    "from skimage.segmentation import felzenszwalb\n",
    "from skimage.transform import rescale, resize\n",
    "\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_mask(im, scale_factor=0.25, threshold=3900, felzenzwalb_scale=0.15, flipped=True):\n",
    "    im_thres = im.copy()\n",
    "    im_thres[im_thres > threshold] = 0\n",
    "    \n",
    "    im_small = rescale(im_thres, scale_factor)\n",
    "    im_small_filt = median(im_small, disk(50))\n",
    "    \n",
    "    im_small_filt = equalize_hist(im_small_filt)\n",
    "    \n",
    "    segments = felzenszwalb(im_small_filt, scale=felzenzwalb_scale)\n",
    "    segments += 1  # to prevent labels() from ignoring segment with 0's\n",
    "    \n",
    "    props = regionprops(segments)\n",
    "    props_sorted = sorted(props, key=lambda x: x.area, reverse=True)\n",
    "    \n",
    "    bg_index = 0\n",
    "    \n",
    "    bg_region = props_sorted[bg_index]\n",
    "    minr, minc, maxr, maxc = bg_region.bbox\n",
    "    filled_mask = bg_region.filled_image\n",
    "    \n",
    "    im_small_fill = np.zeros((im_small_filt.shape[0]+2, im_small_filt.shape[1]+1), dtype=int)\n",
    "    \n",
    "    im_small_fill[minr+1:maxr+1, minc:maxc] = filled_mask\n",
    "    im_small_fill[0, :] = 1  # top\n",
    "    im_small_fill[-1, :] = 1  # bottom\n",
    "    im_small_fill[:, -1] = 1  # right\n",
    "    \n",
    "    im_small_fill = binary_fill_holes(im_small_fill)\n",
    "    im_small_mask = im_small_fill[1:-1, :-1] if flipped == True else im_small_fill[1:-1, 1:]\n",
    "    \n",
    "    shape = (im.shape[1],im.shape[0])\n",
    "    im_mask = np.array(Image.fromarray(im_small_mask).resize(shape)).astype(bool)\n",
    "    \n",
    "    im_mask = ~im_mask  # invert mask\n",
    "    \n",
    "    # determine side of breast in mask and compare\n",
    "    col_sums_split = np.array_split(np.sum(im_mask, axis=0), 2)\n",
    "    left_col_sum = np.sum(col_sums_split[0])\n",
    "    right_col_sum = np.sum(col_sums_split[1])\n",
    "\n",
    "    if left_col_sum > right_col_sum:\n",
    "        breast_side_mask = True\n",
    "    else:\n",
    "        breast_side_mask = False\n",
    "\n",
    "    if breast_side_mask != flipped:  # breast mask is not on the expected side\n",
    "        im_mask = ~im_mask\n",
    "    \n",
    "    if im_mask.ravel().sum() == 0:  # if no region is found, abort and return mask the same size as the input image\n",
    "        res = np.ones_like(im).astype(bool)\n",
    "    else:\n",
    "        minr = np.argwhere(im_mask.any(axis=1)).ravel()[0]\n",
    "        maxr = np.argwhere(im_mask.any(axis=1)).ravel()[-1]\n",
    "        minc = np.argwhere(im_mask.any(axis=0)).ravel()[0]\n",
    "        maxc = np.argwhere(im_mask.any(axis=0)).ravel()[-1]\n",
    "        res = im[minr:maxr, minc:maxc].copy()\n",
    "    \n",
    "    del im\n",
    "    del im_thres\n",
    "    del im_mask\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff418943",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_view_l_cc = get_crop_mask(view_l_cc, scale_factor=0.25, threshold=30000, felzenzwalb_scale=0.15, flipped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_view_r_cc = get_crop_mask(view_r_cc, scale_factor=0.25, threshold=30000, felzenzwalb_scale=0.15, flipped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_view_l_mlo = get_crop_mask(view_l_mlo, scale_factor=0.25, threshold=30000, felzenzwalb_scale=0.15, flipped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bece10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_view_r_mlo = get_crop_mask(view_r_mlo, scale_factor=0.25, threshold=30000, felzenzwalb_scale=0.15, flipped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shape = tuple(map(max, zip(*[cropped_view_l_cc.shape[:2], cropped_view_r_cc.shape[:2], cropped_view_l_mlo.shape[:2], cropped_view_r_mlo.shape[:2]])))\n",
    "new_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_view_l_cc = np.pad(cropped_view_l_cc.copy(), ((0, new_shape[0] - cropped_view_l_cc.shape[0]), (0, new_shape[1] - cropped_view_l_cc.shape[1])), constant_values=0)\n",
    "padded_view_r_cc = np.pad(cropped_view_r_cc.copy(), ((0, new_shape[0] - cropped_view_r_cc.shape[0]), (0, new_shape[1] - cropped_view_r_cc.shape[1])), constant_values=0)\n",
    "padded_view_l_mlo = np.pad(cropped_view_l_mlo.copy(), ((0, new_shape[0] - cropped_view_l_mlo.shape[0]), (0, new_shape[1] - cropped_view_l_mlo.shape[1])), constant_values=0)\n",
    "padded_view_r_mlo = np.pad(cropped_view_r_mlo.copy(), ((0, new_shape[0] - cropped_view_r_mlo.shape[0]), (0, new_shape[1] - cropped_view_r_mlo.shape[1])), constant_values=0)\n",
    "padded_view_l_cc.shape, padded_view_r_cc.shape, padded_view_l_mlo.shape, padded_view_r_mlo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f64c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for standard_view in [padded_view_r_mlo, padded_view_l_mlo, padded_view_r_cc, padded_view_l_cc]:\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.imshow(standard_view, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    unique_colors = len(np.unique(standard_view))\n",
    "    mi = np.min(standard_view)\n",
    "    ma = np.max(standard_view)\n",
    "    print(f'Unique colors: {unique_colors}; Min value: {mi}; Max value: {ma}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e7656",
   "metadata": {},
   "source": [
    "# Density estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\"\"\"\n",
    "BSD 2-Clause License\n",
    "\n",
    "Copyright (c) 2018, Nan Wu, Krzysztof J. Geras, Yiqiu Shen, Jingyi Su, \n",
    "S. Gene Kim, Eric Kim, Stacey Wolfson, Linda Moy, Kyunghyun Cho\n",
    "All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "* Redistributions of source code must retain the above copyright notice, this\n",
    "  list of conditions and the following disclaimer.\n",
    "\n",
    "* Redistributions in binary form must reproduce the above copyright notice,\n",
    "  this list of conditions and the following disclaimer in the documentation\n",
    "  and/or other materials provided with the distribution.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c93366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'bins_histogram': 50}\n",
    "model = models_torch.BaselineHistogramModel(num_bins=parameters['bins_histogram']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'breast_density_classifier/saved_models/BreastDensity_BaselineHistogramModel/model.p'\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046f36b",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a04ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(utils.histogram_features_generator([padded_view_l_cc, padded_view_r_cc, padded_view_l_mlo, padded_view_r_mlo], parameters)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca77e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predicted_density = model(x).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'A: {str(predicted_density[0, 0])}; ' \\\n",
    "      f'B: {str(predicted_density[0, 1])}; ' \\\n",
    "      f'C: {str(predicted_density[0, 2])}; ' \\\n",
    "      f'D: {str(predicted_density[0, 3])}; ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef2718",
   "metadata": {},
   "source": [
    "# Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0229df",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.argmax([predicted_density[0, 0], predicted_density[0, 1], predicted_density[0, 2], predicted_density[0, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0847b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_classes = list('ABCD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182fd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(density_classes)[res]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd07562",
   "metadata": {},
   "source": [
    "The histogram based result is highly dependent on the pre-processing, **cropping** and **number of bins**. The reference class for this patient is **'C'**. It should be noted, that density assesment is somewaht subjective and can also include observer variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0727c9",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The predicted densities, when they are tuned to match the reference densities, can be used, for example, to partition the competition data according to the breast tissue density. 👍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc08a82",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\\[1\\] Nan Wu, Krzysztof J. Geras, Yiqiu Shen, Jingyi Su, S. Gene Kim, Eric Kim, Stacey Wolfson, Linda Moy & Kyunghyun Cho, \"Breast density classification with deep convolutional neural networks,\" In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6682--6686, 2018. URL: https://github.com/nyukat/breast_density_classifier/\n",
    "\n",
    "\\[2\\] Nico Karssemeijer, “Automated classification of parenchymal patterns in mammograms,” Physics in Medicine & Biology, vol. 43, no. 2, pp. 365--378, 1998. DOI: 10.1088/0031-9155/43/2/011."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
