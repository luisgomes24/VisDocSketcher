{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dbb3af9",
   "metadata": {},
   "source": [
    "# Playground Season 3 Episode 9\n",
    "## #35 Solution\n",
    "\n",
    "This notebook will outline the relevent parts of the solution for Season 3 Episode 9's challenge of predicting the strength of concreate. \n",
    "\n",
    "\n",
    "In this notebook we will not go over too much on the Exploritory Data Analysis, but more on the technical side of finding the solution using Feature Engineering and machine learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4defa1a",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,  StratifiedKFold, KFold,GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets, ensemble\n",
    "import catboost as cat\n",
    "import category_encoders as ce\n",
    "import optuna\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff94f24",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/playground-series-s3e9/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/playground-series-s3e9/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b4682",
   "metadata": {},
   "source": [
    "#### Dropping ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8344278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Id column from both train and test data\n",
    "train_df = train_df.drop('id', axis=1)\n",
    "test_df = test_df.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad7867",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13758371",
   "metadata": {},
   "source": [
    "**Time** - *lnAgeInDays* - After testing, the best method of calculating Time was a logged version to help \"reign in\" the outliers.\n",
    "\n",
    "**Chemistry** - *CementToWaterRatio* - I remember my dad mixing concrete, water was important to the factor... Also after testing a bunch of variables, this one helped the variance in RMSE. \n",
    "\n",
    "**Charun Umesh** utilized *FlyAshComponent* as a dummy which worked well for them. They noticed \"FlyAshComponent feature has more than 73% of 0 values. it will be good to categorize that feature to 0 and 1s\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f70617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "train_df['lnAgeInDays'] = np.log(train_df['AgeInDays']) \n",
    "train_df['CementToWaterRatio'] = train_df['CementComponent']/train_df['WaterComponent']\n",
    "train_df['FlyAshComponent'] = np.where(train_df['FlyAshComponent'] == 0.0, 0, 1).astype('int64')\n",
    "\n",
    "#test\n",
    "test_df['lnAgeInDays'] = np.log(test_df['AgeInDays'])\n",
    "test_df['CementToWaterRatio'] = test_df['CementComponent']/test_df['WaterComponent']\n",
    "test_df['FlyAshComponent'] = np.where(test_df['FlyAshComponent'] == 0.0, 0, 1).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24adf700",
   "metadata": {},
   "source": [
    "#### Capping Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d85a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_capping(df, cols, factor):\n",
    "    for col in cols:\n",
    "        print(f\"column name is : {col}\")\n",
    "        df_outliers = df.copy()\n",
    "        df_outliers[col] = sorted(df_outliers[col])\n",
    "        Q1 = df_outliers[col].quantile(0.25)\n",
    "        Q3 = df_outliers[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - (factor*IQR)\n",
    "        upper_bound = Q3 + (factor*IQR)\n",
    "        print(f\"lower_bound is : {lower_bound}\")\n",
    "        print(f\"upper_bound is : {upper_bound}\")\n",
    "        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])  \n",
    "    return df\n",
    "\n",
    "def outliers(data):   \n",
    "    col_attributes = data.select_dtypes([np.int64, np.float64]).columns.values.tolist()\n",
    "    data_cap = data.copy()\n",
    "    new_data = iqr_capping(data_cap, col_attributes, 1.5)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7fe875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = outliers(train_df)\n",
    "test_df = outliers(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22a6ec",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac907f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['Strength'], axis=1)\n",
    "y = train_df['Strength']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30aa63",
   "metadata": {},
   "source": [
    "#### Define Hyperparameters for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd66e90",
   "metadata": {},
   "source": [
    "We will be using:\n",
    "* XBG\n",
    "* LGBM\n",
    "* CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad478a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB (Extreme Gradient Boost)\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\n",
    "    \n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01,1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
    "        'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0)\n",
    "    }\n",
    "    optuna_model = XGBRegressor(**params)\n",
    "    optuna_model.fit(x_train, y_train,  verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    val_preds = optuna_model.predict(x_test)\n",
    "    # Evaluate predictions\n",
    "    root_mean_squared_error = mean_squared_error(y_test, val_preds,squared = False)\n",
    "\n",
    "    return root_mean_squared_error\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study_xgb.optimize(objective_xgb, n_trials=50)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "#LGBM (Light Gradient Boost Machine)\n",
    "\n",
    "def objective_lgbm(trial):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\n",
    "    \n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01,1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0)\n",
    "    }\n",
    "    optuna_model = LGBMRegressor(**params)\n",
    "    optuna_model.fit(x_train, y_train,  verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    val_preds = optuna_model.predict(x_test)\n",
    "    # Evaluate predictions\n",
    "    root_mean_squared_error = mean_squared_error(y_test, val_preds,squared = False)\n",
    "\n",
    "    return root_mean_squared_error\n",
    "\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80140baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'learning_rate': 0.0225,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 3, \n",
    "    'reg_alpha': 0.8435, \n",
    "    'reg_lambda': 0.823545, \n",
    "    'n_estimators': 1000\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "     'learning_rate': 0.05,\n",
    "     'max_depth': 10,\n",
    "     'subsample_for_bin': 20000,\n",
    "     'subsample': 0.8,\n",
    "     'colsample_bytree': 0.8,\n",
    "     'objective': 'regression',\n",
    " }\n",
    "    \n",
    "cbr_params = {'iterations': 2000, \n",
    "              'max_depth': 7, #4, #10,\n",
    "              'learning_rate': 0.0036012104807528686, #0.03725416892898261, #0.01,\n",
    "              'verbose': 100,\n",
    "              'subsample': 0.6215023014443006,\n",
    "              'l2_leaf_reg': 0.7998684766493955}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469c99c",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "pred_xgb = np.zeros(test_df.shape[0])\n",
    "pred_cat = np.zeros(test_df.shape[0])\n",
    "pred_lgb = np.zeros(test_df.shape[0])\n",
    "rmse_xgb=[]  \n",
    "rmse_cat=[]\n",
    "rmse_lgb=[]\n",
    "rmse = []\n",
    "n=0\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kfold.split(X, y)):\n",
    "    x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    x_valid, y_valid = X.loc[valid_index], y.iloc[valid_index]\n",
    "    \n",
    "    # XGBoost regressor\n",
    "    model_xgb = XGBRegressor(**study_xgb.best_params)\n",
    "    model_xgb.fit(x_train,y_train,eval_set=[(x_valid,y_valid)],early_stopping_rounds=100,verbose=False) \n",
    "    pred_xgb+=model_xgb.predict(test_df)/kfold.n_splits\n",
    "    rmse_xgb.append(mean_squared_error(y_valid, model_xgb.predict(x_valid), squared=False))\n",
    "    \n",
    "    # CatBoost regressor\n",
    "    model_cat = cat.CatBoostRegressor(**cbr_params)\n",
    "    model_cat.fit(x_train,y_train,eval_set=[(x_valid,y_valid)],early_stopping_rounds=100,verbose=False) \n",
    "    pred_cat+=model_cat.predict(test_df)/kfold.n_splits\n",
    "    rmse_cat.append(mean_squared_error(y_valid, model_cat.predict(x_valid), squared=False))\n",
    "    \n",
    "    \n",
    "    # LightGBM regressor\n",
    "    model_lgb = LGBMRegressor(**study_lgbm.best_params)\n",
    "    model_lgb.fit(x_train,y_train,eval_set=[(x_valid,y_valid)],early_stopping_rounds=100,verbose=False) \n",
    "    pred_lgb+=model_lgb.predict(test_df)/kfold.n_splits\n",
    "    rmse_lgb.append(mean_squared_error(y_valid, model_lgb.predict(x_valid), squared=False))\n",
    "    \n",
    "    rmse_val = (rmse_xgb[n]+ rmse_cat[n]+ rmse_lgb[n]) / 3    \n",
    "    rmse.append(rmse_val)\n",
    "    print(f\"fold: {n+1} rmse xgb: {rmse_xgb[n]} | rmse cat: {rmse_cat[n]} | rmse LGBM: {rmse_lgb[n]} final rmse is: {rmse[n]}\")\n",
    "    n+=1\n",
    "\n",
    "print(np.mean(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601665e",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bdbc4d",
   "metadata": {},
   "source": [
    "When evaluating, the targeted RSME. XGB consistantly had a higher variance than the other two models combined. So, it will be removed from the final answer. \n",
    "\n",
    "When testing, CatBoost out performed LGBM most times, and a \"mean\" of the model was not the best option in this case. Instead, opting for a 40/60 split leaning more towards CatBoost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486013be",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/kaggle/input/playground-series-s3e9/sample_submission.csv')\n",
    "submission['XGB'] = pred_xgb\n",
    "submission['CAT'] = pred_cat\n",
    "submission['LGB'] = pred_lgb\n",
    "submission['Strength'] = (submission['LGB'] * 0.4 + submission['CAT'] * 0.6 )\n",
    "\n",
    "final_submission = pd.DataFrame(submission, columns=['id', 'Strength'])\n",
    "final_submission"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
