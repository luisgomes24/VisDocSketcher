{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39235ba",
   "metadata": {},
   "source": [
    "# Web Scarpping Amazon for any product using user's input !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d0ae4",
   "metadata": {},
   "source": [
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/featured_image-6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371682f",
   "metadata": {},
   "source": [
    "# What is Web Scrapping?\n",
    "-  Web scraping is the process of using bots to extract content and data from a website. \n",
    "-  Unlike screen scraping, which only copies pixels displayed onscreen, web scraping extracts underlying HTML code and, with it, data stored in a database.\n",
    "-  Web scraping is used to collect large information from websites. \n",
    "\n",
    "# What Packages are Top packages used in Web Scrapping?\n",
    "-  Requests\n",
    "-  Beautiful Soup \n",
    "-  lxml\n",
    "-  Selenium\n",
    "-  Scrapy\n",
    "\n",
    "# Steps for Web Scrapping:\n",
    "-  Step 1: Find the URL that you want to scrape\n",
    "-  Step 2: Inspecting the Page\n",
    "-  Step 3: Find the data you want to extract\n",
    "-  Step 4: Write the code\n",
    "-  Step 5: Run the code and extract the data\n",
    "-  Step 6: Store the data in the required format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment: Install Following Packages for Web Scrapping:\n",
    "\n",
    "\n",
    "#Code:\n",
    "# pip install selenium\n",
    "# pip install webdriver_manager\n",
    "# pip install webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment: loading all the required library for webscrapping and saving the data into csv file.\n",
    "\n",
    "#Code:\n",
    "# import csv\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# def get_url(search_term):\n",
    "\n",
    "#     Comment:  Genrate URL for search term\n",
    "\n",
    "#     Code:\n",
    "#     template='https://www.amazon.com/s?k={}&ref=nb_sb_noss_2'\n",
    "#     search_term=search_term.replace(\" \",\"+\")\n",
    "    \n",
    "    \n",
    "#     Comment: add term query to url\n",
    "\n",
    "#     Code:\n",
    "#     url=template.format(search_term)\n",
    "    \n",
    "#     Comment: Add page query placeholder\n",
    "\n",
    "#     Code:\n",
    "#     url+='&page{}'  \n",
    "    \n",
    "#     return template.format(search_term)\n",
    "\n",
    "# def extract_record(item):\n",
    "#     Comment: First we will take the decrpition\n",
    "#     Code:\n",
    "#     atag=item.h2.a\n",
    "#     descrpition=atag.text.strip()\n",
    "#     new_url='https://www.amazon.com'+atag.get('href')\n",
    "    \n",
    "#     Comment:  Price\n",
    "#     Code:\n",
    "#     try:\n",
    "#         price_parent=item.find('span','a-price')\n",
    "#         price=price_parent.find('span','a-offscreen').text\n",
    "#     except AttributeError:\n",
    "#         return\n",
    "#     Comment: Ranking and rating of the user \n",
    "#     Code:\n",
    "#     try:\n",
    "#         rating=item.i.text\n",
    "#         review_count=item.find('span',{'class':'a-size-base'}).text\n",
    "#     except AttributeError:\n",
    "#         rating=\"\"\n",
    "#         review_count=\"\"\n",
    "        \n",
    "#     result=(descrpition,price,rating,review_count,new_url)\n",
    "    \n",
    "#     return result\n",
    "\n",
    "\n",
    "# def main():\n",
    "    \n",
    "#     Comment: this is a main function where we first load web driver\n",
    "#     Code:\n",
    "#     from webdriver_manager.chrome import ChromeDriverManager\n",
    "#     driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    \n",
    "#     Comment: Take Input parameter\n",
    "#     Code:\n",
    "#     search_term=input('Enter the Keyword:')\n",
    "#     record=[]\n",
    "    \n",
    "#     Comment: create url to search\n",
    "#     Code:\n",
    "#     url=get_url(search_term)\n",
    "    \n",
    "#     for page in range(1,21):\n",
    "#         driver.get(url.format(page))\n",
    "#         soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "#         results=soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "        \n",
    "#         for item in results:\n",
    "#             u=extract_record(item)\n",
    "#             if u:\n",
    "#                 record.append(u)\n",
    "    \n",
    "#     driver.close()\n",
    "    \n",
    "#     df = pd.DataFrame(record, columns=['Descrpition','Price','Rating','ReviewCount','URL'])\n",
    "#     df.to_csv('FinalProductList.csv')\n",
    "     \n",
    "#     Commnet: Or we can do it in another way\n",
    "#     Code:\n",
    "#     with open(\"Productlists.csv\",'w',newline=\"\",encoding='utf-8') as f:\n",
    "#         writer=csv.writer(f)\n",
    "#         writer.writerow(['Descrpition','Price','Rating','ReviewCount','URL'])\n",
    "#         writer.writerow(record)\n",
    "#         print('Done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment:Run this function to run whole program\n",
    "# Code:\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850d591",
   "metadata": {},
   "source": [
    "# ðŸ“Œ  **Note**: \n",
    "## I have commented all the since it wont run on Kaggle but  it can run on Jupyter Notebook."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
