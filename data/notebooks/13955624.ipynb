{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda5beb4",
   "metadata": {},
   "source": [
    "# Imports\n",
    "The following packages are imported:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c635cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import IPython\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageColor\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageOps\n",
    "\n",
    "\n",
    "import tempfile\n",
    "from six.moves.urllib.request import urlopen\n",
    "from six import BytesIO\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# For measuring the inference time.\n",
    "import time\n",
    "\n",
    "# Check available GPU devices.\n",
    "print(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad37fc",
   "metadata": {},
   "source": [
    "# Data\n",
    "- data_dir is the path to the images and the `results.csv`\n",
    "- image_dir is the path exculsively to the images\n",
    "- csv_file is the path to the `results.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input/flickr-image-dataset/flickr30k_images'\n",
    "image_dir = f'{data_dir}/flickr30k_images'\n",
    "csv_file = f'{data_dir}/results.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699e41c",
   "metadata": {},
   "source": [
    "Here we read the csv file as a dataframe and make some observations from it.\n",
    "For a quick EDA we are going to \n",
    "- check the shape of the dataframe\n",
    "- check the names of the columns\n",
    "- find out the unique image names there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef11b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file, delimiter='|')\n",
    "\n",
    "print(f'[INFO] The shape of dataframe: {df.shape}')\n",
    "print(f'[INFO] The columns in the dataframe: {df.columns}')\n",
    "print(f'[INFO] Unique image names: {len(pd.unique(df[\"image_name\"]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9e55e",
   "metadata": {},
   "source": [
    "A quick observation here is to see that the dataframe has `158915` elements but only `31783` image names. This means that there is a duplicacy involved. On further inspection we will see that each image has 5 unique captions attached to it ($31783\\times 5=158915$)\n",
    "\n",
    "While looking into the dataframe I found out that `19999` had some messed up entries. This has led me to manually change the entries in that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81086e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['image_name', 'comment_number', 'comment']\n",
    "del df['comment_number']\n",
    "\n",
    "# Under scrutiny I had found that 19999 had a messed up entry\n",
    "df['comment'][19999] = ' A dog runs across the grass .'\n",
    "\n",
    "# Image names now correspond to the absolute position\n",
    "df['image_name'] = image_dir+'/'+df['image_name']\n",
    "\n",
    "# <start> comment <end>\n",
    "df['comment'] = \"<start> \"+df['comment']+\" <end>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = len(df)\n",
    "\n",
    "train_size = int(0.7* SIZE) \n",
    "val_size = int(0.1* SIZE)\n",
    "test_size = int(0.2* SIZE)\n",
    "\n",
    "train_size, val_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter different indices.\n",
    "index = 10000\n",
    "\n",
    "image_name = df['image_name'][index]\n",
    "comment = df['comment'][index]\n",
    "\n",
    "print(comment)\n",
    "\n",
    "IPython.display.Image(filename=image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb902f",
   "metadata": {},
   "source": [
    "# Text Handling\n",
    "- Defined the size of the vocab which is `5000`.\n",
    "- Initialized the Tokenizer class.\n",
    "    - Standardized (all to lower case)\n",
    "    - Filters the punctuations\n",
    "    - Splits the text\n",
    "    - Creates the vocabulary (`<start>, <end> and <unk>` is defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509649cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157668d",
   "metadata": {},
   "source": [
    "Here we fit the `tokenizer` object on the captions. This helps in the updation of the vocab that the `tokenizer` object might have.\n",
    "\n",
    "In the first iteration the vocabulary does not start from `0`. Both the dictionaries have 1 as the key or value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c78ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "tokenizer.fit_on_texts(df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b032f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sanity check function\n",
    "def check_vocab(word):\n",
    "    i = tokenizer.word_index[word]\n",
    "    print(f\"The index of the word: {i}\")\n",
    "    print(f\"Index {i} is word {tokenizer.index_word[i]}\")\n",
    "    \n",
    "check_vocab(\"<end>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24137f6f",
   "metadata": {},
   "source": [
    "Here we are padding the sentences so that each of the sentences are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c411b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e723c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "path_to_glove_file = os.path.join(\n",
    "    os.path.expanduser(\"~\"), \"/kaggle/working/glove.6B.100d.txt\"\n",
    ")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceafcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1952e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cap = cap_vector[:train_size] \n",
    "val_cap = cap_vector[train_size:train_size+val_size]\n",
    "test_cap = cap_vector[train_size+val_size:]\n",
    "\n",
    "train_cap.shape, val_cap.shape, test_cap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba5d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_train_ds = tf.data.Dataset.from_tensor_slices(train_cap)\n",
    "cap_val_ds = tf.data.Dataset.from_tensor_slices(val_cap)\n",
    "cap_test_ds = tf.data.Dataset.from_tensor_slices(test_cap)\n",
    "\n",
    "cap_train_ds, cap_val_ds, cap_test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac502e",
   "metadata": {},
   "source": [
    "# Image Handling\n",
    "- Load the image\n",
    "- decode jpeg\n",
    "- resize\n",
    "- standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(image_path):\n",
    "    # Decode file as strings\n",
    "    img = tf.io.read_file(image_path)\n",
    "    # Decode image, (0 to 1)\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    # Resize the image to specific height and width\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9221fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = df['image_name'].values\n",
    "\n",
    "train_img = img_name[:train_size] \n",
    "val_img = img_name[train_size:train_size+val_size]\n",
    "test_img = img_name[train_size+val_size:]\n",
    "\n",
    "train_img.shape, val_img.shape, test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train_ds = tf.data.Dataset.from_tensor_slices(train_img).map(load_img)\n",
    "img_val_ds = tf.data.Dataset.from_tensor_slices(val_img).map(load_img)\n",
    "img_test_ds = tf.data.Dataset.from_tensor_slices(test_img).map(load_img)\n",
    "\n",
    "img_train_ds, img_val_ds, img_test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a0e99",
   "metadata": {},
   "source": [
    "# Joint Dataset\n",
    "We have the individual datasets with us. We need to zip the img and the cap dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.zip((img_train_ds, cap_train_ds))\n",
    "val_ds = tf.data.Dataset.zip((img_val_ds, cap_val_ds))\n",
    "test_ds = tf.data.Dataset.zip((img_test_ds, cap_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache, prefecth and batch the dataset\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_ds = train_ds.shuffle(42).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.shuffle(42).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.shuffle(42).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa0330",
   "metadata": {},
   "source": [
    "Sanity check for the division of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, cap in test_ds.take(1):\n",
    "    print(img.shape)\n",
    "    print(cap.shape)\n",
    "    plt.imshow(img[0])\n",
    "    for c in cap[0]:\n",
    "        print(tokenizer.index_word[c.numpy()],end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b4d28",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Show (Encoder)\n",
    "- InceptionV3: This will act like the feature extractor\n",
    "- Use an FC layer to extract the features of the image\n",
    "- The features will be used as the initial hidden state for the RNN\n",
    "\n",
    "### Tell (Decoder)\n",
    "- The initial hidden state is used\n",
    "- The text is embedded\n",
    "- Usage of an LSTM to produce softmax on the vocab\n",
    "- Loss with captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48866017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global variables\n",
    "EMBEDDIN_DIM = 100\n",
    "VOCAB_SIZE = 5000\n",
    "UNITS_RNN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc01749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.inception = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                           weights='imagenet')\n",
    "        self.inception.trainable=False\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "        self.fc = Dense(units=self.embedding_dim,\n",
    "                        activation='sigmoid')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.inception(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a07d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the CNN\n",
    "encoder = CNN_Encoder(EMBEDDIN_DIM)\n",
    "for image, caption in train_ds.take(1):\n",
    "    print(encoder(image).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31730a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = Embedding(len(word_index),\n",
    "                                   EMBEDDING_DIM,\n",
    "                                   weights=[embedding_matrix],\n",
    "                                   input_length=80,\n",
    "                                   trainable=True)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gru = GRU(units=self.units,\n",
    "                       return_sequences=True,\n",
    "                       return_state=True)\n",
    "        self.fc1 = Dense(self.units)\n",
    "        self.fc2 = Dense(self.vocab_size)\n",
    "\n",
    "    def call(self, x, initial_zero=False):\n",
    "        # x, (batch, 512)\n",
    "        # hidden, (batch, 256)\n",
    "        if initial_zero:\n",
    "            initial_state = decoder.reset_state(batch_size=x.shape[0])\n",
    "            output, state = self.gru(inputs=x,\n",
    "                                     initial_state=initial_state)\n",
    "        else:\n",
    "            output, state = self.gru(inputs=x)\n",
    "        # output, (batch, 256)\n",
    "        x = self.fc1(output)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, state\n",
    "    \n",
    "    def embed(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b996d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the RNN\n",
    "decoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n",
    "                      units=UNITS_RNN,\n",
    "                      vocab_size=VOCAB_SIZE)\n",
    "for image, caption in train_ds.take(1):\n",
    "    features = tf.expand_dims(encoder(image),1) # (batch, 1, 128)\n",
    "    em_words = decoder.embed(caption)\n",
    "    x = tf.concat([features,em_words],axis=1)\n",
    "    print(x.shape)\n",
    "    predictions, state = decoder(x, True)\n",
    "    print(predictions.shape)\n",
    "    print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(EMBEDDIN_DIM)\n",
    "decoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n",
    "                      units=UNITS_RNN,\n",
    "                      vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48439ef0",
   "metadata": {},
   "source": [
    "We use `Adam` as the optimizer.\n",
    "\n",
    "The loss is `SparseCategoricalCrossentropy`, because here it would be inefficient to use one-hot-encoders are the ground truth. We will also use mask to help mask the `<pad>` so that we do not let the sequence model learn to overfit on the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277bf385",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    # img_tensor (batch, 224,224,3)\n",
    "    # target     (batch, 80)\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = tf.expand_dims(encoder(img_tensor),1) # (batch, 1, 128)\n",
    "        em_words = decoder.embed(target)\n",
    "        x = tf.concat([features,em_words],axis=1)\n",
    "        predictions, _ = decoder(x, True)\n",
    "\n",
    "        loss = loss_function(target[:,1:], predictions[:,1:-1,:])\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87572fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc360a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_ds.take(20)):\n",
    "        loss = train_step(img_tensor, target)\n",
    "        total_loss += loss\n",
    "        if batch % 10 == 0:\n",
    "            print (f'Epoch {epoch} Batch {batch} Loss {loss.numpy():.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(loss)\n",
    "    print(f'Loss: {total_loss/20:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163cd81",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af59998",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cap = next(iter(test_ds.take(1)))\n",
    "\n",
    "img[0].shape, cap[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.expand_dims(img[0],0)\n",
    "cap = tf.expand_dims(cap[0],0)\n",
    "\n",
    "img.shape, cap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f4e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = tf.expand_dims(encoder(img),1) # (1, 1, 128)\n",
    "\n",
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the image\n",
    "prediction, _ = decoder(feature, True)\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d84de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = tf.reshape(tokenizer.word_index['<start>'], shape=(1,1))\n",
    "em_words = decoder.embed(word)\n",
    "print(em_words.shape)\n",
    "\n",
    "prediction, _ = decoder(em_words)\n",
    "idx = tf.random.categorical(tf.squeeze(prediction,1), 1)[0][0].numpy()\n",
    "word = tokenizer.index_word[idx]\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "while word != '<end>':\n",
    "    print(word, end=\" \")\n",
    "    if count > 20:\n",
    "        break\n",
    "    word_int = tf.reshape(tokenizer.word_index[word], shape=(1,1))  \n",
    "    em_words = decoder.embed(word_int)\n",
    "    prediction, _ = decoder(em_words)\n",
    "    idx = tf.random.categorical(tf.squeeze(prediction,1), 1)[0][0].numpy()\n",
    "    word = tokenizer.index_word[idx]\n",
    "    count += 1\n",
    "\n",
    "plt.imshow(image[0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
