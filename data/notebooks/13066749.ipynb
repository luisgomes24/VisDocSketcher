{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36edf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameter as hp\n",
    "from keras.layers import Dense,Dropout,Activation,Add,MaxPooling2D,Conv2D,Flatten,BatchNormalization,MaxPool2D\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import VGG19,Xception,VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75393230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import train data\n",
    "train_datagen = ImageDataGenerator(rescale=1/255,\n",
    "                                   shear_range = 0.3,\n",
    "                                   zoom_range = 0.3,horizontal_flip = True,\n",
    "                                   vertical_flip =  True , \n",
    "                                   rotation_range=60,\n",
    "                                   brightness_range = (0.4, 1.4))\n",
    "\n",
    "\n",
    "train_data = train_datagen.flow_from_directory('../input/real-life-industrial-dataset-of-casting-product/casting_data/casting_data/train',\n",
    "                                                 target_size = (244, 244),\n",
    "                                                 class_mode='sparse',\n",
    "                                                 shuffle=True,seed=1)\n",
    "#import test data\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "test_data = test_datagen.flow_from_directory(\"../input/real-life-industrial-dataset-of-casting-product/casting_data/casting_data/test\",\n",
    "                                                           batch_size=32,\n",
    "                                                           target_size=(244,244),\n",
    "                                                           class_mode='sparse',\n",
    "                                                           shuffle=True,seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679230ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def_front example\n",
    "image_path = \"../input/real-life-industrial-dataset-of-casting-product/casting_data/casting_data/test/ok_front/cast_ok_0_1019.jpeg\"\n",
    "new_img = image.load_img(image_path, target_size=(250, 250))\n",
    "print(\"ok_font\")\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok example\n",
    "image_path = \"../input/real-life-industrial-dataset-of-casting-product/casting_data/casting_data/test/def_front/cast_def_0_108.jpeg\"\n",
    "new_img = image.load_img(image_path, target_size=(250, 250))\n",
    "img = image.img_to_array(new_img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "print(\"def_font\")\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbeaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viualize data\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar([\"ok\"],[3394],color= \"g\",label='OK')\n",
    "ax.bar([\"def\"],[4211],color =\"b\",label =\"DEF\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fc161",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_count = {\"def_front\" : 4211,\"ok_front\" :3394 }\n",
    "classes_labels = {0:\"def_front\",1:\"ok_front\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3489ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build our model \n",
    "vgg_model =  VGG16(include_top=True , weights='imagenet')\n",
    "for models in vgg_model.layers:\n",
    "  models.trainable= False\n",
    "vgg_model = keras.Model(inputs=vgg_model.input, outputs=vgg_model.layers[-2].output)\n",
    "vgg = keras.Sequential()\n",
    "for layer in vgg_model.layers:\n",
    "  vgg.add(layer)\n",
    "vgg.add(Dropout(0.2))\n",
    "vgg.add(Dense(2, activation='softmax'))\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa849e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile my model\n",
    "vgg.compile(optimizer=keras.optimizers.Adam(0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fae1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our model\n",
    "h = vgg.fit(train_data,validation_data=test_data,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evulate model \n",
    "vgg.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict test data\n",
    "y_prediction =  vgg.predict(test_data)\n",
    "y_prediction =  np.argmax(y_prediction,axis = 1 )\n",
    "\n",
    "print(y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true and y_pred\n",
    "y_true = np.array([])\n",
    "y_pred = np.array([])\n",
    "\n",
    "i = 0\n",
    "for data, labels in test_data:\n",
    "  i += 1\n",
    "  y = np.argmax(vgg.predict(data), axis=1)\n",
    "  y_true = np.append(y_true, labels)\n",
    "  y_pred = np.append(y_pred, y)\n",
    "  \n",
    "  if i == test_data.samples // 32 + 1:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09184e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "cm = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn \n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [0,1]],\n",
    "                  columns = [i for i in [0,1]])\n",
    "seaborn .heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='d')\n",
    "plt.title('confusion matrix')\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59697381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_example 1 \n",
    "image_path = \"../input/real-life-industrial-dataset-of-casting-product/casting_512x512/casting_512x512/def_front/cast_def_0_1055.jpeg\"\n",
    "new_img = image.load_img(image_path, target_size=(244, 244))\n",
    "img = image.img_to_array(new_img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "prediction = vgg.predict(img)\n",
    "prediction = np.argmax(prediction,axis=1)\n",
    "print(prediction)\n",
    "print(classes_labels[prediction[0]])\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8255ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_example 2 \n",
    "image_path = \"../input/real-life-industrial-dataset-of-casting-product/casting_data/casting_data/train/ok_front/cast_ok_0_1016.jpeg\"\n",
    "new_img = image.load_img(image_path, target_size=(244, 244))\n",
    "img = image.img_to_array(new_img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "prediction = vgg.predict(img)\n",
    "prediction = np.argmax(prediction,axis=1)\n",
    "print(prediction)\n",
    "print(classes_labels[prediction[0]])\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e50cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_example 3 \n",
    "image_path = \"../input/real-life-industrial-dataset-of-casting-product/casting_data/casting_data/train/ok_front/cast_ok_0_1019.jpeg\"\n",
    "new_img = image.load_img(image_path, target_size=(244, 244))\n",
    "img = image.img_to_array(new_img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "prediction = vgg.predict(img)\n",
    "prediction = np.argmax(prediction,axis=1)\n",
    "print(prediction)\n",
    "print(classes_labels[prediction[0]])\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2563383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_example 4\n",
    "image_path = \"../input/real-life-industrial-dataset-of-casting-product/casting_512x512/casting_512x512/def_front/cast_def_0_1188.jpeg\"\n",
    "new_img = image.load_img(image_path, target_size=(244, 244))\n",
    "img = image.img_to_array(new_img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "prediction = vgg.predict(img)\n",
    "prediction = np.argmax(prediction,axis=1)\n",
    "print(prediction)\n",
    "print(classes_labels[prediction[0]])\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ada55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = h.history['accuracy']\n",
    "val_acc = h.history['val_accuracy']\n",
    "loss = h.history['loss']\n",
    "val_loss = h.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "#accuracy plot\n",
    "plt.plot(epochs, acc, color='green', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "#loss plot\n",
    "plt.plot(epochs, loss, color='pink', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, color='red', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "vgg.save(\"model_casting.h5\") "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
