{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf95aaad",
   "metadata": {},
   "source": [
    "# BirdCLEF 2023 ðŸ¦\n",
    "> Identify bird calls in soundscapes\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/44224/logos/header.png?t=2023-03-06-18-30-53\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45c5ff",
   "metadata": {},
   "source": [
    "# Methodology  ðŸŽ¯\n",
    "* This notebook will demonstrate **Bird Call Identification** with `TensorFlow`. \n",
    "* This notebook will also show how to infer using `TensorFlow`. For training check below mentioned notebook.\n",
    "* This notebook will use `5sec` audio recording as per requirements. But training is done on much more larger size recording. Dynamic shape is utilize to infer on a different resolution.\n",
    "* This notebook will consider one recording as one batch which will speed up the processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6ea25",
   "metadata": {},
   "source": [
    "# Notebooks ðŸ““\n",
    "\n",
    "* EffNet + FSR + CutMixUp\n",
    "    * Train: [BirdCLEF23: EffNet + FSR + CutMixUp [Train]](https://www.kaggle.com/awsaf49/birdclef23-effnet-fsr-cutmixup-train/edit)\n",
    "    * Infer: [BirdCLEF23: EffNet + FSR + CutMixUp [Infer]](https://www.kaggle.com/awsaf49/birdclef23-effnet-fsr-cutmixup-infer/edit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b6668",
   "metadata": {},
   "source": [
    "# Update ðŸ†•\n",
    "* `v3`:\n",
    "    * Creates batch spectrogram w/o using map function\n",
    "    * Loads all ckpts to memory to reduce bottleneck\n",
    "    * Fixed prediction dtype = `object`.\n",
    "* `v7`:\n",
    "    * upsampe + cv_filter\n",
    "    * effnetb0 + fsr\n",
    "* `v9`: (Bug)!\n",
    "    * train duration: 10 sec\n",
    "    * spec shape: 128 x 384\n",
    "* `v10`:\n",
    "    * Fix for `v9`\n",
    "    * MinMax normalization\n",
    "    * 10 sec recording in train\n",
    "    * spec shape 128 x 384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703d50eb",
   "metadata": {},
   "source": [
    "# Install Libraries ðŸ› "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c83bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('/kaggle/input/efficientnet-keras-dataset/efficientnet_kaggle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9cd802",
   "metadata": {},
   "source": [
    "# Import Libraries ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9670653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "import librosa\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display as lid\n",
    "import IPython.display as ipd\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.optimizer.set_jit(True) # enable xla for speed up\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c736b",
   "metadata": {},
   "source": [
    "## Library Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78495de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('np:', np.__version__)\n",
    "print('pd:', pd.__version__)\n",
    "print('sklearn:', sklearn.__version__)\n",
    "print('librosa:', librosa.__version__)\n",
    "print('tf:', tf.__version__)\n",
    "print('tfio:', tfio.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7d104",
   "metadata": {},
   "source": [
    "# Configuration âš™ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ea29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    verbose = 0\n",
    "    \n",
    "    device = 'CPU'\n",
    "    seed = 42\n",
    "    \n",
    "    # Input image size and batch size\n",
    "    img_size = [128, 384]\n",
    "    batch_size = 16\n",
    "    infer_bs = 2\n",
    "    tta = 1\n",
    "    drop_remainder = True\n",
    "    \n",
    "    # STFT parameters\n",
    "    duration = 5 # duration for test\n",
    "    train_duration = 10\n",
    "    sample_rate = 32000\n",
    "    downsample = 1\n",
    "    trim = True\n",
    "    audio_len = duration*sample_rate\n",
    "    nfft = 2028\n",
    "    window = 2048\n",
    "    hop_length = train_duration*32000 // (img_size[1] - 1)\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "    normalize = True\n",
    "\n",
    "    # Data Preprocessing Settings\n",
    "    class_names = sorted(os.listdir('/kaggle/input/birdclef-2023/train_audio/'))\n",
    "    num_classes = len(class_names)\n",
    "    class_labels = list(range(num_classes))\n",
    "    label2name = dict(zip(class_labels, class_names))\n",
    "    name2label = {v:k for k,v in label2name.items()}\n",
    "    \n",
    "    target_col = ['target']\n",
    "    tab_cols = ['filename','common_name','rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc111d3",
   "metadata": {},
   "source": [
    "# Reproducibility â™»ï¸\n",
    "Sets value for random seed to produce similar result in each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9542c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "#     os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print('seeding done!!!')\n",
    "seeding(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3733a",
   "metadata": {},
   "source": [
    "# Set Up Device  ðŸ“±\n",
    "Following codes automatically detects hardware(tpu or tpu-vm or gpu). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"Detect and intializes GPU/TPU automatically\"\n",
    "    # Check TPU category\n",
    "    tpu = 'local' if CFG.device=='TPU-VM' else None\n",
    "    try:\n",
    "        # Connect to TPU\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu) \n",
    "        # Set TPU strategy\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "        print(f'> Running on {CFG.device} ', tpu.master(), end=' | ')\n",
    "        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n",
    "        device=CFG.device\n",
    "    except:\n",
    "        # If TPU is not available, detect GPUs\n",
    "        gpus = tf.config.list_logical_devices('GPU')\n",
    "        ngpu = len(gpus)\n",
    "         # Check number of GPUs\n",
    "        if ngpu:\n",
    "            # Set GPU strategy\n",
    "            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n",
    "            # Print GPU details\n",
    "            print(\"> Running on GPU\", end=' | ')\n",
    "            print(\"Num of GPUs: \", ngpu)\n",
    "            device='GPU'\n",
    "        else:\n",
    "            # If no GPUs are available, use CPU\n",
    "            print(\"> Running on CPU\")\n",
    "            strategy = tf.distribute.get_strategy()\n",
    "            device='CPU'\n",
    "    return strategy, device, tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPU/TPU/TPU-VM\n",
    "strategy, CFG.device, tpu = get_device()\n",
    "CFG.replicas = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405bc0e8",
   "metadata": {},
   "source": [
    "# Dataset Path ðŸ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/birdclef-2023'\n",
    "GCS_PATH = BASE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f36a7",
   "metadata": {},
   "source": [
    "# Meta Data ðŸ“–\n",
    "* **test_soundscapes/** - directory contains $~200$ recordings to be used for scoring when a notebook is submitted. Without submission only $1$ recording is accessible.  All recordings are $10$ minutes long and in `.ogg` audio format.\n",
    "* **sample_submission.csv** - is the valid sample submission.\n",
    "    * `row_id`: A slug of [soundscape_id]_[end_time] for the prediction.\n",
    "    * `[bird_id]`: There are $264$ bird ID columns. The probability of the presence of each bird for each row needs to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfefa559",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = glob('/kaggle/input/birdclef-2023/test_soundscapes/*ogg')\n",
    "test_df = pd.DataFrame(test_paths, columns=['filepath'])\n",
    "test_df['filename'] = test_df.filepath.map(lambda x: x.split('/')[-1].replace('.ogg',''))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97266a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.gfile.exists(test_df.filepath.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec431b4",
   "metadata": {},
   "source": [
    "# Data Loader ðŸš"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(filepath, sr=32000, normalize=True):\n",
    "    audio, orig_sr = librosa.load(filepath, sr=None)\n",
    "    if sr!=orig_sr:\n",
    "        audio = librosa.resample(y, orig_sr, sr)\n",
    "    audio = audio.astype('float32').ravel()\n",
    "    audio = tf.convert_to_tensor(audio)\n",
    "    if normalize:\n",
    "        audio = Normalize(audio)\n",
    "    return audio\n",
    "\n",
    "# @tf.function\n",
    "# def load_audio_tf(filepath, normalize=True):\n",
    "#     file_bytes = tf.io.read_file(filepath)\n",
    "#     audio = tfio.audio.decode_vorbis(file_bytes)\n",
    "# #     audio = tf.cast(audio_file.to_tensor, tf.float32)\n",
    "#     audio = tf.squeeze(audio, axis=-1)\n",
    "#     if normalize:\n",
    "#         audio = Normalize(audio)\n",
    "#     return audio\n",
    "\n",
    "# Standardize the audio\n",
    "@tf.function(jit_compile=True)\n",
    "def Normalize(data, min_max=True):\n",
    "    # Compute the mean and standard deviation of the data\n",
    "    MEAN = tf.math.reduce_mean(data)\n",
    "    STD = tf.math.reduce_std(data)\n",
    "    # Standardize the data\n",
    "    data = tf.math.divide_no_nan(data - MEAN, STD)\n",
    "    # Normalize to [0, 1]\n",
    "    if min_max:\n",
    "        MIN = tf.math.reduce_min(data)\n",
    "        MAX = tf.math.reduce_max(data)\n",
    "        data = tf.math.divide_no_nan(data - MIN, MAX - MIN)\n",
    "    return data\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def Spec2Img(spec):\n",
    "    spec = tf.tile(spec[..., tf.newaxis], [1, 1, 1, 3])\n",
    "    return spec\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def Img2Spec(img):\n",
    "    return img[..., 0]\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def MakeFrame(audio, duration=5, sr=32000):\n",
    "    frame_length = int(duration * sr)\n",
    "    frame_step = int(duration * sr)\n",
    "    chunks = tf.signal.frame(audio, frame_length, frame_step, pad_end=True)\n",
    "    return chunks\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def Audio2Spec(audio, spec_shape = CFG.img_size, sr=CFG.sample_rate, \n",
    "                    nfft=CFG.nfft, window=CFG.window, fmin=CFG.fmin, fmax=CFG.fmax, return_img=True):\n",
    "    spec_height = spec_shape[0]\n",
    "    spec_width = spec_shape[1]\n",
    "    hop_length = tf.cast(CFG.hop_length, tf.int32) # sample rate * duration / spec width - 1 == 627\n",
    "    spec = tfio.audio.spectrogram(audio, nfft=nfft, window=window, stride=hop_length)\n",
    "    mel_spec = tfio.audio.melscale(spec, rate=sr, mels=spec_height, fmin=fmin, fmax=fmax)\n",
    "    db_mel_spec = tfio.audio.dbscale(mel_spec, top_db=80)\n",
    "    db_mel_spec = tf.linalg.matrix_transpose(db_mel_spec) # to keep it (batch, mel, time)\n",
    "    return db_mel_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e92718",
   "metadata": {},
   "source": [
    "# EDA ðŸŽ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a17a76",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_audio(row):\n",
    "    # Caption for viz\n",
    "    caption = f'Id: {row.filename}'\n",
    "    # Read audio file\n",
    "    audio = load_audio(row.filepath)\n",
    "    # Keep fixed length audio\n",
    "    audio = audio[:CFG.audio_len]\n",
    "    # Spectrogram from audio\n",
    "    spec = Audio2Spec(audio, return_img=False)\n",
    "    # Display audio\n",
    "    print(\"# Audio:\")\n",
    "    display(ipd.Audio(audio.numpy(), rate=CFG.sample_rate))\n",
    "    print('# Visualization:')\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 2*3), sharex=True, tight_layout=True)\n",
    "    fig.suptitle(caption)\n",
    "    # Waveplot\n",
    "    lid.waveshow(audio.numpy(),\n",
    "                 sr=CFG.sample_rate,\n",
    "                 ax=ax[0])\n",
    "    # Specplot\n",
    "    lid.specshow(spec.numpy(), \n",
    "                 sr = CFG.sample_rate, \n",
    "                 hop_length = CFG.hop_length,\n",
    "                 n_fft=CFG.nfft,\n",
    "                 fmin=CFG.fmin,\n",
    "                 fmax=CFG.fmax,\n",
    "                 x_axis = 'time', \n",
    "                 y_axis = 'mel',\n",
    "                 cmap = 'coolwarm',\n",
    "                 ax=ax[1])\n",
    "    ax[0].set_xlabel('');\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2693bcf",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ddd72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_audio(test_df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abfcdc0",
   "metadata": {},
   "source": [
    "# Inference Configs ðŸ”§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fbea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of checkpoint\n",
    "CKPT_DIR = '/kaggle/input/birdclef23-effnet-fsr-cutmixup-train-ds'\n",
    "# Get file paths of all trained models in the directory\n",
    "CKPT_PATHS = sorted(glob(f'{CKPT_DIR}/*h5'))\n",
    "# Load all the models in memory to speed up\n",
    "CKPTS = [tf.keras.models.load_model(x, compile=False) for x in tqdm(CKPT_PATHS, desc=\"Loading ckpts \")]\n",
    "# Num of ckpt to use\n",
    "NUM_CKPTS = 1\n",
    "\n",
    "# Submit or Interactive mode\n",
    "SUBMIT = pd.read_csv('/kaggle/input/birdclef-2023/sample_submission.csv').shape[0] != 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4de98",
   "metadata": {},
   "source": [
    "# Inference ðŸ§ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e52ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start stopwatch\n",
    "tick = time.time()\n",
    "\n",
    "# Initialize empty list to store ids\n",
    "ids = []\n",
    "# Initialize empty array to store predictions\n",
    "preds = np.empty(shape=(0, 264), dtype='float32')\n",
    "\n",
    "# Iterate over each audio file in the test dataset\n",
    "for filepath in tqdm(test_df.filepath.tolist(), 'test '):\n",
    "    # Extract the filename without the extension\n",
    "    filename = filepath.split('/')[-1].replace('.ogg','')\n",
    "    \n",
    "    # Load audio from file and create audio frames, each recording will be a batch input\n",
    "    audio = load_audio(filepath)\n",
    "    chunks = MakeFrame(audio)\n",
    "    \n",
    "#     # If not submitting, only use the first three frames for speed\n",
    "#     if not SUBMIT:\n",
    "#         chunks = chunks[:3]\n",
    "    \n",
    "    # Convert audio frames to spectrograms + rgb image using a vectorized function \n",
    "    specs = Audio2Spec(chunks)\n",
    "    specs = Spec2Img(specs)\n",
    "    \n",
    "    # Predict bird species for all frames in a recording using all trained models\n",
    "    chunk_preds = np.zeros(shape=(len(specs), 264), dtype=np.float32)\n",
    "    for model in CKPTS[:NUM_CKPTS]:\n",
    "        # Get the model's predictions for the current audio frames\n",
    "        rec_preds = model(specs, training=False).numpy()\n",
    "        # Ensemble all prediction with average\n",
    "        chunk_preds += rec_preds/len(CKPTS)\n",
    "    \n",
    "    # Create a ID for each frame in a recording using the filename and frame number\n",
    "    rec_ids = [f'{filename}_{(frame_id+1)*5}' for frame_id in range(len(chunks))]\n",
    "    \n",
    "    # Concatenate the ids\n",
    "    ids += rec_ids\n",
    "    # Concatenate the predictions\n",
    "    preds = np.concatenate([preds, chunk_preds], axis=0)\n",
    "    \n",
    "# Stop stopwatch\n",
    "tock = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e2094",
   "metadata": {},
   "source": [
    "# Submission ðŸ“®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit prediction\n",
    "pred_df = pd.DataFrame(ids, columns=['row_id'])\n",
    "pred_df.loc[:, CFG.class_names] = preds\n",
    "pred_df.to_csv('submission.csv',index=False)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb4af2",
   "metadata": {},
   "source": [
    "## Check Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SUBMIT:\n",
    "    pred_labels = pred_df[pred_df.columns[1:]].values.argmax(axis=1)\n",
    "    pred_classes = list(map(lambda x: CFG.label2name[x], pred_labels))\n",
    "    print(pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305586d",
   "metadata": {},
   "source": [
    "# Submission Time â°\n",
    "Estimated time to complete the submission.\n",
    "> **Note**: There are nearly ~$200$ recordings on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c57b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_time = (tock-tick)*200 # ~200 recording on the test data\n",
    "sub_time = time.gmtime(sub_time)\n",
    "sub_time = time.strftime(\"%H hr: %M min : %S sec\", sub_time)\n",
    "print(f\">> Time for submission: ~ {sub_time}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
