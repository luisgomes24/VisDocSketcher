{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c96d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer #word stemmer class\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e99465",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\n",
    "display(df_total.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tweet):\n",
    "    tweet = tweet.lower() # Convert to lowercase\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet) # Remove words with non-ASCII characters\n",
    "    words = tweet.split()\n",
    "    words = filter(lambda x: x[0]!= '@' , tweet.split()) # Remove user tags\n",
    "    words = [word for word in words if word not in set(stopwords.words('english'))] # Remove stop words\n",
    "    tweet = \" \".join(words)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['preprocessedTweet'] = df_total.tweet.apply(preprocess_text)\n",
    "display(df_total.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bacb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = df_total.preprocessedTweet.apply(lambda x: len(x.split())).max()\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(df_total.preprocessedTweet)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "encoded_tweets = t.texts_to_sequences(df_total.preprocessedTweet)\n",
    "padded_tweets = pad_sequences(encoded_tweets, maxlen=max_length, padding='post')\n",
    "\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028361b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(padded_tweets, df_total.label, test_size=0.2, stratify=df_total.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1717d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocab_size, 50, input_length=max_length, weights=[embedding_matrix], trainable=True))\n",
    "model_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(BatchNormalization())\n",
    "model_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(BatchNormalization())\n",
    "model_glove.add(Bidirectional(LSTM(20)))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(BatchNormalization())\n",
    "model_glove.add(Dense(64, activation='relu'))\n",
    "model_glove.add(Dense(64, activation='relu'))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit train data\n",
    "model_glove.fit(x_train, y_train, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_glove.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c626c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr, rc, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "plt.plot(thresholds, pr[1:])\n",
    "plt.plot(thresholds, rc[1:])\n",
    "plt.show()\n",
    "crossover_index = np.max(np.where(pr == rc))\n",
    "crossover_cutoff = thresholds[crossover_index]\n",
    "crossover_recall = rc[crossover_index]\n",
    "print(\"Crossover at {0:.2f} with recall {1:.2f}\".format(crossover_cutoff, crossover_recall))\n",
    "print(classification_report(y_test, y_pred > crossover_cutoff))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
