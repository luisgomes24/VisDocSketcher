{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2749a5e0",
   "metadata": {},
   "source": [
    "In this notebook, we build a simple convoluation neural network to classify Fashion-MNIST dataset,  which is a dataset of Zalando's article images. The training set consists of 60k examples and test set consists of 10k examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes as mentioned below.\n",
    "* 0 T-shirt/top\n",
    "* 1 Trouser\n",
    "* 2 Pullover\n",
    "* 3 Dress\n",
    "* 4 Coat\n",
    "* 5 Sandal\n",
    "* 6 Shirt\n",
    "* 7 Sneaker\n",
    "* 8 Bag\n",
    "* 9 Ankle boot \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deceebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc777ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [None, 28, 28, 1]\n",
    "number_of_classes = 10\n",
    "\n",
    "#Hyper parameters\n",
    "learning_rate = 0.01\n",
    "epoch = 15\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training and testing data\n",
    "train_data = pd.read_csv('../input/fashion-mnist_train.csv')\n",
    "test_data = pd.read_csv('../input/fashion-mnist_test.csv')\n",
    "\n",
    "print(\"Training data shape: {}\".format(train_data.shape))\n",
    "print(\"Test data shape: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training data separate features and labels\n",
    "train_data_x = train_data.iloc[:, 1:785]\n",
    "train_data_y = train_data.iloc[:, 0:1]\n",
    "\n",
    "#for test data separate features and labels\n",
    "test_data_x = test_data.iloc[:, 1:785]\n",
    "test_data_y = test_data.iloc[:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3112de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale features data\n",
    "train_data_x = train_data_x/255\n",
    "test_data_x = test_data_x/255\n",
    "\n",
    "#Lets convert our training and testing data from pandas dataframes into numpy \n",
    "#arrays needed to train our model\n",
    "\n",
    "train_x = train_data_x.as_matrix()\n",
    "train_y = train_data_y.as_matrix()\n",
    "\n",
    "test_x = test_data_x.as_matrix()\n",
    "test_y = test_data_y.as_matrix()\n",
    "\n",
    "train_x = train_x.reshape(train_x.shape[0], input_shape[1], \n",
    "                          input_shape[2], input_shape[3])\n",
    "test_x = test_x.reshape(test_x.shape[0], input_shape[1], \n",
    "                          input_shape[2], input_shape[3])\n",
    "#Conver output label to one hot vector\n",
    "train_y = to_categorical(train_y, 10)\n",
    "test_y = to_categorical(test_y, 10)\n",
    "\n",
    "\n",
    "print(\"Shape of training features: {}\".format(train_x.shape))\n",
    "print(\"Shape of training lables: {}\".format(train_y.shape))\n",
    "print(\"Shape of testing features: {}\".format(test_x.shape))\n",
    "print(\"Shape of testing lables: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4214a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training set into training and cross validation set. We will use the cross validation\n",
    "#set to check how well our model is doing on unseen training example.\n",
    "\n",
    "train_x, cv_x, train_y, cv_y = train_test_split(train_x, train_y, \n",
    "                                                test_size = 5000, random_state = 42)\n",
    "\n",
    "print(\"Number of examples in training set: {}\".format(train_x.shape[0]))\n",
    "print(\"Number of examples in cross validation set: {}\".format(cv_x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(input_shape, number_of_classes, learning_rate):\n",
    "    input_layer = tf.placeholder(tf.float32, shape=input_shape)\n",
    "    labels = tf.placeholder(tf.float32, shape=[None, number_of_classes])\n",
    "    train_mode = tf.placeholder(tf.bool)\n",
    "    #image ->conv2d->maxpooling->conv2d->maxpooling->flatten->dense->dropout->logits\n",
    "    #convolution layer 1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer, \n",
    "        filters=32, \n",
    "        kernel_size=[5, 5], \n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    #pooling layer 1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    #convolution layer 2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1, \n",
    "        filters=64, \n",
    "        kernel_size=[5, 5], \n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    #pooling layer 1\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    #flatten the output volume of pool2 into a vector\n",
    "    pool2_flat = tf.reshape(pool2, shape=[-1, 7*7*64])\n",
    "    \n",
    "    #dense layer\n",
    "    dense = tf.layers.dense(\n",
    "        inputs=pool2_flat, \n",
    "        units=1024,\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    #dropout regularization\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense, \n",
    "        rate=0.2, \n",
    "        training= train_mode)\n",
    "    \n",
    "    #logits layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\" : tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\" : tf.nn.softmax(logits=logits)\n",
    "    }\n",
    "    \n",
    "    #loss\n",
    "    loss = tf.losses.softmax_cross_entropy(labels, logits)\n",
    "    \n",
    "    #training operartion\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    #accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)), tf.float32))\n",
    "    \n",
    "    return { \"logits\": logits,\n",
    "             \"predictions\": predictions,\n",
    "             \"loss\": loss,\n",
    "             \"train_op\": train_op,\n",
    "             \"accuracy\": accuracy,\n",
    "             \"x\": input_layer,\n",
    "             \"y\": labels,\n",
    "             \"train_mode\": train_mode }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "cnn_model = cnn_model_fn(input_shape, number_of_classes, learning_rate)\n",
    "x = cnn_model[\"x\"]\n",
    "y= cnn_model[\"y\"]\n",
    "train_mode = cnn_model[\"train_mode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64cd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #Divide input training set into mini batches of size batch_size.\n",
    "    #If the total number of training examles is not exactly divisible by batch_size, \n",
    "    #the last batch will have less number of examples than batch_size.\n",
    "    \n",
    "    total_size = train_x.shape[0]\n",
    "    number_of_batches = int(total_size/batch_size)\n",
    "    \n",
    "    print(\"Training:Start\")\n",
    "    for e in range(epoch):\n",
    "        epoch_cost = 0\n",
    "        epoch_accuracy = 0\n",
    "        for i in range(number_of_batches):\n",
    "            mini_x = train_x[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "            mini_y = train_y[i*batch_size:(i+1)*batch_size, :]\n",
    "            _, cost = sess.run([cnn_model[\"train_op\"], cnn_model[\"loss\"]], \n",
    "                feed_dict={x:mini_x, \n",
    "                           y:mini_y,\n",
    "                           train_mode:True})\n",
    "            train_accuracy = sess.run(cnn_model[\"accuracy\"], \n",
    "                feed_dict={x:mini_x, \n",
    "                           y:mini_y,\n",
    "                           train_mode:False})\n",
    "            epoch_cost += cost\n",
    "            epoch_accuracy += train_accuracy\n",
    "        \n",
    "        #If the total number of training examles is not exactly divisible by batch_size, \n",
    "        #we have one more batch of size (total_size - number_of_batches*batch_size)\n",
    "        if total_size % batch_size != 0:\n",
    "            mini_x = train_x[number_of_batches*batch_size:total_size, :, :, :]\n",
    "            mini_y = train_y[number_of_batches*batch_size:total_size, :]\n",
    "            _, cost = sess.run([cnn_model[\"train_op\"], cnn_model[\"loss\"]], \n",
    "                feed_dict={x:mini_x, \n",
    "                           y:mini_y,\n",
    "                           train_mode:True})\n",
    "            train_accuracy = sess.run(cnn_model[\"accuracy\"], \n",
    "                feed_dict={x:mini_x, \n",
    "                           y:mini_y,\n",
    "                           train_mode: False})\n",
    "            epoch_cost += cost\n",
    "            epoch_accuracy += train_accuracy\n",
    "        \n",
    "        epoch_cost /= number_of_batches\n",
    "        \n",
    "        if total_size % batch_size != 0:\n",
    "            epoch_accuracy /= (number_of_batches+1)\n",
    "        else:\n",
    "            epoch_accuracy /= number_of_batches\n",
    "        print(\"Epoch: {} Cost: {} accuracy: {} \".format(e+1, np.squeeze(epoch_cost), epoch_accuracy))\n",
    "    \n",
    "    print(\"Training:End\")\n",
    "    #Cross validation loss and accuracy\n",
    "    cv_loss, cv_accuracy = sess.run([cnn_model[\"loss\"], cnn_model[\"accuracy\"]], \n",
    "            {x:cv_x, \n",
    "             y:cv_y,\n",
    "             train_mode: False})\n",
    "    print(\"Cross validation loss: {} accuracy: {}\".format(np.squeeze(cv_loss), cv_accuracy))\n",
    "    \n",
    "    #prediction for test set\n",
    "    test_accuracy, prediction = sess.run([cnn_model[\"accuracy\"], \n",
    "                                          cnn_model[\"predictions\"][\"classes\"]], \n",
    "                                         {x:test_x, y:test_y, train_mode:False})\n",
    "    print(\"Test set accuracy {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ae30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={'0':'T-shirt/top', '1': 'Trouser', '2' :'Pullover', '3':'Dress', '4': 'Coat', \n",
    "        '5': 'Sandal', '6': 'Shirt', '7': 'Sneaker', '8': 'Bag', '9': 'Ankle boot'} \n",
    "\n",
    "#Let us choose random 100 images from test set and see \n",
    "#actual label and predicted label by our model for the same\n",
    "predicted_labels = prediction\n",
    "permutations = np.random.permutation(10000)\n",
    "\n",
    "fig, axs = plt.subplots(10, 10, figsize = (20, 20))\n",
    "for r in range(10):\n",
    "  for c in range(10):\n",
    "    axs[r, c].imshow(np.reshape(test_x[permutations[10*r+c], :, :, :]*255, [input_shape[1], input_shape[2]]), cmap='Greys')\n",
    "    axs[r, c].axis('off')\n",
    "    axs[r, c].set_title(labels[str(predicted_labels[permutations[10*r+c]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
