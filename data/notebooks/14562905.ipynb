{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488c90d1",
   "metadata": {},
   "source": [
    "# Data Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267184b0",
   "metadata": {},
   "source": [
    "United States 45th President Donald Trump has used Twitter as no one else. He primarily ran his government from a twitter firehose. Twitter has officially banned his account on January 8th 2021 after a deadly riot at Capitol on January 6th 2021. Twitter cites its World Leaders on Twitter: Principles and Approach as a guide to adhere to for public leaders.\n",
    "\n",
    "Trump tweets and policies have far reaching effects that one can realize or he would accept to realize himself. Since, twitter is suspended there is no public way to read his past tweets and analyze it for public policy outcome or link it with global issues.\n",
    "\n",
    "Here we are presenting the complete treasure trove of President Trump's tweet, all 56,572 for the public, data scientists and researchers.\n",
    "\n",
    "The dataset contains 56,572 tweets, tweet IDs, Tweet Date, How many liked and retweeted it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17727424",
   "metadata": {},
   "source": [
    "**Please upvote if you find this notebook helpful! ðŸ˜Š Thank you! I would also be very happy to receive feedback on my work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee65ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#ignore warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb05c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/trumps-legacy/Trumps Legcy.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2357ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc751c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates(subset=['id','text'],keep='first')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length']=df.text.apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0fb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['device'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02133d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = 'device', width = 800, height = 500, title = 'Frequency of Tweets device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b3205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "pattern = r'\\b[^\\d\\W]+\\b'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "en_stop = get_stop_words('en')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22112881",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop  = {\"https\",\"t\",\"co\",\"u\",\"s\",\"rt\"}\n",
    "\n",
    "def converter(x):\n",
    "    try: \n",
    "        return ' '.join([x.lower() for x in str(x).split() if x not in en_stop])\n",
    "    except AttributeError:\n",
    "        return None  # or some other value\n",
    "    \n",
    "    \n",
    "def lematize(x):\n",
    "    try:\n",
    "        return ' '.join([lemmatizer.lemmatize(x)])\n",
    "    except AttributeError:\n",
    "        return None  # or some other value\n",
    "\n",
    "def converterCustom(x):\n",
    "    try: \n",
    "        return ' '.join([x.lower() for x in str(x).split() if x not in stop])\n",
    "    except AttributeError:\n",
    "        return None  # or some other value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_without_stopwords'] = df['text'].apply(converter)\n",
    "df['text_without_stopwords'] = df['text_without_stopwords'].apply(converterCustom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a78f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "# loop through document list\n",
    "for i in df['text_without_stopwords'].iteritems():\n",
    "    # clean and tokenize document string\n",
    "    raw = str(i[1]).lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
    "    \n",
    "    # remove stop words from tokens\n",
    "    #stopped_tokens_new = [raw for raw in stopped_tokens if not raw in remove_words]\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens]\n",
    "    \n",
    "    # remove word containing only single char\n",
    "    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(new_lemma_tokens)\n",
    "\n",
    "# sample data\n",
    "# print(texts[0])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f5c14",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "## Latent Dirichlet allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create term dictionary and document-term matrix\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\n",
    "import pprint\n",
    "pprint.pprint(ldamodel.top_topics(corpus,topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52683b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=ldamodel, corpus=corpus, dictionary=dictionary)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c30c787",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cfd47",
   "metadata": {},
   "outputs": [],
   "source": [
    " def showWordCloud(data):\n",
    "    words = ' '.join(data)\n",
    "    STOPWORDS.update([\"https\",\"t\",\"co\",\"u\",\"s\",\"rt\"])    \n",
    "    cleaned_word = \" \".join([word for word in words.split()])\n",
    "    wordcloud = WordCloud(stopwords = STOPWORDS,\n",
    "                         background_color = 'black',\n",
    "                         width = 2500,\n",
    "                         height = 2500\n",
    "                         ).generate(cleaned_word)\n",
    "    plt.figure(1,figsize = (13,13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "showWordCloud(df['text_without_stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19b729",
   "metadata": {},
   "source": [
    "# sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3434f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "df['sentiment'] = df.text_without_stopwords.map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(df.sentiment);\n",
    "# plt.title(\"Distribution of sentiment polarity of comments\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x=\"sentiment\",width = 800, height = 500,)\n",
    "fig.update_layout(title_text='Distribution of sentiment polarity of comments',\n",
    "                   xaxis_title_text='sentiment', \n",
    "    yaxis_title_text='Density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5214b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " def label(x):\n",
    "    try: \n",
    "        if x > 0.0:\n",
    "            return 'POSITIVE'\n",
    "        if x < 0.0:\n",
    "            return 'NEGITIVE'        \n",
    "        if x == 0.0:\n",
    "            return 'NEUTRAL'\n",
    "        \n",
    "    except AttributeError:\n",
    "        return None  # or some other value\n",
    "\n",
    "df['sentimentLabel'] = df['sentiment'].apply(label)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de811409",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x=\"sentimentLabel\",width = 800, height = 500,)\n",
    "fig.update_layout(title_text='Distribution of sentiment of comments',\n",
    "                   xaxis_title_text='sentiment', \n",
    "    yaxis_title_text='Density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb39ba",
   "metadata": {},
   "source": [
    "# t-SNE\n",
    "\n",
    "Rebuild LDA model with some extra imputs and reduce number of topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbfe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary, \n",
    "                                    num_topics=8, passes=5, minimum_probability=0)\n",
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ea132",
   "metadata": {},
   "source": [
    "Refactoring results of LDA into numpy matrix (number_of_papers x number_of_topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf77884",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = np.array([[y for (x,y) in ldamodel[corpus[i]]] for i in range(len(corpus))])\n",
    "hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfbe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(perplexity=30, early_exaggeration=120)\n",
    "embedding = tsne.fit_transform(hm)\n",
    "embedding = pd.DataFrame(embedding, columns=['x','y'])\n",
    "embedding['hue'] = hm.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bcfffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider\n",
    "from bokeh.layouts import column\n",
    "from bokeh.palettes import all_palettes\n",
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x = embedding.x,\n",
    "            y = embedding.y,\n",
    "            colors = [all_palettes['Set1'][8][i] for i in embedding.hue],\n",
    "            title = df.text,\n",
    "            year = df.date,\n",
    "            alpha = [0.9] * embedding.shape[0],\n",
    "            size = [7] * embedding.shape[0]\n",
    "        )\n",
    "    )\n",
    "hover_tsne = HoverTool(names=[\"df\"], tooltips=\"\"\"\n",
    "    <div style=\"margin: 10\">\n",
    "        <div style=\"margin: 0 auto; width:300px;\">\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">Text:</span>\n",
    "            <span style=\"font-size: 12px\">@title</span>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">Date:</span>\n",
    "            <span style=\"font-size: 12px\">@year</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(plot_width=700, plot_height=700, tools=tools_tsne, title='Tweets')\n",
    "plot_tsne.circle('x', 'y', size='size', fill_color='colors', \n",
    "                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\")\n",
    "\n",
    "layout = column(plot_tsne)\n",
    "show(layout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4010ed",
   "metadata": {},
   "source": [
    "## Work in progress tring to add intresting stuff. if you like my work do \"up vote\""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
