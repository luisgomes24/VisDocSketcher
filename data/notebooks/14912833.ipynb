{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from tensorflow import tfds\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "import math\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.color import lab2rgb\n",
    "import seaborn as sns\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f161e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/tensorflow/io/v0.20.0/tensorflow_io/python/experimental/color_ops.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64591e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from color_ops import rgb_to_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1bda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5047fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9eae5",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37345b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_MODEL = 'lab'\n",
    "IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(image_file):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    \n",
    "    if COLOR_MODEL == 'lab':\n",
    "        image = rgb_to_lab(image)\n",
    "        lightness = image[:,:,0]\n",
    "        lightness = lightness/50-1\n",
    "        lightness = lightness[...,tf.newaxis]\n",
    "        color = image[:,:,1:]/100\n",
    "        color = color*1.3\n",
    "        return lightness, color\n",
    "    else:\n",
    "        lightness = tf.image.rgb_to_grayscale(image)\n",
    "        image = image*2 - 1\n",
    "#     lightness = image[:,:,0]\n",
    "#     lightness = lightness/100\n",
    "#     lightness = lightness[...,tf.newaxis]\n",
    "#     color = image[:,:,1:]/100\n",
    "    \n",
    "        return lightness, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_PATH = KaggleDatasets().get_gcs_path('landscape-pictures')\n",
    "# GCS_PATH = 'gs://tfds-data/datasets/coco'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/*.jpg'))\n",
    "print('Monet TFRecord Files:', len(FILENAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be641d",
   "metadata": {},
   "source": [
    "All the images for the competition are already sized to 256x256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a [-1, 1] scale. Because we are building a generative model, we don't need the labels or the image id so we'll only return the image from the TFRecord."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518ff08",
   "metadata": {},
   "source": [
    "Define the function to extract the image from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd02c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715be2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with strategy.scope():\n",
    "dataset = tf.data.Dataset.from_tensor_slices(FILENAMES)\n",
    "dataset = dataset.map(load, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_PATH = KaggleDatasets().get_gcs_path('artstation-landscape-thumbnails')\n",
    "FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/images/*.jpeg'))\n",
    "print('Monet TFRecord Files:', len(FILENAMES))\n",
    "\n",
    "art_dataset = tf.data.Dataset.from_tensor_slices(FILENAMES)\n",
    "art_dataset = art_dataset.map(load, num_parallel_calls=AUTOTUNE)\n",
    "art_dataset_train = art_dataset.skip(100)\n",
    "art_dataset_val = art_dataset.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(input):\n",
    "    if COLOR_MODEL == 'lab':\n",
    "        l, ab = input\n",
    "        image = np.zeros((IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "        image[:,:,:1] = l[0,...]*50+50\n",
    "        image[:,:,1:] = ab[0,...]*100\n",
    "        image = lab2rgb(image)\n",
    "        lightness = np.array(l[0,...,0])\n",
    "        return image, lightness\n",
    "    else:\n",
    "        lightness, image = input\n",
    "        image = lab2rgb(image*100)\n",
    "    \n",
    "        return image[0,...], lightness[0,...,0]\n",
    "\n",
    "def color_hist(color):\n",
    "    for i in range(color.shape[-1]):\n",
    "        sns.distplot(color[...,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_input, example_target in art_dataset_val.batch(1).take(2):\n",
    "    light = example_input[:1,...]\n",
    "    color = example_target[:1,...]\n",
    "\n",
    "def check_images(light, color):\n",
    "    image, lightness = get_image((light, color))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(tf.squeeze(lightness), cmap='gray')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(tf.squeeze(image))\n",
    "    \n",
    "check_images(light, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c0100",
   "metadata": {},
   "source": [
    "# Build the generator\n",
    "\n",
    "We'll be using a UNET architecture for our CycleGAN. To build our generator, let's first define our `downsample` and `upsample` methods.\n",
    "\n",
    "The `downsample`, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.\n",
    "\n",
    "We'll be using an instance normalization instead of batch normalization. As the instance normalization is not standard in the TensorFlow API, we'll use the layer from TensorFlow Add-ons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd323ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "if COLOR_MODEL=='lab':\n",
    "    OUTPUT_CHANNELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_models.set_framework('tf.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    unet = segmentation_models.Unet('resnet18', encoder_weights='imagenet', classes=OUTPUT_CHANNELS, activation='tanh', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), decoder_use_batchnorm=False)\n",
    "    inp = layers.Input(shape=[IMAGE_SIZE, IMAGE_SIZE, 1], name='input')\n",
    "    x = layers.Concatenate()([inp, inp, inp])\n",
    "    x = unet(x)\n",
    "#     x = tf.linalg.normalize(x)[0]\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ed3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator().summary()\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff45a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_out = generator(example_input, training=False)\n",
    "check_images(example_input, gen_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca31b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_hist(gen_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb331c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "loss_object = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "generator.compile(optimizer=tf.keras.optimizers.Adam(2e-4, beta_1=0.5), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.fit(\n",
    "    art_dataset_train.take(len(art_dataset_train)//8*8).batch(8),\n",
    "    epochs=20,\n",
    "#     batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ce590",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    for example_input, example_target in art_dataset_val.batch(1):\n",
    "        gen_out = generator(example_input, training=False)\n",
    "        check_images(example_input, gen_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64973d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "localhost_save_option = tf.saved_model.SaveOptions(experimental_io_device=\"/job:localhost\")\n",
    "localhost_load_option = tf.saved_model.LoadOptions(experimental_io_device=\"/job:localhost\")\n",
    "generator.save('generator_lab_deterministic', options=localhost_save_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with strategy.scope():\n",
    "#     generator = tf.keras.models.load_model('generator_lab_deterministic', options=localhost_load_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccccd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('generator_lab_deterministic', 'zip', './generator_lab_deterministic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bcc673",
   "metadata": {},
   "source": [
    "# Build the discriminator\n",
    "\n",
    "The discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8949e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_instancenorm=True, name=None):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = keras.Sequential(name=name)\n",
    "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_instancenorm:\n",
    "        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
    "\n",
    "    result.add(layers.LeakyReLU())\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False, name=None):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = keras.Sequential(name=name)\n",
    "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                      padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      use_bias=False))\n",
    "\n",
    "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(layers.Dropout(0.5))\n",
    "\n",
    "    result.add(layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f0d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 5)\n",
    "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n",
    "\n",
    "    inp = layers.Input(shape=[IMAGE_SIZE, IMAGE_SIZE, 1], name='input')\n",
    "    tar = layers.Input(shape=[IMAGE_SIZE, IMAGE_SIZE, OUTPUT_CHANNELS], name='target')\n",
    "#     tar_d = layers.Lambda(color_decoder)(tar)\n",
    "#     tar_d = color_decoder(tar)\n",
    "    x = layers.Concatenate()([tar, inp])\n",
    "#     x = inp\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
    "    down3 = downsample(IMAGE_SIZE, 4)(down2) # (bs, 32, 32, 256)\n",
    "\n",
    "    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
    "    conv = layers.Conv2D(512, 4, strides=1,\n",
    "                         kernel_initializer=initializer,\n",
    "                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "\n",
    "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n",
    "\n",
    "    leaky_relu = layers.LeakyReLU()(norm1)\n",
    "\n",
    "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "\n",
    "    last = layers.Conv2D(1, 4, strides=1,\n",
    "                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18524d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as K \n",
    "# K.clear_session()\n",
    "\n",
    "with strategy.scope():\n",
    "    discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d594221",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_out = discriminator([example_input, gen_out], training=False)\n",
    "plt.imshow(disc_out[0, ..., -1], cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a30b4",
   "metadata": {},
   "source": [
    "# Build the CycleGAN model\n",
    "\n",
    "We will subclass a `tf.keras.Model` so that we can run `fit()` later to train our model. During the training step, the model transforms a photo to a Monet painting and then back to a photo. The difference between the original photo and the twice-transformed photo is the cycle-consistency loss. We want the original photo and the twice-transformed photo to be similar to one another.\n",
    "\n",
    "The losses are defined in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGan(keras.Model):\n",
    "    def __init__(\n",
    "                self,\n",
    "                generator,\n",
    "                discriminator,\n",
    "                lambda_cycle=10,\n",
    "                ):\n",
    "        super(CycleGan, self).__init__()\n",
    "        self.gen = generator\n",
    "        self.disc = discriminator\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        \n",
    "    def compile(\n",
    "                self,\n",
    "                gen_optimizer,\n",
    "                disc_optimizer,\n",
    "                gen_loss_fn,\n",
    "                disc_loss_fn,\n",
    "                ):\n",
    "        super(CycleGan, self).compile()\n",
    "        self.gen_optimizer = gen_optimizer\n",
    "        self.disc_optimizer = disc_optimizer\n",
    "        self.gen_loss_fn = gen_loss_fn\n",
    "        self.disc_loss_fn = disc_loss_fn\n",
    "        \n",
    "    def train_step(self, batch_data):\n",
    "        input_image, target = batch_data\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "            gen_output = self.gen(input_image, training=True)\n",
    "\n",
    "            # discriminator used to check, inputing real images\n",
    "            disc_real_output = self.disc([input_image, target], training=True)\n",
    "            # discriminator used to check, inputing fake images\n",
    "            disc_generated_output = self.disc([input_image, gen_output], training=True)\n",
    "\n",
    "            # evaluates generator loss\n",
    "            gen_total_loss, gen_gan_loss, gen_l1_loss = self.gen_loss_fn(disc_generated_output, gen_output, target)\n",
    "            # evaluates discriminator loss\n",
    "            disc_loss = self.disc_loss_fn(disc_real_output, disc_generated_output)\n",
    "\n",
    "        # Calculate the gradients for generator and discriminat\n",
    "        generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                              self.gen.trainable_variables)\n",
    "\n",
    "        discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                                   self.disc.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the optimizer\n",
    "        \n",
    "        generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                              self.gen.trainable_variables))\n",
    "\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                                  self.disc.trainable_variables))\n",
    "        return {\n",
    "                'gen_total_loss': gen_total_loss,\n",
    "                'gen_gan_loss': gen_gan_loss,\n",
    "                'gen_l1_loss': gen_l1_loss,\n",
    "                'disc_loss': disc_loss\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1764687b",
   "metadata": {},
   "source": [
    "# Define loss functions\n",
    "\n",
    "The discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd80d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "    def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "        real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "        generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "        total_disc_loss = real_loss + generated_loss\n",
    "        return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f625bd83",
   "metadata": {},
   "source": [
    "The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb35841",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    LAMBDA = 100000\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "#     loss_object_2 = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "    def generator_loss(disc_generated_output, gen_output, target):\n",
    "        gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "        # Mean absolute error\n",
    "        l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "        total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "        return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a164ef",
   "metadata": {},
   "source": [
    "We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2836dc",
   "metadata": {},
   "source": [
    "The identity loss compares the image with its generator (i.e. photo with photo generator). If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84d1cd",
   "metadata": {},
   "source": [
    "# Train the CycleGAN\n",
    "\n",
    "Let's compile our model. Since we used `tf.keras.Model` to build our CycleGAN, we can just ude the `fit` function to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791fb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be05d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    for example_input, example_target in dataset.batch(1).take(1):\n",
    "        gen_out = generator(example_input, training=False)\n",
    "        check_images(example_input, gen_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        with strategy.scope():\n",
    "            gen_out = self.model.gen(example_input, training=False)\n",
    "            check_images(example_input, gen_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    cycle_gan_model = CycleGan(generator, discriminator)\n",
    "    cycle_gan_model.compile(\n",
    "                                gen_optimizer = generator_optimizer,\n",
    "                                disc_optimizer = discriminator_optimizer,\n",
    "                                gen_loss_fn = generator_loss,\n",
    "                                disc_loss_fn = discriminator_loss,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfac4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c24e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_gan_model.fit(\n",
    "    art_dataset_train.take(len(art_dataset_train)//8*8).batch(8),\n",
    "    epochs=40,\n",
    "    batch_size=8,\n",
    "#     callbacks=[CustomCallback()],\n",
    "#     validation_data=art_dataset_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70502ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    for example_input, example_target in art_dataset_val.batch(1):\n",
    "        gen_out = generator(example_input*1, training=False)\n",
    "        check_images(example_input, gen_out*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3005d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('generator_lab_gan_art', options=localhost_save_option)\n",
    "shutil.make_archive('generator_lab_gan_art', 'zip', './generator_lab_gan_art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
