{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d156b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f67872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15652aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pmdarima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98727138",
   "metadata": {},
   "source": [
    "## Step 1. Convert into Time Series Data\n",
    "> Let's first grab the demand data from the pizza sales data in this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pd.read_excel(\"../input/pizza-sales/Data Model - Pizza Sales.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = _[['order_date', 'quantity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50366864",
   "metadata": {},
   "outputs": [],
   "source": [
    "_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = _.groupby(['order_date'])['quantity'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e549131",
   "metadata": {},
   "outputs": [],
   "source": [
    "_['order_date'] = pd.to_datetime(_['order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259481fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "_.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394bcad",
   "metadata": {},
   "source": [
    ">Our data is now good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bdf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a4021",
   "metadata": {},
   "source": [
    "## Step 2. Visualize Plot\n",
    "> We are visual human beings, let's appreciate how our time series graph looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87be714",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,8))\n",
    "fig = plt.plot(_.order_date, _.quantity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd152996",
   "metadata": {},
   "source": [
    "> Looks like a doctor's handwriting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4c3c8",
   "metadata": {},
   "source": [
    "## Step 3. Dickey-Fuller Test\n",
    "> Stats time! This test is one way of assessing whether a time series is **stationary or not**.\n",
    "\n",
    "> A time series is denoted as **stationary** if it has no trend or doesn't exhibit constant variance over time.\n",
    "\n",
    "> The test assumes the following null and alternative hypotheses:\n",
    "\n",
    "        - H0: The time series is non-stationary.\n",
    "        - HA: The time series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32104435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntax for Dickey-Fuller Test\n",
    "adfuller(_['quantity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3451c9",
   "metadata": {},
   "source": [
    "> The 2nd measure in the list is the most important to look at for it's the **p-value**.\n",
    "\n",
    "> Since the p-value < .05, we reject the null hypothesis and conclude that the time series is **stationary.** That means less work for us.\n",
    "\n",
    "> However, if the time series turned out not to be stationary, then there is an additional step that needs to be done. This step is known as **differencing,** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3b7b4",
   "metadata": {},
   "source": [
    "## Step 4. Autocorrelation and Partial Autocorrelation\n",
    "> Before you can fit an **ARIMA** model, you need a threesome of the following parameters\n",
    "\n",
    "        - p: autoregressive: the number of autoregressive terms\n",
    "        - d: difference: the number of nonseasonal differences to attain stationary.\n",
    "        - q: moving average: number of lagged forecast errors in the prediction equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d702d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(_['quantity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254dcf88",
   "metadata": {},
   "source": [
    "> To determine p value for our ARIMA model, we look at **Partial Autocorrelation Function (PACF) plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = sm.graphics.tsa.plot_pacf(_['quantity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3021ecc",
   "metadata": {},
   "source": [
    "> To determine q value for our ARIMA model, we look at **Autocorrelation Function (ACF) plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac35c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = sm.graphics.tsa.plot_acf(_['quantity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3585db",
   "metadata": {},
   "source": [
    "How do we interpret this? \n",
    "\n",
    "> Highly recommend u to read this [How to Interpret ACF and PACF plots for Dummies like you :3](https://medium.com/@ooemma83/how-to-interpret-acf-and-pacf-plots-for-identifying-ar-ma-arma-or-arima-models-498717e815b6#:~:text=The%20basic%20guideline%20for%20interpreting,q%20for%20MA(q).)\n",
    "\n",
    "> It's clearly shown in both ACF and PACF plots that they start to cut off at **1 (lag score).**\n",
    "\n",
    "> It's generally advised to select a lower lag score.\n",
    "\n",
    "> Since our data is already stationary from the get-go, we didn't perform any differencing, so our d remains 0.\n",
    "\n",
    "> Our final ARIMA model will take the following parameters then **ARIMA(1,0,1).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3165f",
   "metadata": {},
   "source": [
    "## Step 5. ARIMA time baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedde04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(_['quantity'], order=(1,0,1))\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "_['forecast'] = model_fit.predict()\n",
    "_[['quantity','forecast']].plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2592e",
   "metadata": {},
   "source": [
    "## Step 6. ARIMA, but more efficient...\n",
    "\n",
    "> This works kind of like **hyperparameter tuning**. Essentially, you test every combinations of ARIMA parameters, then select the best parameters that yielded the best performing ARIMA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5404f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run combinations of ARIMA(p,d,q)\n",
    "# Set max p and max q at 7\n",
    "run_tests = auto_arima(_['quantity'], max_p=7, d=0, max_q=7, trace=True, suppress_warnings=True)\n",
    "run_tests.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(_['quantity'], order=(0,0,1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "_['forecast'] = model_fit.predict()\n",
    "_[['quantity','forecast']].plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17822960",
   "metadata": {},
   "outputs": [],
   "source": [
    "_['order_month'] = pd.DatetimeIndex(_['order_date']).month\n",
    "monthly_sales = _.groupby(['order_month'])['quantity'].sum().reset_index()\n",
    "# plt.figure(figsize=(12,8))\n",
    "fig = plt.plot(monthly_sales.order_month, monthly_sales.quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb506f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
