{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23e00b",
   "metadata": {},
   "source": [
    "# NLP Twitter Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635756b8",
   "metadata": {},
   "source": [
    "In this notebook, we will practice our NLP modeling skill to build a classification model to predict if a twitter concerns a disaster. The workflow is like follows:\n",
    "\n",
    "1. Tokenize the tweets and convert into sequences.\n",
    "2. Apply word embedding to convert the sequences into tensors.\n",
    "3. Build a CNN model to classify the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bbf00",
   "metadata": {},
   "source": [
    "## Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, NLTKWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import gensim.downloader as api\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474322b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAT_DIR = \"../input/nlp-getting-started\"\n",
    "train = pd.read_csv(os.path.join(DAT_DIR, \"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a96f16",
   "metadata": {},
   "source": [
    "Let quickly review sample of keywords and locations. It seems the keyword, if non-empty, could also repeat in the text. In addition, without context, the location may not be a good indicator of the disaster. Therefore, we will just use the text field in this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcea656",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[~train.keyword.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dbc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fad862",
   "metadata": {},
   "source": [
    "We will use the Twitter tokenizer first, then lemmerize the texts and convert to lower case, and finally save the results into texts. We will then use Keras's tokenizer to transform the text into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk =TweetTokenizer()\n",
    "#tk =NLTKWordTokenizer()\n",
    "texts = train.text.apply(tk.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "texts2 = [[lemmer.lemmatize(w).lower() for w in sentence] for sentence in texts]\n",
    "text_jnt = [' '.join(txt) for txt in texts2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47852169",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_jnt)\n",
    "text_sequences = tokenizer.texts_to_sequences(text_jnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686556c4",
   "metadata": {},
   "source": [
    "Let's pad the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
    "print(f'text_sequences.shape = {text_sequences.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = len(text_sequences)\n",
    "max_seqlen = len(text_sequences[0])\n",
    "print(f'num_records = {num_records}, max_seqlen= {max_seqlen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90648f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "NUM_CLASSES = 2\n",
    "cat_labels = tf.keras.utils.to_categorical(train.target,\n",
    "                                          num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx['PAD'] = 0\n",
    "idx2word[0] = 'PAD'\n",
    "vocab_size = len(word2idx)\n",
    "print(f'vocab_size = {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9e98c",
   "metadata": {},
   "source": [
    "For word embedding, let's review the available Twitter embeddings in the gensim module as are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624cc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in api.info()['models'].keys() if 'twitter' in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6dde2b",
   "metadata": {},
   "source": [
    "For simplicity, let's use the smallest embedding, i.e., 'glove-twitter-25'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'glove-twitter-25'\n",
    "EMBEDDING_DIM = 25\n",
    "#EMBEDDING_MODEL = 'glove-twitter-50'\n",
    "#EMBEDDING_DIM = 50\n",
    "DATA_DIR = '.'\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, 'E.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2747277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(sequences, word2idx, embedding_dim, embedding_file):\n",
    "    if os.path.exists(embedding_file):\n",
    "        E = np.load(embedding_file)\n",
    "    else:\n",
    "        vocab_size = len(word2idx)\n",
    "        E = np.zeros((vocab_size, embedding_dim))\n",
    "        word_vectors = api.load(EMBEDDING_MODEL)\n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx] = word_vectors.word_vec(word)\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "        np.save(EMBEDDING_NUMPY_FILE, E)\n",
    "        \n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM, EMBEDDING_NUMPY_FILE)\n",
    "print(f'Embedding matrix: {E.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0094bfec",
   "metadata": {},
   "source": [
    "# Build CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa53bf1",
   "metadata": {},
   "source": [
    "In this section, we will build a simple CNN to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_num_filters = 514\n",
    "conv_kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, input_length=max_seqlen,\n",
    "                             weights = [E], trainable=False),\n",
    "    \n",
    "    # block 1\n",
    "    tf.keras.layers.Conv1D(filters = conv_num_filters,\n",
    "                          kernel_size=conv_kernel_size,\n",
    "                          activation='relu'),    \n",
    "    tf.keras.layers.SpatialDropout1D(0.2),\n",
    "    tf.keras.layers.GlobalMaxPool1D(),\n",
    "    \n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5191542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = [tfa.metrics.F1Score(num_classes=NUM_CLASSES)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa89110",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "VERBOSE = 1\n",
    "SEED = 1234\n",
    "BATCH_SIZE = 128\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "model.fit(text_sequences,\n",
    "          cat_labels,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          verbose = VERBOSE,\n",
    "          validation_split = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828775b0",
   "metadata": {},
   "source": [
    "The validation results seem good.  Therefore, we will proceed to prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb202b71",
   "metadata": {},
   "source": [
    "## Prediction on the testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc106d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(DAT_DIR, \"test.csv\"))\n",
    "test_texts = test.text.apply(tk.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts2 = [[lemmer.lemmatize(w).lower() for w in sentence] for sentence in test_texts]\n",
    "test_text_jnt = [' '.join(txt) for txt in test_texts2]\n",
    "test_text_sequences = tokenizer.texts_to_sequences(test_text_jnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_sequences = tf.keras.preprocessing.sequence.pad_sequences(test_text_sequences)\n",
    "print(f'test_text_sequences.shape = {test_text_sequences.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat_labels = model.predict(test_text_sequences)\n",
    "print(test_cat_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.argmax(test_cat_labels, axis=1)\n",
    "test['predict'] = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e911df",
   "metadata": {},
   "source": [
    "Let's review a few positive and negative predictions for sanity check. Overall, the prediction seems to make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae02aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test.predict == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec069ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test.predict == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3cddf",
   "metadata": {},
   "source": [
    "## Submit Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(os.path.join(DAT_DIR, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afeda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['target'] = test['predict']\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664219d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission.csv', index=None)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
