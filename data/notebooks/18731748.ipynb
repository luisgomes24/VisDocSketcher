{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce138948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f9b7d",
   "metadata": {},
   "source": [
    "# Predicting Loan Application\n",
    "This code below developement version of code that credit to [vipin kumar](https://www.kaggle.com/vipin20/loan-prediction-problem) who gave me idea to improvise the code. Several additional data preprocessing is updated to improve the quality of data in order to reach better quality of prediction, accuracy, recall and the score. \n",
    "The objective of the data analyisis is finding best model that give better prediction for loans status whether will be approved(yes) or rejected(No) based on criteria of applicant(Gender, Status (married, Dependent), Education, Jobs(employee or self employed), Income(Total Income), Loan Amount,Loan Term Credit History, and Property Area. \n",
    "The analysis will be divided to several steps\n",
    "1. Data Overview\n",
    "2. Data Cleaning & Transform\n",
    "   * 2.1 Drop unecessary columns\n",
    "   * 2.2 Transform 'Total_Income' data from Object into Float\n",
    "   * 2.3 Review the Data Statistik to Understand the Distribution\n",
    "   * 2.4  Checking of Missing Data\n",
    "   * 2.5 Checking the Skewness and Kurtosis of Data\n",
    "3. Build Model Select the suitable Model\n",
    "   * 3.1 Dividing Dataframe\n",
    "   * 3.2 Splitting the Data of x and y \n",
    "   * 3.3 Testing Several Model\n",
    "   * 3.4 Chosen Model, Threshold and Scores\n",
    "\n",
    "We are going to use data of [Loan Application Data](http://https://www.kaggle.com/vipin20/loan-application-data).\n",
    "Before we start making the code and start data processing, we import necessary module to our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import warnings as wr\n",
    "wr.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef67d18",
   "metadata": {},
   "source": [
    "## 1. Data Overview\n",
    "We will load dataset and tryhing to review the data. The number or rows and columns, the atributes, the quality of data and make prelimenery judgement what to do for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and overview the dataset\n",
    "loans = pd.read_csv('../input/loan-application-data/df1_loan.csv')\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdaf870",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecda9af",
   "metadata": {},
   "source": [
    "the dataset has 15 columns and 500 row of data. 6 numerical columns (float and integer), and 9 object. We found Total_Income columns identified as \"object(string)\" ( We need to transform this data later to numerical later in the proces.)\n",
    "\n",
    "\n",
    "## 2. DATA CLEANING\n",
    "After reviewing the data, We have and idea what the data about and the quality of data, to ensure the data integrity, we will started process data cleaning.\n",
    "### 2.1 Drop unecessary columns \n",
    "'Unnamed:0' and 'Loan_ID' is not important for our data analyisis and we need to drop the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d435e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Unnecessary columns\n",
    "loans = loans.drop(['Unnamed: 0', 'Loan_ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4133f3",
   "metadata": {},
   "source": [
    "### 2.2 Transform 'Total_Income' data from Object into Float\n",
    "Total_Income is important keys for the analysis. We need to transform 'Total_Income' into numeric data so we can process the data for further process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove $ in data 'Total_Income'\n",
    "loans['Total_Income'] = loans['Total_Income'].str.replace('$',' ')\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5af06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the 'Total Income' Data type from Objet into Float\n",
    "loans['Total_Income'] = loans['Total_Income'].astype(float)\n",
    "loans.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3cddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947ea56",
   "metadata": {},
   "source": [
    "### 2.3 Review the Data Statistik to Understand the Distribution\n",
    "Knowing data distribution is important step for data analysis. We need to avoid data that can skew our data set. this is can be reached by view the data statistic describtion and boxplot\n",
    "\n",
    "#### 2.3.1 Dataset Statistic Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da720483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average, Percentile,Median, Maximum and Minimum Data\n",
    "loans.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2b983",
   "metadata": {},
   "source": [
    "#### 2.3.2 Boxplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "boxplot = loans.boxplot(column=['ApplicantIncome', 'CoapplicantIncome', 'Total_Income' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d90d2e",
   "metadata": {},
   "source": [
    "From Statistic Describtion and Box plot, we will focus on \"Total_Income\". We can use both data to remove outliers that can skew the dataset and analysis result. We will find out the maximum outside range and will filter out data from outside range.\n",
    "Formula:\n",
    "Threshold_Max_range = Q75 - 1.5 *(Q75-Q25) = 7495.25 + 1.5*(7495.25-4166) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa05c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold_Max_Total_Income = 7495.25 + 1.5*(7495.25-4166)\n",
    "loans1 =loans[loans['Total_Income']< Threshold_Max_Total_Income]\n",
    "boxplot = loans1.boxplot(column=['ApplicantIncome', 'CoapplicantIncome', 'Total_Income' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total rows and columns\n",
    "loans1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18583d",
   "metadata": {},
   "source": [
    "New Box plot shown about 41 rows of data that outliers removed from the dataset. We will reflect the data distribution on histogram below.\n",
    "\n",
    "#### 2.3.3 Histogram Graphic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ee82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram Graphic\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(loans1['Total_Income'], bins=20, align='right', color='blue', edgecolor='black')\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlabel(\"Total Income\")\n",
    "plt.title(' Histogram of Total Income ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e276e3",
   "metadata": {},
   "source": [
    "### 2.4  Checking of Missing Data\n",
    "In order to enhance data integrity, we will find missing data and fill the data. We use average value to fill the numerical data and generate string for object data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the missing data\n",
    "loans1.isnull().sum().sort_values(ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41840a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking The missing data Percentage From total rows of each atributes\n",
    "total_null = loans1.isnull().sum().sort_values(ascending=False) \n",
    "count = loans1.isnull().count().sort_values(ascending=False) #total data of rows #500\n",
    "percentage = (loans1.isnull().sum()/loans1.isnull().count()).round(2).sort_values(ascending=False)*100 #First sum and order all null values for each variabl\n",
    "missing_data = pd.concat([total_null, percentage], axis=1, keys=['Total', 'Percentage'])\n",
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd333a0a",
   "metadata": {},
   "source": [
    "The missing data maximum is 8.5 % from 459 observation which mean it is considering low. Generating data with this amount consider safe without skewing the data.\n",
    "The Missing data will be generated using existing data.\n",
    "\n",
    "#### 2.4.1 Filling numerical missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdfea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding numeric column\n",
    "num_column = loans1._get_numeric_data().columns.tolist()\n",
    "#finding category column\n",
    "cat_column = set(loans1.columns)-set(num_column)\n",
    "\n",
    "#Filling numerical missing values\n",
    "for col in num_column:\n",
    "    loans1[col].fillna(loans1[col].mean(), inplace=True)\n",
    "# Filling string missing values\n",
    "for col in cat_column:\n",
    "    loans1[col].fillna(loans1[col].mode()[0],inplace=True)  \n",
    "# Verification if there null numbers \n",
    "loans1.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fb9bc",
   "metadata": {},
   "source": [
    "### 2.4 Finding the Duplicate Data\n",
    "Duplicate data can skew the analysis. We will check if there any duplicate data exist in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans1.duplicated().sum() \n",
    "# Data shown no duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdd765",
   "metadata": {},
   "source": [
    "### 2.5 Checking the Skewness and Kurtosis of Data\n",
    "Skewness is id symetrical distribution and Kurtosis is heaviness distribution\n",
    "The guidance for Skewness:\n",
    " 1. Data is fairly symmetrical in range -0.5 and 0.5\n",
    " 2. Moderate Skewness in between -1 and -0.5 or 0.5 and 1\n",
    " 3. Highly Skewness is under -1 and above 1\n",
    "THe guidance of Kurtosis:\n",
    " 1. Leptokurtik, Distribution is tall and thin (K>3)\n",
    " 2. Plotikurtik, Distribution is flat and value is spread out (K <3)\n",
    " 2. Mesokurtik, Distribution is in between ( K =3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a6c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the \"Total_Income\" Skewness and Kurtosis\n",
    "sns.distplot(loans1['Total_Income'])\n",
    "print(\"Skewness coeff. is: %f\" % loans1['Total_Income'].skew().round(2)) # 0.934\n",
    "print(\"Kurtosis coeff. is: %f\" % loans1['Total_Income'].kurt().round(2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065d2d2",
   "metadata": {},
   "source": [
    "The distribution graphic show the data is moderate skewness and the data moderatly spreadout(Platykurtik). It mean data in in good range distribution\n",
    "\n",
    "After data Cleaning and Transform, we can start build model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e44a1",
   "metadata": {},
   "source": [
    "## 3. Build Model \n",
    "To build model, we need to import necessery module for the program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdd2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0ec1d",
   "metadata": {},
   "source": [
    "### 3.1 Dividing Dataframe\n",
    "We divide dataframe into x and y to build relation model. for x, we use all attributes except for 'Loan_Status'. for y, we use the 'Loan_Status'. String data in dataframe willbe transform into binary format (1 and 0). for Loan_Status/y, which was previously contain string 'Yes' and 'No', will be transform into 1 and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ffc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making two variable dataframe x, y\n",
    "x=loans1.drop(['Loan_Status'], axis=1)\n",
    "y=loans1['Loan_Status']\n",
    "# Transform the x data into binary\n",
    "x=pd.get_dummies(x)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf799c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Binarizer, transform string to binary value \n",
    "lb =LabelBinarizer()\n",
    "y=lb.fit_transform(y)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9dd4e",
   "metadata": {},
   "source": [
    "### 3.2 Splitting the Data of x and y \n",
    "We will split the data into Train and Test. Will take 30% of data set for the test. The data will split into x_train, y_train, x_test, y_test. The Train data are the datasets used to build the model and the test data are data used to testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b264e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting the data\n",
    "x_train,x_test, y_train, y_test = train_test_split(x,y, test_size =0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21c380",
   "metadata": {},
   "source": [
    "### 3.3 Testing Several Model\n",
    "We will find suitable model from 5 models that gives more accurate prediction. The Model we use is Decision Tree Classifier, Gaussian NB, K neighbors Classifier, and Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "dtf = DecisionTreeClassifier()\n",
    "dtf.fit(x_train, y_train)\n",
    "\n",
    "# Gaussian NB\n",
    "n_b = GaussianNB()\n",
    "n_b.fit(x_train, y_train)\n",
    "\n",
    "# K Neighbors Classifier\n",
    "knn = KNeighborsClassifier()  \n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a361da",
   "metadata": {},
   "source": [
    "We test the accuracy of model using data test in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bee32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# least model\n",
    "print(\"Decision Tree Classifier Score:\",dtf.score(x_test, y_test).round(2))\n",
    "print(\"Gaussian NB Score:\",n_b.score(x_test, y_test).round(2)) \n",
    "print(\"K Neighbors Classifier Score:\",knn.score(x_test, y_test).round(2))\n",
    "print(\"Random Forest Classifier Score:\",rfc.score(x_test, y_test).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a14f33",
   "metadata": {},
   "source": [
    "Random Forest Classifier Show promising accuracy Score in 0.82. However, to improve the accuracy of the model, we will analyise model using confusing matrix, the Receiver operating characteristic (ROC) Curve and find the threshold. \n",
    "\n",
    "#### 3.3.1 Confusing Matrix\n",
    " With confusing Matrix, we will find True Positive Value (TP), True Negative Value (TN), False Positif Value (FP), and False Negative Value (FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y predict and Confusing Matrix\n",
    "y_predict=rfc.predict(x_test)\n",
    "CM = confusion_matrix(y_test, y_predict)\n",
    "print(CM) # TP = 89, TN = 20, FP=23, FN = 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aeed8d",
   "metadata": {},
   "source": [
    "#### 3.3.2 Receiver Operating Charaacteristing (ROC)\n",
    "ROC is visual represent how well the classification model work. We will define True Positif Rate (TPR), False Positive Rate (FPR), and Threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3797a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.predict_proba(x_test[0:10])\n",
    "# roc_curve, auc,roc_auc_score \n",
    "fpr,tpr,threshold = roc_curve(y_test, rfc.predict_proba(x_test)[:,1] )\n",
    "roc_auc=roc_auc_score(y_test,rfc.predict(x_test))\n",
    "#graphic plot\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)'%roc_auc )\n",
    "plt.plot([0,1],[0,1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.1])\n",
    "plt.ylim([0.0,1.1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d4f1d",
   "metadata": {},
   "source": [
    "#### 3.3.3 Finding Threshold\n",
    "Finding Threshold between 0 to 1 is very qualitative approach. When the Threshold high, The data will be more precision (less false positive) Since we are handling loans approval data, which mean we want to people have chance to get loans. Threshold value is depend on the need. For this case, we will generate threshold dummy and get several output of Accuracy, Precision, Recall, and f1 scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fcaa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_proba=rfc.predict_proba(x_test)\n",
    "threshold_dummy = [0,0.2,0.4,0.6,0.8,1]\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "for i in threshold_dummy:\n",
    " y_pred_class = binarize(y_predict_proba,i)\n",
    " y_pred_class1 = y_pred_class[:,1].astype(int)  \n",
    " accuracy_scores.append(accuracy_score(y_test, y_pred_class1))\n",
    " precision_scores.append(precision_score(y_test, y_pred_class1))\n",
    " recall_scores.append(recall_score(y_test, y_pred_class1))\n",
    " f1_scores.append(f1_score(y_test, y_pred_class1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "TL = DataFrame([threshold_dummy, accuracy_scores,precision_scores, recall_scores, f1_scores]).transpose().round(2)\n",
    "TL.columns =['threshold', 'accuracy_scores','precision_scores', 'recall_scores', 'f1_scores']\n",
    "TL.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07956f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphic threshold vs Score\n",
    "plt.plot(TL['threshold'],TL['accuracy_scores'] , color='darkorange', lw=lw, label='accuracy ' )\n",
    "plt.plot(TL['threshold'],TL['precision_scores'] , color='red', lw=lw, label='precision ')\n",
    "plt.plot(TL['threshold'],TL['recall_scores'] , color='blue', lw=lw, label='recall')\n",
    "plt.plot(TL['threshold'],TL['f1_scores'] , color='yellow', lw=lw, label='f1_scores ')\n",
    "plt.plot([0.4,0.4],[0,1], color='black', lw=lw,label='threshold =0.4', linestyle='--')\n",
    "plt.xlim([0.0, 1.1])\n",
    "plt.ylim([0.0,1.1])\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Threshold vs Score')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03057d89",
   "metadata": {},
   "source": [
    "We found optimal score about 0.4 where the scores show optimal numbers using random forest classifier.\n",
    "\n",
    "### 3.4 Chosen Model, Threshold and Scores\n",
    "to summarise, we use Random forest classifier because it show better accuracy compare than other model. The optimal threshold at 0.4. the model using to make prediction and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40233eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_proba=rfc.predict_proba(x_test)\n",
    "y_pred_class=binarize(y_predict_proba,0.4)\n",
    "y_pred_class2=y_pred_class[:,1].astype(int) \n",
    "# confusing matrix\n",
    "results = confusion_matrix(y_test, y_pred_class2) \n",
    "print('Confusion Matrix :', results)\n",
    "print('Data Value :')\n",
    "print('True Positif Value (TP):', results[1,1])\n",
    "print('True Negative Value (TN):', results[0,0])\n",
    "print('False Positive Value (FP):', results[0,1])\n",
    "print('False Negative Value (FN):', results[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy:\", accuracy_score(y_test, y_pred_class2).round(2))\n",
    "print(\"precision:\", precision_score(y_test, y_pred_class2).round(2))\n",
    "print(\"recall:\", recall_score(y_test, y_pred_class2).round(2))\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred_class2).round(2))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
