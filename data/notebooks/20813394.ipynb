{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787aace3",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b095b8",
   "metadata": {},
   "source": [
    "![image.png](attachment:811e5ccd-00dc-4b7c-bdf0-70dbbd8d3c3f.png)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519bb49",
   "metadata": {},
   "source": [
    " **Sentiment Analysis is the process of computationally identifying and categorizing opinions from a piece of text and determining whether attitude towards a product/topic is positive , negative or neutral. Sentiment Analysis is one of the main applications of Natural Language processing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a50b3",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b779298",
   "metadata": {},
   "source": [
    "**In this kernel, let's go through the basic steps in creating a basic sentiment analysis system where it will predict whether the news is positive or negative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cc875a",
   "metadata": {},
   "source": [
    "Steps followed\n",
    "\n",
    "* Importing Dataset and required libraries\n",
    "* Text Data pre-processing (Cleaning)\n",
    "* One Hot representation (converting words to vector)\n",
    "* Padding sequence and embedding layer\n",
    "* Creating LSTM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3160d43",
   "metadata": {},
   "source": [
    "# A] Importing Dataset and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../input/covid19-india-news-headlines-for-nlp/raw_data.csv',usecols=['Headline','Sentiment','Description'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7feaf2",
   "metadata": {},
   "source": [
    "**Here the positive sentiment is represented by '1' and negative sentiment by '0'. Sentiment is our target variable and other variables as independent(X) variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd0ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77433f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492b2b1",
   "metadata": {},
   "source": [
    "# B] Text Data pre-processing and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7816a9d",
   "metadata": {},
   "source": [
    "* 1)The first step is to remove the Stop words like 'the' , 'of' , 'is' , 'a' from every sentence as they do not contribute to the algorithm performance and then perform Stemming on each word where we remove the suffix of the word and reduce it to it's root word. eg.) history-histori\n",
    "\n",
    "* 2) We do stemming on the corpus of text to bring uniformity as there are many words which have same meaning with different suffixes. So we reduce such similar words to one root word. Stemming words at times makes no sense as the words it produces may not have dictionary meaning. So, we also use Lemmatization at times because it gives dictionary meaning of words. Stemming is used in Sentiment Analysis and Lemmatization is used in Q/A applications, chatbots.\n",
    "\n",
    "* 3) We also convert all the words to similar cases so that two similar words with different cases won't be treated distinct.\n",
    "\n",
    "* 4) We use NLTK library, regular expressions library, PorterStemmer(for stemming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec6a70",
   "metadata": {},
   "source": [
    "![image.png](attachment:566b8987-0076-4de5-85ee-74e6f5a35e32.png)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f9ef80",
   "metadata": {},
   "source": [
    "**First lets define the independent and dependent variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('Sentiment',axis=1)\n",
    "y=df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d693b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac796827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "ps=PorterStemmer()\n",
    "corpus=[]\n",
    "for i in range(len(messages)):\n",
    "    review=re.sub('[^a-zA-Z]',' ',messages['Headline'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb179c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75675e2f",
   "metadata": {},
   "source": [
    "# C] One Hot Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae95f5",
   "metadata": {},
   "source": [
    "One of the most important step in NLP is to convert text data to vector so that it generalizes well to the predictions . One of the word representation technique is One Hot Representation where we assign index to the words based on a vocabulary size . https://medium.com/p/6b633f296337/edit You can refer my mediuma article for intuition ,\n",
    "\n",
    "One of the disadvantage is that semantic information is not captured and size is also huge . It won't treat 'good' and 'great' similar ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size=5000\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_repr=[one_hot(words,voc_size) for words in corpus]\n",
    "onehot_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7dafc2",
   "metadata": {},
   "source": [
    "# D] Padding sequence and embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1341477",
   "metadata": {},
   "source": [
    "**Before passing One Hot representation to the embedding layer , we need to make sure that all the length of the sentences are equal . If it is not the same , we apply pre padding with zeroes to make the lengths equal by first defining a sentence length .**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39def112",
   "metadata": {},
   "source": [
    "![image.png](attachment:feb16b8e-91ab-458e-a176-ef69abb21a09.png)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length=20\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a23a6a",
   "metadata": {},
   "source": [
    "# E] LSTM Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b9821",
   "metadata": {},
   "source": [
    "![image.png](attachment:4c83009a-953c-440b-aed8-d3e03f89ebdb.png)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "embedding_vector_features=40\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_final=np.array(embedded_docs)\n",
    "y_final=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_final,y_final,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ecbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d11ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict_classes(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
