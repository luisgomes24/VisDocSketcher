{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a75304",
   "metadata": {},
   "source": [
    "# NBME / Fine-tuning DeBERTa | TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc95579",
   "metadata": {},
   "source": [
    "\n",
    "This notebook contains complete code to fine-tune **DeBERTa** to develop an automated way of identifying the relevant features within each patient note on dataset of  text data presented from the USMLE® Step 2 Clinical Skills examination, a medical licensure exam. In addition to training a model using  **Tensorflow**, you will learn how to preprocess text into an appropriate format.\n",
    "\n",
    "\n",
    "**In this notebook, you will:**\n",
    "* have a quick overview at Transformers and DeBERTa model.\n",
    "* Load dataset.\n",
    "* Load Tokenizer and prepare the data.\n",
    "* Load  DeBERTa and build your own model.\n",
    "* Train your own model, fine-tuning DeBERTa as part of that\n",
    "* Save your model and use it.\n",
    "\n",
    "\n",
    "## Transformers\n",
    "Transformer models are used to solve all kinds of  NLP (**N**atural **L**anguage **P**rocessing ) tasks, and The Transformers library provides the functionality to create and use those models.The Transformer architecture was introduced in 2017 and the original research  focused  on translation tasks, after that several models were  introducted such as (GPT, BERT,GPT-2,DistilBERT, BART, and  GPT-3) [[Ref]](https://huggingface.co/course/chapter1/4?fw=tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5320759",
   "metadata": {},
   "source": [
    "## DeBERTa Model\n",
    "**DeBERTa** (**D**ecoding-**e**nhanced **BERT** with disentangled **a**ttention) improves the BERT and RoBERTa models using two novel techniques, the first is disentangled attention mechanism and the Second is enhanced mask decoder [[Ref]](https://www.microsoft.com/en-us/research/publication/deberta-decoding-enhanced-bert-with-disentangled-attention-2/).\n",
    "\n",
    "DeBERTa outperforms BERT and RoBERTa on majority of NLU(**N**atural **L**anguage **U**nderstanding) tasks with 80GB training data [[Ref]](https://github.com/microsoft/DeBERTa).\n",
    "\n",
    "![Capture.JPG](attachment:8533a015-835d-4af6-8c8a-18e0cffa4a22.JPG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d38190",
   "metadata": {},
   "source": [
    "## Transformers Pipeline\n",
    "All NLP tasks use Transformers pipeline that contains three  steps.\n",
    "![Capture.JPG](attachment:a629d232-a81d-4ea0-820f-7b9e3181fa25.JPG)\n",
    "\n",
    "### 1- Tokenizer (Preprocessing):\n",
    "Transformer models can’t process raw text(Like other neural networks) directly, we have  to convert the text inputs into numbers. we use a tokenizer to do that. All preprocessing need to done as same way the model pretrained, so we use the ***AutoTokenizer*** class and its ***from_pretrained()*** method with the checkpoint name of our model. After using the checkpoint of our model the tokenizer of model will automatically be downloaded.\n",
    "\n",
    "### 2- Model:\n",
    "Tranformers provides ***TFAutoModel*** class that also has a ***from_pretrained()*** method to download pretrained model in same way we do for tokenizer.\n",
    "\n",
    "### 3- Postprocessing:\n",
    "The outputs we get from our model don't make sense, they are logits (not probabilities), to convert them to probabilities we need to pass them through a Softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe1297",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cca152",
   "metadata": {},
   "source": [
    "***AutoConfig*** is a  configuration class that will be instantiated as one of the configuration \n",
    "classes of the library when created with the ***from_pretrained()*** class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de513b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import ast\n",
    "import spacy\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, TFAutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e324c6",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4226b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Model ---------- \n",
    "MODEL_NAME = 'microsoft/deberta-base'\n",
    "TOKENIZER_PATH = f'{MODEL_NAME}_tokenizer'\n",
    "MAX_LEN = 512\n",
    "\n",
    "# ---------- Training ----------\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "CLIP_NORM = 1000\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "seed=42\n",
    "n_fold=5\n",
    "fold=4\n",
    "\n",
    "debug=False\n",
    "if debug:\n",
    "    EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa8ff6",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1cdca",
   "metadata": {},
   "source": [
    "### train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval) # Construct an object from a string\n",
    "train['location'] = train['location'].apply(ast.literal_eval) # Construct an object from a string\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ac5e8",
   "metadata": {},
   "source": [
    "### features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f9a85",
   "metadata": {},
   "source": [
    "### patient_notes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cbac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "patient_notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45b44a",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23336d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train = train.sample(n=500, random_state=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a30af",
   "metadata": {},
   "source": [
    "## Annotations Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1#5910\n",
    "\n",
    "locations = train.loc[idx,'location']\n",
    "pn_history= train.loc[idx,'pn_history']\n",
    "\n",
    "start_pos = []\n",
    "end_pos = []\n",
    "\n",
    "for location in locations:\n",
    "    for loc in [s.split() for s in location.split(';')]:\n",
    "        start_pos.append(int(loc[0]))\n",
    "        end_pos.append(int(loc[1]))\n",
    "\n",
    "\n",
    "ents = []\n",
    "for i in range(len(start_pos)):\n",
    "    ents.append({\n",
    "        'start': int(start_pos[i]), \n",
    "        'end' : int(end_pos[i]),\n",
    "        \"label\" : \"Annotation\"\n",
    "    })\n",
    "doc = {\n",
    "    'text' : pn_history,\n",
    "    \"ents\" : ents\n",
    "}\n",
    "\n",
    "colors = {\"Annotation\": \"linear-gradient(0deg, #888, #eeaaaa)\"} \n",
    "options = {\"colors\": colors}\n",
    "spacy.displacy.render(doc, style=\"ent\", options=options , manual=True, jupyter=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769fe4de",
   "metadata": {},
   "source": [
    "## CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fold = GroupKFold(n_splits=n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['annotation_length'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "print(train.groupby('fold').size())\n",
    "\n",
    "valid=train[train.fold==fold]\n",
    "train=train[train.fold!=fold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149526da",
   "metadata": {},
   "source": [
    "train, valid = train_test_split(\n",
    "    train[['pn_history', 'feature_text','annotation_length', 'location']],\n",
    "    test_size=0.25,\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf6090",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb850399",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.save_pretrained(f'{TOKENIZER_PATH}')\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.save_pretrained(f'{TOKENIZER_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a8445",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- prepare_location ------------------------------\n",
    "def prepare_location(locations: str) -> List[Tuple[int]]:\n",
    "    \"\"\"\n",
    "    This function returns list of tuples of locations\n",
    "    \"\"\"\n",
    "    location_tuple_list = []\n",
    "    for location in locations:\n",
    "        for loc in [s.split() for s in location.split(';')]:\n",
    "            start, end = int(loc[0]), int(loc[1])\n",
    "            location_tuple_list.append((start, end))\n",
    "    \n",
    "    return location_tuple_list\n",
    "# ------------------------- prepare_input ------------------------------\n",
    "def prepare_input(pn_history: str, feature_text: str):\n",
    "    \"\"\"\n",
    "    This function tokenizes pn_history and feature text and\n",
    "    returns numpy array of input_ids and attention_masks\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(\n",
    "        pn_history,\n",
    "        feature_text,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "    return np.array(input_ids), np.array(attention_mask)\n",
    "# ------------------------- prepare_labels ------------------------------\n",
    "# Thanks yasufuminakama \n",
    "# https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train\n",
    "def prepare_labels(pn_history, annotation_length, location_list):\n",
    "    \"\"\"\n",
    "    This function creates labels with are vectors of zeros (no entity)\n",
    "    and ones (entity)\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        pn_history,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    offset_mapping = tokenized[\"offset_mapping\"]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    if annotation_length != 0:\n",
    "        locations = prepare_location(location_list)\n",
    "        for location in locations:\n",
    "            start_idx, end_idx = -1, -1\n",
    "            start, end = location\n",
    "            for idx in range(len(offset_mapping)):\n",
    "                if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                    start_idx = idx - 1\n",
    "                if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                    end_idx = idx + 1\n",
    "            if start_idx == -1:\n",
    "                start_idx = end_idx\n",
    "            if (start_idx != -1) & (end_idx != -1):\n",
    "                label[start_idx:end_idx] = 1\n",
    "            \n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22173832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(dataframe: pd.DataFrame,train=True):\n",
    "    pn_history = dataframe[\"pn_history\"].values\n",
    "    feature_text = dataframe[\"feature_text\"].values\n",
    "    if train:\n",
    "        annotation_length = dataframe['annotation_length'].values\n",
    "        location = dataframe['location'].values\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        inputs, masks = prepare_input(pn_history[i], feature_text[i])\n",
    "        input_ids.append(inputs)\n",
    "        attention_mask.append(masks)\n",
    "        if train:\n",
    "            lbls = prepare_labels(pn_history[i], annotation_length[i], location[i])\n",
    "            labels.append(lbls)\n",
    "    return {\"input_ids\":input_ids,\"attention_mask\":attention_mask}, labels\n",
    "    \n",
    "train_data, train_labels = create_data(train,train=True)\n",
    "val_data, val_labels = create_data(valid,train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6e269",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3399e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X =(np.asarray(train_data['input_ids']),np.asarray(train_data['attention_mask']))          \n",
    "train_data_Y =np.asarray(train_labels).ravel()\n",
    "\n",
    "valid_data = ((np.asarray(val_data['input_ids']),\n",
    "              np.asarray(val_data['attention_mask']),\n",
    "             ),np.asarray(val_labels).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64405f89",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    learning_rate = LEARNING_RATE\n",
    "    if epoch == 0:\n",
    "        return learning_rate * 1\n",
    "    else:\n",
    "        return learning_rate * (0.7**epoch)\n",
    "\n",
    "plt.plot([scheduler(e) for e in range(EPOCHS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "model_save = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f'./model_deberta_fold{fold}.h5', \n",
    "    save_best_only = True, \n",
    "    save_weights_only = True,\n",
    "    monitor = 'val_loss', \n",
    "    mode = 'min', verbose = 1\n",
    ")\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=1e-5, \n",
    "    patience=5, \n",
    "    verbose=1,\n",
    "    mode='auto', \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=2, \n",
    "    mode='auto', \n",
    "    min_delta=0.001,\n",
    "    verbose = 1\n",
    ")\n",
    "#reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c92d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.f1 = tfa.metrics.F1Score(num_classes=2, average='micro', threshold=0.50)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.reshape(y_true, (-1,MAX_LEN))\n",
    "        y_pred = tf.reshape(y_pred, (-1,MAX_LEN))\n",
    "        self.f1.update_state(y_true, y_pred)\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.f1.reset_state()\n",
    "    \n",
    "    def result(self):\n",
    "        return self.f1.result()\n",
    "    \n",
    "metrics = [\n",
    "    F1Score()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_model\n",
    "def create_model() -> tf.keras.Model:\n",
    "    input_tokens = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
    "    attention_mask = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, output_hidden_states=True)\n",
    "    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n",
    "    #backbone = TFAutoModel.from_pretrained(f'{MODEL_NAME})\n",
    "    backbone.save_pretrained(f'{MODEL_NAME}/model')\n",
    "\n",
    "    out = backbone(input_tokens, attention_mask=attention_mask)[0]\n",
    "    out = tf.keras.layers.Dropout(0.2)(out)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "    return tf.keras.Model(inputs=[input_tokens, attention_mask], outputs=out)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, clipnorm=CLIP_NORM)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(reduction=\"none\")\n",
    "model.compile(optimizer=optimizer,loss=loss,metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd22b4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X =(np.asarray(train_data['input_ids']),np.asarray(train_data['attention_mask']))          \n",
    "train_data_Y =np.asarray(train_labels)\n",
    "\n",
    "valid_data = ((np.asarray(val_data['input_ids']),\n",
    "              np.asarray(val_data['attention_mask']),\n",
    "             ),np.asarray(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daee8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=np.asarray(train_data['input_ids'])\n",
    "print(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d513ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(\n",
    "            train_data_X,\n",
    "            train_data_Y, \n",
    "            epochs = EPOCHS,\n",
    "            shuffle=True,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            validation_data= valid_data,\n",
    "            callbacks = [model_save, early_stop, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e17247",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(history.epoch[-1]+1),history.history['val_loss'],label='val_loss')\n",
    "plt.plot(range(history.epoch[-1]+1),history.history['loss'],label='loss')\n",
    "plt.title('loss'); plt.xlabel('Epoch'); plt.ylabel('loss');plt.legend(); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8338b0f",
   "metadata": {},
   "source": [
    "## F1 curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373be552",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(history.epoch[-1]+1),history.history['val_f1'],label='val_f1')\n",
    "plt.plot(range(history.epoch[-1]+1),history.history['f1'],label='f1')\n",
    "plt.title('F1'); plt.xlabel('Epoch'); plt.ylabel('f1');plt.legend(); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8bbfb",
   "metadata": {},
   "source": [
    "## OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = model.predict((np.asarray(val_data['input_ids']),\n",
    "                            np.asarray(val_data['attention_mask']),))\n",
    "preds_valid = preds_valid.reshape(len(valid), MAX_LEN)\n",
    "valid[[i for i in range(MAX_LEN)]] = preds_valid\n",
    "print(valid.shape) \n",
    "valid.to_pickle(f'./oof_fold{fold}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be012d",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11261c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\n",
    "test = test.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "test = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40303504",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff69b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = create_data(test,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabef5c",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57631c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks yasufuminakama \n",
    "# https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict((np.asarray(test_data['input_ids']),\n",
    "                            np.asarray(test_data['attention_mask']),))\n",
    "preds = preds.reshape(len(test), MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_probs = get_char_probs(test['pn_history'].values, preds, tokenizer)\n",
    "results = get_results(char_probs, th=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6277e",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\n",
    "submission['location'] = results\n",
    "display(submission.head())\n",
    "submission[['id', 'location']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bf883",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe82911",
   "metadata": {},
   "source": [
    "1. https://huggingface.co/course/chapter1/4?fw=tf\n",
    "1. https://www.microsoft.com/en-us/research/publication/deberta-decoding-enhanced-bert-with-disentangled-attention-2/\n",
    "1. https://github.com/microsoft/DeBERTa\n",
    "1. https://huggingface.co/course/chapter1/1\n",
    "1. https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train\n",
    "1. https://www.kaggle.com/huchlatymon/nbme-eda-deberta-train-cv-0-85\n",
    "1. https://colab.research.google.com/drive/1pH9NKhAHT40ygOOf4bwld8j221r9SRq_?usp=sharing#scrollTo=SzCwE_mHkPxj\n",
    "1. https://tensorexamples.com/2020/07/27/Using-the-tf.data.Dataset.html"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
