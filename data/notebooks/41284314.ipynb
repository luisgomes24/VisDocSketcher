{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aea339c",
   "metadata": {},
   "source": [
    "\n",
    "200 examples in the public dataset leaves very little room for training!\n",
    "\n",
    "Using `gpt-3.5-turbo` I created another 500 high quality examples at my expense [that I share freely here](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam). They are what (as of now) pushes this notebook to the highest achieved score across the public notebooks (`0.723`)!\n",
    "\n",
    "If you find the additional training examples useful, please upvote the dataset! ğŸ˜Š\n",
    "\n",
    "ğŸ‘‰ [additional train data for LLM Science Exam ğŸ¥³](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam)\n",
    "\n",
    "Thank you! Appreciate your help! ğŸ™ğŸ™ğŸ™\n",
    "\n",
    "This notebook builds on a [notebook](https://www.kaggle.com/code/leonidkulyk/lb-0-709-llm-se-deberta-v3-large-i-1k-wiki) by LEONID KULYK. Among some of the changes are:\n",
    "\n",
    "* use of a new high quality dataset for training\n",
    "* modified training procedure carried out in the notebook\n",
    "* general streamlining of code/training for readability\n",
    "\n",
    "**A couple of related resources you might find useful:**\n",
    "\n",
    "* [ğŸ“Š 15k high-quality train examples ğŸ†ğŸ”¥ğŸš€](https://www.kaggle.com/datasets/radek1/15k-high-quality-examples) - another 15 000 examples I created to help you grow that train/validation set of yours and improve results\n",
    "* [ğŸ“Š Best Open Source LLM Starter Pack ğŸ§™ğŸš€](https://www.kaggle.com/datasets/radek1/best-llm-starter-pack) -- the largest (and best) open source model I managed to run on Kaggle!\n",
    "* [Science Exam Trained Model Weights ğŸš€](https://www.kaggle.com/datasets/radek1/science-exam-trained-model-weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed795f",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer, AutoModel\n",
    "\n",
    "deberta_v3_large = '/kaggle/input/deberta-v3-large-hf-weights'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae2f15",
   "metadata": {},
   "source": [
    "We begin by loading and processing the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3126a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/train.csv')\n",
    "df_train = df_train.drop(columns=\"id\")\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46966250",
   "metadata": {},
   "source": [
    "Let's add another 500 examples to the train set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([\n",
    "    df_train,\n",
    "    pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/extra_train_set.csv'),\n",
    "])\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218f4cf",
   "metadata": {},
   "source": [
    "Now that we have gone from 200 -> 700 train examples, let us preprocess the data and begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77001bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentences = [example[option] for option in 'ABCDE']\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=False)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b44075",
   "metadata": {},
   "source": [
    "We first create a HuggingFace `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a14f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(deberta_v3_large)\n",
    "\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c214a3",
   "metadata": {},
   "source": [
    "And let us now preprocess the examples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd83c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00cbe0e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999639a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=0.8,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    report_to='none',\n",
    "    output_dir='.',\n",
    ")\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(deberta_v3_large)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f2fbe",
   "metadata": {},
   "source": [
    "# Predicting on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b15eb",
   "metadata": {},
   "source": [
    "Now that we have trained our model, let us predict on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')\n",
    "test_df['answer'] = 'A' # dummy answer that allows us to preprocess the test dataset just like we preprocessed the train dataset\n",
    "\n",
    "tokenized_test_dataset = Dataset.from_pandas(test_df.drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = trainer.predict(tokenized_test_dataset).predictions\n",
    "test_predictions[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a5a57",
   "metadata": {},
   "source": [
    "The predictions are values output from the last layer of our neural network.\n",
    "\n",
    "Let's obtain the predicted answer ids by sorting them from largest to the smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_ids[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19033f96",
   "metadata": {},
   "source": [
    "Let us now assign a letter corresponding to each predicted id (0 -> 'A', 1 -> 'B', etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "predictions_as_answer_letters[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd352747",
   "metadata": {},
   "source": [
    "And let us now go from this representation to outputting a string with 3 highest rated answers seperated by a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e737c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_as_string = test_df['prediction'] = [\n",
    "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "]\n",
    "predictions_as_string[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf9a7e",
   "metadata": {},
   "source": [
    "And we are done! ğŸ¥³\n",
    "\n",
    "Let us now output our submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93996052",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_df[['id', 'prediction']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "pd.read_csv('submission.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6adfe",
   "metadata": {},
   "source": [
    "I hope you enjoyed this notebook!\n",
    "\n",
    "If you found it useful, please upvote ğŸ‘‰ [the corresponding dataset with 500 new training examples](https://www.kaggle.com/datasets/radek1/additional-train-data-for-llm-science-exam) ğŸ‘ˆ\n",
    "\n",
    "Thank you, appreciate your help! ğŸ™ğŸ˜Š\n",
    "\n",
    "Thank you for reading and happy Kaggling!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
