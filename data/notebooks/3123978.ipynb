{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfd5da8",
   "metadata": {},
   "source": [
    "**A better baseline (Code repository: https://github.com/sattree/gpr_pub)**\n",
    "===\n",
    "\n",
    "In my previous kernel, namely 'Reproducing GAP results', I had briefly mentioned a Confidence Model. While we can train our models on the GAP domain to get better results, the burning question remains: **Can we do better** with existing resources and leverage Domain Adaptation techniques to establish a good baseline, perhaps even a strong baseline - a baseline stemming from existing work done on same or similar problems?\n",
    "\n",
    "To demonstrate the strength of existing coref models and establish a domain transfer baseline, we **must not access labels from the GAP dataset**. This kernel investigates and demonstrates just that, **without any training on the GAP dataset**.\n",
    "\n",
    "***\n",
    "\n",
    "This is the concluding kernel in this tri-series of self-contained installments to introduce the GPR problem.\n",
    "1. Coref visualization - https://www.kaggle.com/sattree/1-coref-visualization-jupyter-allenlp-stanford\n",
    "1. Reproducing GAP results - lays the foundation for this work - https://www.kaggle.com/sattree/2-reproducing-gap-results\n",
    "1. **A better baseline - without any training**\n",
    "\n",
    "The notebook only presents results and demonstrates model usage to avoid flooding it with code statements. The model codes are available at https://github.com/sattree/gpr_pub. Hope you will find the code structure easily navigable.\n",
    "***\n",
    "**Executive Summary**\n",
    "---\n",
    "\n",
    "The primary contribution of this kernel is the design of a Confidence Model allowing existing pre-trained solutions to be applied to this problem.\n",
    "\n",
    "**Confidence Model by Bootstrapping**\n",
    "\n",
    "Given a set of m classifiers and their predicted labels, we can estimate the probability of a sample belonging to a particular class as follows:\n",
    "\n",
    "$$P(\\frac{y=A}{C_1,C_2,....,C_m}) = \\frac{P(\\frac{C_1,C_2,....,C_m}{y=A}) P(y=A)}{\\sum_{k={A, B, NEITHER}} P(\\frac{C_1,C_2,...,C_m}{y=k}) P(y=k)}$$\n",
    "\n",
    "$$P(\\frac{C_1,C_2,....,C_m}{y=A}) = P(\\frac{C_1}{y=A})P(\\frac{C_2}{C_1,y=A}) ... P(\\frac{C_{m}}{C_1,C_2,...,C_{m-1},y=A})   $$\n",
    "\n",
    "Barring sparsity issues, the probabilities in RHS above can be easily estimated from labeled training data. However, since we do not have access to true labels, we will take a bootstrapping approach by identifying proxy labels. We will introduce a bias that when the majority of a rich set of models agree on a label, then it must be a good proxy for the true label with high probability. This will seed our bootstrapping solution, albeit a noisy one.\n",
    "\n",
    "In what follows, the above problem is formulated as a density estimation problem and an estimation procedure is described.\n",
    "\n",
    "1. **Generate predictions from constituent models**: Drifting from our approach in the previous kernel, we will allow each statistical model to assign multiple labels to a sample (multi-label classfication), meaning that a model can assign a pronoun to both 'A' and 'B', even though we know apriori that a given pronoun in the text will only resolve to either 'A' or 'B' (if at all) but not both. This is important from the perspective of confidence modelling, since this indicates that there is evidence that the pronoun refers to one of the two mentions, but not enough to know which one. This is in contrast to the case where there is evidence that the pronoun does not refer to either. We would like our confidence model to capture this behavior in the domain.\n",
    "\n",
    "1. **Identify a diverse set of models** (noisyness is not important here): To keep things simple, we will pre-select Lee et al (given that it is state of the art, and leverages most modern developments in lingusitic modelling generalization, such as ELMo embeddings, etc.). The qualitative superiority of this model both in terms of precision as well as recall is also illustrated below. Thereafter, we will perform univariate testing wrt to every other classifier in the bag. Non-parametric mutual information measure is used to estimate the correlation. Given the limited size of data, only rejection of classifiers can be done conclusively - hence the models with a degree of parity with Lee et all and presumably less diversity are rejected.\n",
    "\n",
    "1. **Density estimation on labels** to model the conditional distribution of classes and models: Given the noise in labels, this step becomes tricky.\n",
    "    1. **Choose a simple model** like Logistic Regression to avoid tuning to noise in the labels. LR is also known to produce well calibrated probabilities.\n",
    "    1. **Impose L1 regularization** to encourage classifier selection relevant for each target class, since not every classifier performs equally well for 'A', 'B', and 'NEITHER'. This also helps in weeding out noise in the labels.\n",
    "    1. **Model selection by cross-validation** Since the labels are noisy, we must make sure that the model does not fit perfectly to the data. We select models that give around 95% cross-validation performance as opposed to 100%.\n",
    "    \n",
    "1. **Backoff to non-smoothed labels**: Density estimation from multiple models has a smoothing effect (as can be seen from before and after figures). While it is good to avoid a large negative penalty for misclassification, it also hurts our strong predictions for easy samples to some extent. Hence we will restore the majority labels for such samples that were supposedly 'easy samples' to begin with.\n",
    "\n",
    "A confidence model is built to convert labels of models into probabilties to be compatible with logloss. The underlying coref models include a combination of heuristic and coref models mentioned in the GAP paper and a few others.\n",
    "\n",
    "**Heuristic Models**\n",
    "\n",
    "* Random - not included for confidence modeling as it enforces a prediction on all samples\n",
    "* Token Distance - not included for confidence modeling as it enforces a prediction on all samples\n",
    "* Syntactical Distance\n",
    "* Parallelism\n",
    "* URL\n",
    "\n",
    "**Off the shelf Coref Solvers**\n",
    "* Lee et al (cited in the GAP paper) - end to end coref system, one of the most recent state-of-the art\n",
    "* AllenNLP\n",
    "* Huggingface - neural coref - not included in this kernel due to memory limitations, but the results can be found in the github repository\n",
    "* Stanford (cited in the GAP paper)  - includes three variants\n",
    "    1.     Deterministic coref model\n",
    "    1.     Statistical coref model\n",
    "    1.     Neural coref model\n",
    "* Berkeley Coref System (BCS) - built using a diverse set of interesting linguistic features\n",
    "* Wiseman et al (cited in the GAP paper) - I wanted to include this to leverage its diversity in features but couldn't get it to work.\n",
    "\n",
    "Moving on, below is a summary of the results and findings:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"https://github.com/sattree/gpr_pub/blob/master/docs/baseline4.png?raw=true\" alt=\"\" style=\"width: 85%;\"/>\n",
    "                <div style=\"text-align:center; margin-top: 8px;\">Diversity(parity) of heuristic and pre-trained coref models. Higher parity indicates higher correlation in the predictions and lower diversity.</div>\n",
    "        </td>\n",
    "         <td> <img src=\"https://github.com/sattree/gpr_pub/blob/master/docs/baseline5.png?raw=true\" alt=\"\" style=\"width: 85%;\"/>\n",
    "                <div style=\"text-align:center; margin-top: 8px;\">Class-wise distribution of votes from the shortlisted set of classifiers. Class 'B' seems to be underrepresented.</div>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> <img src=\"https://github.com/sattree/gpr_pub/blob/master/docs/baseline2.png?raw=true\" alt=\"\" style=\"width: 85%;\"/>\n",
    "                <div style=\"text-align:center; margin-top: 8px;\">Class-specific distribution of probabilties estimated by the confidence model. The probabilties seem to have been over-smoothed.</div>\n",
    "        </td>\n",
    "        <td> <img src=\"https://github.com/sattree/gpr_pub/blob/master/docs/baseline3.png?raw=true\" alt=\"\" style=\"width: 85%;\"/>\n",
    "                <div style=\"text-align:center; margin-top: 8px;\">The same distribution after adjustment for highly confident (easy) samples.</div>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "**Further Improvements**\n",
    "\n",
    "As you can see, a very strong baseline for the problem has been established.\n",
    "\n",
    "Straightforward extensions:\n",
    "1. Use the labels from the GAP dataset to improve the confidence model. This alone should give a good boost to the logloss score.\n",
    "2. The current demonstration paves the way for building models by training on the GAP dataset and leveraging insights from existing general coref models. I expect a logloss score of 0.4 to be easily achievable.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e950b04",
   "metadata": {},
   "source": [
    "Download and set up all the required data and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install all dependencies\n",
    "# gpr_pub contains the heuristics models and supplementary code\n",
    "!git clone https://github.com/sattree/gpr_pub.git\n",
    "\n",
    "!pip install allennlp --ignore-installed greenlet\n",
    "\n",
    "# !pip install ../input/neural-coref/en_coref_lg-3.0.0/en_coref_lg-3.0.0/\n",
    "# Huggingface neuralcoref model has issues with spacy-2.0.18\n",
    "# !conda install -y cymem==1.31.2 spacy==2.0.12\n",
    "\n",
    "!pip install attrdict pyhocon\n",
    "\n",
    "!wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
    "!unzip stanford-corenlp-full-2018-10-05.zip\n",
    "!rm stanford-corenlp-full-2018-10-05.zip\n",
    "\n",
    "# setup berkeley coref system\n",
    "!git clone https://github.com/gregdurrett/berkeley-entity.git\n",
    "!curl -s http://nlp.cs.berkeley.edu/downloads/berkeley-entity-models.tgz | tar xvz -C berkeley-entity\n",
    "!mkdir berkeley-entity/data\n",
    "!wget http://www.cs.utexas.edu/~gdurrett/data/gender.data.tgz\n",
    "!tar -xvf gender.data.tgz\n",
    "!mv gender.data berkeley-entity/data/\n",
    "!rm gender.data.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "# Add css styles and js events to DOM, so that they are available to rendered html\n",
    "display(HTML(open('gpr_pub/visualization/highlight.css').read()))\n",
    "display(HTML(open('gpr_pub/visualization/highlight.js').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680784e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for Lee et al coref model\n",
    "# details can be found here https://github.com/kentonl/e2e-coref\n",
    "\n",
    "import tensorflow as tf\n",
    "TF_CFLAGS = \" \".join(tf.sysconfig.get_compile_flags())\n",
    "TF_LFLAGS = \" \".join(tf.sysconfig.get_link_flags())\n",
    "\n",
    "# Linux (build from source)\n",
    "!g++ -std=c++11 -shared gpr_pub/modified_e2e_coref/coref_kernels.cc -o coref_kernels.so -fPIC $TF_CFLAGS $TF_LFLAGS -O2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27083188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyhocon\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "from attrdict import AttrDict\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mutual_info_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import classification_report, log_loss\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp.models.archival import load_archive\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'gpr_pub/modified_e2e_coref/')\n",
    "sys.path.insert(0, 'gpr_pub/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpr_pub.models.coref import Coref\n",
    "from gpr_pub.models.heuristics.stanford_base import StanfordModel\n",
    "from gpr_pub.models.heuristics.spacy_base import SpacyModel\n",
    "\n",
    "# Heuristics models implement coref resolution based on heuristics described in the paper\n",
    "# Pronoun resolution is a simple wrapper to convert coref predictions into class-specific labels\n",
    "from gpr_pub.models.heuristics.random_distance import RandomModel\n",
    "from gpr_pub.models.heuristics.token_distance import TokenDistanceModel\n",
    "from gpr_pub.models.heuristics.syntactic_distance import StanfordSyntacticDistanceModel\n",
    "from gpr_pub.models.heuristics.parallelism import AllenNLPParallelismModel as ParallelismModel\n",
    "from gpr_pub.models.heuristics.url_title import StanfordURLTitleModel as URLModel\n",
    "\n",
    "from gpr_pub.models.pretrained.lee_et_al import LeeEtAl2017\n",
    "from gpr_pub.models.pretrained.stanford import StanfordCorefModel\n",
    "from gpr_pub.models.pretrained.allennlp import AllenNLPCorefModel\n",
    "from gpr_pub.models.pretrained.huggingface import HuggingfaceCorefModel\n",
    "from gpr_pub.models.pretrained.berkley_coref_system import BCS\n",
    "\n",
    "from gpr_pub.models.pronoun_resolution import PronounResolutionModel, PronounResolutionModelV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27083a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpr_pub import visualization\n",
    "from gpr_pub.utils import CoreNLPServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cafc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_MODEL = spacy.load('en_core_web_lg')\n",
    "\n",
    "STANFORD_CORENLP_PATH = 'stanford-corenlp-full-2018-10-05/'\n",
    "server = CoreNLPServer(classpath=STANFORD_CORENLP_PATH,\n",
    "                        corenlp_options=AttrDict({'port': 9090, \n",
    "                                                  'timeout': '600000',\n",
    "                                                  'thread': '2',\n",
    "                                                  'quiet': 'true',\n",
    "                                                  'preload': 'tokenize,ssplit,pos,lemma,parse,depparse,ner,coref'}))\n",
    "server.start()\n",
    "STANFORD_SERVER_URL = server.url\n",
    "STANFORD_MODEL = CoreNLPParser(url=STANFORD_SERVER_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99640e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 'https://s3-us-west-2.amazonaws.com/allennlp/models/coref-model-2018.02.05.tar.gz'\n",
    "archive = load_archive(model_url, cuda_device=0)\n",
    "ALLEN_COREF_MODEL = Predictor.from_archive(archive)\n",
    "\n",
    "model_url = 'https://s3-us-west-2.amazonaws.com/allennlp/models/biaffine-dependency-parser-ptb-2018.08.23.tar.gz'\n",
    "archive = load_archive(model_url, cuda_device=0)\n",
    "ALLEN_DEP_MODEL = Predictor.from_archive(archive)\n",
    "\n",
    "# HUGGINGFACE_COREF_MODEL = en_coref_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2c6ad",
   "metadata": {},
   "source": [
    "**Load test data**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567681f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/gendered-pronoun-resolution/test_stage_1.tsv', sep='\\t')\n",
    "\n",
    "# normalizing column names\n",
    "test.columns = map(lambda x: x.lower().replace('-', '_'), test.columns)\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_colwidth', 15):\n",
    "    display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a141585",
   "metadata": {},
   "outputs": [],
   "source": [
    "lee_coref_model = LeeEtAl2017(SPACY_MODEL, config = {'name': 'final',\n",
    "                                                     'log_root': '../input/e2e-coref-data/',\n",
    "                                                    'model': 'gpr_pub/modified_e2e_coref/experiments.conf',\n",
    "                                                    'context_embeddings_root': '../input/e2e-coref-data/',\n",
    "                                                    'head_embeddings_root': '../input/e2e-coref-data/',\n",
    "                                                    'char_vocab_root': '../input/e2e-coref-data/'\n",
    "                                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af37cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test.loc[17]\n",
    "data = ALLEN_COREF_MODEL.predict(sample.text)\n",
    "print('{:-<100}'.format('Example where Coref resolves to both gold mentions: Pronoun={}, A={}, B={}'.format(sample.pronoun, sample.a, sample.b)))\n",
    "visualization.render(data, allen=True, jupyter=True)\n",
    "\n",
    "sample = test.loc[13]\n",
    "data = ALLEN_COREF_MODEL.predict(sample.text)\n",
    "print('{:-<100}'.format('Example where a single antecedent contains both gold mentions: Pronoun={}, A={}, B={}'.format(sample.pronoun, sample.a, sample.b)))\n",
    "visualization.render(data, allen=True, jupyter=True)\n",
    "\n",
    "sample = test.loc[13]\n",
    "data = lee_coref_model.predict(**sample)\n",
    "print('{:-<100}'.format('Example to compare the performance of Lee et al: Pronoun={}, A={}, B={}'.format(sample.pronoun, sample.a, sample.b)))\n",
    "visualization.render({'document': data[0], 'clusters': data[1]}, allen=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917847e7",
   "metadata": {},
   "source": [
    "**Setup models and generate predictions**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85868f8",
   "metadata": {},
   "source": [
    "Heuristic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_coref_model = RandomModel(SPACY_MODEL)\n",
    "random_proref_model = PronounResolutionModel(random_coref_model)\n",
    "\n",
    "token_distance_coref_model = TokenDistanceModel(SPACY_MODEL)\n",
    "token_distance_proref_model = PronounResolutionModel(token_distance_coref_model)\n",
    "\n",
    "syntactic_distance_coref_model = StanfordSyntacticDistanceModel(STANFORD_MODEL)\n",
    "syntactic_distance_proref_model = PronounResolutionModel(syntactic_distance_coref_model, n_jobs=1)\n",
    "\n",
    "parallelism_coref_model = ParallelismModel(ALLEN_DEP_MODEL, SPACY_MODEL)\n",
    "parallelism_proref_model = PronounResolutionModel(parallelism_coref_model)\n",
    "\n",
    "url_title_coref_model = URLModel(STANFORD_MODEL)\n",
    "url_title_proref_model = PronounResolutionModel(url_title_coref_model, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = random_proref_model.predict(test)\n",
    "# test['random_a_coref'], test['random_b_coref'] = zip(*preds)\n",
    "\n",
    "# preds = token_distance_proref_model.predict(test)\n",
    "# test['token_distance_a_coref'], test['token_distance_b_coref'] = zip(*preds)\n",
    "\n",
    "preds = syntactic_distance_proref_model.predict(test)\n",
    "test['syntactic_distance_a_coref'], test['syntactic_distance_b_coref'] = zip(*preds)\n",
    "\n",
    "preds = parallelism_proref_model.predict(test)\n",
    "test['parallelism_a_coref'], test['parallelism_b_coref'] = zip(*preds)\n",
    "\n",
    "preds = url_title_proref_model.predict(test)\n",
    "test['parallelism_url_a_coref'], test['parallelism_url_b_coref'] = zip(*preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cea91",
   "metadata": {},
   "source": [
    "Pre-trained coref model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdc2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_coref_model = StanfordCorefModel(STANFORD_MODEL, algo='clustering')\n",
    "deterministic_stanford_proref_model = PronounResolutionModelV2(stanford_coref_model, n_jobs=1, multilabel=True)\n",
    "\n",
    "stanford_coref_model = StanfordCorefModel(STANFORD_MODEL, algo='statistical')\n",
    "statistical_stanford_proref_model = PronounResolutionModelV2(stanford_coref_model, n_jobs=1, multilabel=True)\n",
    "\n",
    "stanford_coref_model = StanfordCorefModel(STANFORD_MODEL, algo='neural', greedyness=0.5)\n",
    "neural_stanford_proref_model = PronounResolutionModelV2(stanford_coref_model, n_jobs=1, multilabel=True)\n",
    "\n",
    "allen_coref_model = AllenNLPCorefModel(ALLEN_COREF_MODEL, SPACY_MODEL)\n",
    "allen_proref_model = PronounResolutionModelV2(allen_coref_model, n_jobs=2, multilabel=True)\n",
    "\n",
    "# huggingface_coref_model = HuggingfaceCorefModel(HUGGINGFACE_COREF_MODEL)\n",
    "# hugginface_proref_model = PronounResolutionModelV2(huggingface_coref_model, multilabel=True)\n",
    "\n",
    "lee_coref_model = LeeEtAl2017(SPACY_MODEL, config = {'name': 'final',\n",
    "                                                     'log_root': '../input/e2e-coref-data/',\n",
    "                                                    'model': 'gpr_pub/modified_e2e_coref/experiments.conf',\n",
    "                                                    'context_embeddings_root': '../input/e2e-coref-data/',\n",
    "                                                    'head_embeddings_root': '../input/e2e-coref-data/',\n",
    "                                                    'char_vocab_root': '../input/e2e-coref-data/'\n",
    "                                                    })\n",
    "lee_proref_model = PronounResolutionModelV2(lee_coref_model, multilabel=True)\n",
    "\n",
    "bcs_coref_model = BCS(STANFORD_MODEL)\n",
    "bcs_proref_model = PronounResolutionModelV2(bcs_coref_model, multilabel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0024b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = hugginface_proref_model.predict(test)\n",
    "# test['huggingface_ml_a_coref'], test['huggingface_ml_b_coref'] = zip(*preds)\n",
    "\n",
    "preds = allen_proref_model.predict(test)\n",
    "test['allen_ml_a_coref'], test['allen_ml_b_coref'] = zip(*preds)\n",
    "\n",
    "preds = deterministic_stanford_proref_model.predict(test)\n",
    "test['stanford_ml_deterministic_a_coref'], test['stanford_ml_deterministic_b_coref'] = zip(*preds)\n",
    "\n",
    "preds = statistical_stanford_proref_model.predict(test)\n",
    "test['stanford_ml_statistical_a_coref'], test['stanford_ml_statistical_b_coref'] = zip(*preds)\n",
    "\n",
    "preds = neural_stanford_proref_model.predict(test)\n",
    "test['stanford_ml_neural_a_coref'], test['stanford_ml_neural_b_coref'] = zip(*preds)\n",
    "\n",
    "preds = lee_proref_model.predict(test)\n",
    "test['lee_a_coref'], test['lee_b_coref'] = zip(*preds)\n",
    "\n",
    "preds = bcs_proref_model.predict(test, preprocessor=BCS.preprocess)\n",
    "test['bcs_a_coref'], test['bcs_b_coref'] = zip(*preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebffe4a",
   "metadata": {},
   "source": [
    "**Confidence Model (by bootstrapping)**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b385004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate diversity of the models\n",
    "models = (\n",
    "        'parallelism_url',\n",
    "        'allen_ml', \n",
    "        # 'huggingface_ml', \n",
    "        'parallelism', \n",
    "        'stanford_ml_deterministic', \n",
    "        'syntactic_distance', \n",
    "        'stanford_ml_statistical',\n",
    "        'stanford_ml_neural',\n",
    "        'bcs',\n",
    "        'lee',\n",
    "       )\n",
    "\n",
    "scores = []\n",
    "for model in models[:-1]:\n",
    "    score = mutual_info_score(test['{}_a_coref'.format(model)], test['lee_a_coref'])\n",
    "    score2 = mutual_info_score(test['{}_b_coref'.format(model)], test['lee_b_coref'])\n",
    "    scores.append((model, score, score2))\n",
    "    \n",
    "models = pd.DataFrame(scores, columns=['model', 'score_a', 'score_b']).set_index('model').sort_values('score_b')\n",
    "models['parity(~diversity)'] = models.min(axis=1)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03dbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortlisted diverse set of models\n",
    "models = (\n",
    "        'parallelism_url',\n",
    "        'allen_ml',\n",
    "        'parallelism', \n",
    "        'syntactic_distance', \n",
    "        'stanford_ml_statistical',\n",
    "        'lee',\n",
    "        'bcs'\n",
    "       )\n",
    "\n",
    "models_a = [model+'_a_coref' for model in models]\n",
    "models_b = [model+'_b_coref' for model in models]\n",
    "\n",
    "test['votes_a'] = test[models_a].sum(axis=1)\n",
    "test['votes_b'] = test[models_b].sum(axis=1)\n",
    "test['votes_a_b'] = test[models_a+models_b].sum(axis=1)\n",
    "\n",
    "plt.hist([test['votes_a'], test['votes_b']], label=['Class A', 'Class B'], bins=range(1, len(models)+2))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa9a03",
   "metadata": {},
   "source": [
    "Now, let's invest some time in understanding this figure. Given that we have rejected highly correlated models, the number of votes should be directly reflective of confidence and ease in classification. We will use the data sampled based on voting stength to learn how different models interact with each other, and then transfer that behavior to the remaining samples where model predictions are sparse.\n",
    "\n",
    "As can be seen from the figure above, the model predictions are biased towards samples belonging to class 'A'. Therefore, to balance the sampled set, we will set the threshold for proxy labels as 5 votes and above, and 4 votes and above for classes 'A' and 'B' respectively. We can also see that the category 'NEITHER' continues to elude us. The question of whether there is evidence in the data to support linking of any entity at all is rather an involved one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26138383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define proxy labels based on votes\n",
    "mask_a = test['votes_a'] >=5\n",
    "mask_b = test['votes_b'] >=4\n",
    "mask_a_b = test['votes_a_b'] <= 1\n",
    "\n",
    "true_proxy = test[mask_a | mask_b | mask_a_b]\n",
    "\n",
    "true_proxy['label'] = 2\n",
    "true_proxy.loc[mask_a, 'label'] = 0\n",
    "true_proxy.loc[mask_b, 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad618e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = models_a + models_b\n",
    "\n",
    "X = true_proxy[feats]\n",
    "y = true_proxy['label']\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "clf = LogisticRegression(multi_class='auto', solver='liblinear', penalty='l1', C=.05, max_iter=30)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=StratifiedKFold(3, random_state=21))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52eda75",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)\n",
    "\n",
    "X_tst = test[feats]\n",
    "probabilties = clf.predict_proba(X_tst)\n",
    "\n",
    "plt.hist(probabilties, range=(0,1), label=['A', 'B', 'NEITHER'])\n",
    "plt.legend()\n",
    "plt.title('Class-wise probability distributions from Confidence Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37129781",
   "metadata": {},
   "source": [
    "Judging from the above figure, it appears that the confidence model has over-smoothed examples, specifically class A.\n",
    "In order to avoid over-smoothing really easy samples, we will reinstate the predictions for samples where the top 3 diverse models agree a 100%. Once again, to avoid logloss from going berserk on that 1 sample (there may be more in reality) where the above may not hold, we will set a small lower bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_perfect = ('parallelism_url',\n",
    "        'parallelism', \n",
    "        'lee')\n",
    "\n",
    "models_a = [model+'_a_coref' for model in models_perfect]\n",
    "models_b = [model+'_b_coref' for model in models_perfect]\n",
    "\n",
    "mask_a_perfect = test[models_a].all(axis=1)\n",
    "mask_b_perfect = test[models_b].all(axis=1)\n",
    "print(test[mask_a_perfect].shape, test[mask_b_perfect].shape)\n",
    "\n",
    "# set the lower bound, assuming 1% chance of failure\n",
    "probabilties[mask_a_perfect] = [1,.02,.02]\n",
    "probabilties[mask_b_perfect] = [.01,1,.01]\n",
    "\n",
    "# Softmax of probabilities of joint model to convert them to labels for analysis\n",
    "y_pred = np.zeros_like(probabilties)\n",
    "y_pred[np.arange(len(probabilties)), probabilties.argmax(1)] = 1\n",
    "y_pred = y_pred.astype(bool)\n",
    "\n",
    "plt.hist(probabilties, range=(0, 1), label=['A', 'B', 'NEITHER'])\n",
    "plt.title('Class-wise probability distributions from Confidence Model after adjustment')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7afd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.concat([pd.DataFrame(y_pred, columns=['A', 'B', 'NEITHER']), \n",
    "                 pd.DataFrame(probabilties, columns=['prob_A', 'prob_B', 'prob_NEITHER'])], \n",
    "                axis=1)\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist([res[res['A']]['prob_A'], res[~res['A']]['prob_A']], bins=10, rwidth=0.7, label=['True', 'False'])\n",
    "plt.title('Distribution of probabilties over \\nsamples predicted as class A')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist([res[res['B']]['prob_B'], res[~res['B']]['prob_B']], bins=10, rwidth=0.7, label=['True', 'False'])\n",
    "plt.title('Distribution of probabilties over \\nsamples predicted as class B')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist([res[res['NEITHER']]['prob_NEITHER'], res[~res['NEITHER']]['prob_NEITHER']], bins=10, rwidth=0.7, label=['True', 'False'])\n",
    "plt.title('Distribution of probabilties over \\nsamples predicted as class NEITHER')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3072a",
   "metadata": {},
   "source": [
    "Interesting!\n",
    "\n",
    "- Class A can definitely benefit from more work.\n",
    "- Class B seems to be modeled well, we can see a nice exponential envelope.\n",
    "- Class NEITHER continues to elude us but the distribution looks reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61a2ec",
   "metadata": {},
   "source": [
    "Generate predictions for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../input/gendered-pronoun-resolution/sample_submission_stage_1.csv')\n",
    "sub_df.loc[:, 'A'] = probabilties[:, 0]\n",
    "sub_df.loc[:, 'B'] = probabilties[:, 1]\n",
    "sub_df.loc[:, 'NEITHER'] = probabilties[:, 2]\n",
    "\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20285227",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r stanford-corenlp-full-2018-10-05/\n",
    "!rm -r gpr_pub/\n",
    "!rm -r berkeley-entity/\n",
    "!rm -r tmp/"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
