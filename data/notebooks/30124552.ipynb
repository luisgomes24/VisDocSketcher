{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26399ce2",
   "metadata": {},
   "source": [
    "\n",
    "## One Million Reddit Jokes\n",
    "\n",
    "### Introduction\n",
    "\n",
    "**Overview**\n",
    "The dataset was downloaded as a CSV file containing 1M posts from the r/Jokes subreddit. Of the relevant features, the \"title\" is the title's post or the joke's setup. The \"selftext\" is the punchline, or what you see once a user clicks on the post's content. It's worth nothin that many jokes in this data table don't meet this criterion (nans). \n",
    "\n",
    "**Score**\n",
    "The \"score\" value describes the number of upvotes, i.e. the number of positive ratings the post received. Posts can additionally be downvoted, and while Reddit allows for negative values, the minimum value in the dataset is zero. When a user posts something to Reddit, however, they are automatically given a single upvote, so I am making the assumption that values of zero in this dataset were downvoted. \n",
    "\n",
    "[Original Source](https://query.data.world/s/htrdsouy327xqa4w457qx6k6sjtj6r)\n",
    "\n",
    "### Project Ideas\n",
    "\n",
    "**Exploratory Data Analysis** \n",
    "- Try to understand intuitively \"what makes a joke funny\" using simple exploratory data analysis. \n",
    "\n",
    "**Funny / Not Funny - Classification**\n",
    "- The ultimate goal in wrangling these data is to create a dataset to classify as either funny or not funny using the upvotes. \n",
    "\n",
    "**Jokes Generation**\n",
    "- Train and generate jokes using a language generation model (GPT for example).\n",
    "\n",
    "**Funny Jokes Generation**\n",
    "- Training and generating jokes using language models is one thing but generating **Funny** jokes using language models is a completely different task! (which is much much harder to do)\n",
    "\n",
    "Are you up for a challenge? ;) \n",
    "\n",
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df_one_million_reddit_jokes = pd.read_csv('../input/one-million-reddit-jokes/one-million-reddit-jokes.csv', names = ['type', 'id', 'subreddit.id', 'subreddit.name', 'subreddit.nsfw', 'created_utc', 'permalink', 'domain', 'url', 'selftext', 'title', 'score'])\n",
    "\n",
    "print('Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2e37c",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "- Let's take a look at the average score for each setup (note that in a joke the setup is the title). Which setups have the highest average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# let's check the number of missing values per column\n",
    "df_one_million_reddit_jokes.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3606d",
   "metadata": {},
   "source": [
    "lots of nans in the selftext column, as we'd expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062bd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_million_reddit_jokes = df_one_million_reddit_jokes.loc[df_one_million_reddit_jokes['score'].apply(lambda x: str(x).isdigit())]\n",
    "df_one_million_reddit_jokes['score'] = df_one_million_reddit_jokes['score'].astype(int)\n",
    "df_one_million_reddit_jokes['title'] = df_one_million_reddit_jokes['title'].astype(str)\n",
    "\n",
    "# Get average score per setup\n",
    "setup_score = df_one_million_reddit_jokes.groupby('title').score.mean()\n",
    "setup_score = setup_score.reset_index()\n",
    "\n",
    "# Take a look at the top 25 setups\n",
    "setup_score.sort_values(by = 'score', ascending = False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 1000 rows of the above data\n",
    "plt.figure(figsize = (48,8))\n",
    "sns.barplot(x = 'title', y = 'score', data = setup_score.head(300))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efafe5b1",
   "metadata": {},
   "source": [
    "- What's the average setup length? What's the average setup length for the setups with the highest average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot average setup length\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.distplot(df_one_million_reddit_jokes.title.str.len())\n",
    "plt.show()\n",
    "\n",
    "# Let's get a list of the top 25 setups\n",
    "top_setups = setup_score.sort_values(by = 'score', ascending = False).head(25).title.values\n",
    "\n",
    "# Plot average setup length for the setups with the highest average score\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.distplot(df_one_million_reddit_jokes[df_one_million_reddit_jokes.title.isin(top_setups)].title.str.len())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fef2c0",
   "metadata": {},
   "source": [
    "- What's the average punchline length? What's the average punchline length for the setups with the highest average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd29da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average punchline length\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.distplot(df_one_million_reddit_jokes.selftext.str.len())\n",
    "plt.show()\n",
    "\n",
    "# Plot average punchline length for the setups with the highest average score\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.distplot(df_one_million_reddit_jokes[df_one_million_reddit_jokes.title.isin(top_setups)].selftext.str.len())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9957d",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
