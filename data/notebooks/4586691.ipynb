{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e39a262",
   "metadata": {},
   "source": [
    "<h1><center>APTOS 2019 Blindness Detection</center></h1>\n",
    "<h2><center>Detect diabetic retinopathy to stop blindness before it's too late</center></h2>\n",
    "<center><img src=\"https://raw.githubusercontent.com/dimitreOliveira/MachineLearning/master/Kaggle/APTOS%202019%20Blindness%20Detection/aux_img.png\"></center>\n",
    "\n",
    "In this synchronous Kernels-only competition, you'll build a machine learning model to speed up disease detection. Youâ€™ll work with thousands of images collected in rural areas to help identify diabetic retinopathy automatically. If successful, you will not only help to prevent lifelong blindness, but these models may be used to detect other sorts of diseases in the future, like glaucoma and macular degeneration.\n",
    "\n",
    "In this notebook, I will be using basic deep learning and transfer learning (ResNet50) to create a baseline.\n",
    "##### Image source: http://cceyemd.com/diabetes-and-eye-exams/\n",
    "\n",
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b4a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from keras.models import Model\n",
    "from keras import optimizers, applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "from tensorflow import set_random_seed\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(0)\n",
    "seed_everything()\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e9316",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5420f31",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "## Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of train samples: ', train.shape[0])\n",
    "print('Number of test samples: ', test.shape[0])\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7989313",
   "metadata": {},
   "source": [
    "## Label class distribution\n",
    "\n",
    "As we can see we have an unbalanced database, we have two times more class 0 than 2, and classes 1, 2 and 4 each have less than half of the class 2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f61503",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(14, 8.7))\n",
    "ax = sns.countplot(x=\"diagnosis\", data=train, palette=\"GnBu_d\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e6d7d",
   "metadata": {},
   "source": [
    "##### Legend\n",
    "- 0 - No DR\n",
    "- 1 - Mild\n",
    "- 2 - Moderate\n",
    "- 3 - Severe\n",
    "- 4 - Proliferative DR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75278441",
   "metadata": {},
   "source": [
    "### Now let's see some of the images\n",
    "\n",
    "The images have different sizes, they may need resizing or some padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "count = 1\n",
    "plt.figure(figsize=[20, 20])\n",
    "for img_name in train['id_code'][:15]:\n",
    "    img = cv2.imread(\"../input/aptos2019-blindness-detection/train_images/%s.png\" % img_name)[...,[2, 1, 0]]\n",
    "    plt.subplot(5, 5, count)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Image %s\" % count)\n",
    "    count += 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac5ce0",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bca27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "WARMUP_EPOCHS = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "WARMUP_LEARNING_RATE = 1e-3\n",
    "HEIGHT = 512\n",
    "WIDTH = 512\n",
    "CANAL = 3\n",
    "N_CLASSES = train['diagnosis'].nunique()\n",
    "ES_PATIENCE = 5\n",
    "RLROP_PATIENCE = 3\n",
    "DECAY_DROP = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92361314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocecss data\n",
    "train[\"id_code\"] = train[\"id_code\"].apply(lambda x: x + \".png\")\n",
    "test[\"id_code\"] = test[\"id_code\"].apply(lambda x: x + \".png\")\n",
    "train['diagnosis'] = train['diagnosis'].astype('str')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eeb358",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255, \n",
    "                                 validation_split=0.2,\n",
    "                                 horizontal_flip=True)\n",
    "\n",
    "train_generator=train_datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    directory=\"../input/aptos2019-blindness-detection/train_images/\",\n",
    "    x_col=\"id_code\",\n",
    "    y_col=\"diagnosis\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    subset='training')\n",
    "\n",
    "valid_generator=train_datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    directory=\"../input/aptos2019-blindness-detection/train_images/\",\n",
    "    x_col=\"id_code\",\n",
    "    y_col=\"diagnosis\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",    \n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    subset='validation')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(  \n",
    "        dataframe=test,\n",
    "        directory = \"../input/aptos2019-blindness-detection/test_images/\",\n",
    "        x_col=\"id_code\",\n",
    "        target_size=(HEIGHT, WIDTH),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        class_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e33f71",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = applications.ResNet50(weights=None, \n",
    "                                       include_top=False,\n",
    "                                       input_tensor=input_tensor)\n",
    "    base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    final_output = Dense(n_out, activation='softmax', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=(HEIGHT, WIDTH, CANAL), n_out=N_CLASSES)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i in range(-5, 0):\n",
    "    model.layers[i].trainable = True\n",
    "\n",
    "metric_list = [\"accuracy\"]\n",
    "optimizer = optimizers.Adam(lr=WARMUP_LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\",  metrics=metric_list)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b08c3",
   "metadata": {},
   "source": [
    "# Train top layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8866c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "history_warmup = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=WARMUP_EPOCHS,\n",
    "                              verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59316529",
   "metadata": {},
   "source": [
    "# Fine-tune the complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db086b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)\n",
    "\n",
    "callback_list = [es, rlrop]\n",
    "optimizer = optimizers.Adam(lr=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\",  metrics=metric_list)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46efe238",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_finetunning = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=EPOCHS,\n",
    "                              callbacks=callback_list,\n",
    "                              verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0cbee2",
   "metadata": {},
   "source": [
    "# Model loss graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'loss': history_warmup['loss'] + history_finetunning['loss'], \n",
    "           'val_loss': history_warmup['val_loss'] + history_finetunning['val_loss'], \n",
    "           'acc': history_warmup['acc'] + history_finetunning['acc'], \n",
    "           'val_acc': history_warmup['val_acc'] + history_finetunning['val_acc']}\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex='col', figsize=(20, 14))\n",
    "\n",
    "ax1.plot(history['loss'], label='Train loss')\n",
    "ax1.plot(history['val_loss'], label='Validation loss')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_title('Loss')\n",
    "\n",
    "ax2.plot(history['acc'], label='Train Accuracy')\n",
    "ax2.plot(history['val_acc'], label='Validation accuracy')\n",
    "ax2.legend(loc='best')\n",
    "ax2.set_title('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612eccef",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_datagen = ImageDataGenerator(rescale=1./255)\n",
    "complete_generator = complete_datagen.flow_from_dataframe(  \n",
    "        dataframe=train,\n",
    "        directory = \"../input/aptos2019-blindness-detection/train_images/\",\n",
    "        x_col=\"id_code\",\n",
    "        target_size=(HEIGHT, WIDTH),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        class_mode=None)\n",
    "\n",
    "STEP_SIZE_COMPLETE = complete_generator.n//complete_generator.batch_size\n",
    "train_preds = model.predict_generator(complete_generator, steps=STEP_SIZE_COMPLETE)\n",
    "train_preds = [np.argmax(pred) for pred in train_preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9acb13",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']\n",
    "cnf_matrix = confusion_matrix(train['diagnosis'].astype('int'), train_preds)\n",
    "cnf_matrix_norm = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(cnf_matrix_norm, index=labels, columns=labels)\n",
    "plt.figure(figsize=(16, 7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='.2f', cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da4c6a",
   "metadata": {},
   "source": [
    "## Quadratic Weighted Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df443b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds, train['diagnosis'].astype('int'), weights='quadratic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688df67",
   "metadata": {},
   "source": [
    "# Apply model to test set and output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
    "preds = model.predict_generator(test_generator, steps=STEP_SIZE_TEST)\n",
    "predictions = [np.argmax(pred) for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb48f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "results = pd.DataFrame({'id_code':filenames, 'diagnosis':predictions})\n",
    "results['id_code'] = results['id_code'].map(lambda x: str(x)[:-4])\n",
    "results.to_csv('submission.csv',index=False)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e388883",
   "metadata": {},
   "source": [
    "## Predictions class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(14, 8.7))\n",
    "ax = sns.countplot(x=\"diagnosis\", data=results, palette=\"GnBu_d\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
