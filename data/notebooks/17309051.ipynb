{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "training_paths = []\n",
    "interpolate_paths = []\n",
    "extrapolate_paths = []\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        p = str(os.path.join(dirname, filename))\n",
    "        if 'train' in p:\n",
    "            training_paths.append(p)\n",
    "        elif 'interpolate' in p:\n",
    "            interpolate_paths.append(p)\n",
    "        elif 'extrapolate' in p:\n",
    "            extrapolate_paths.append(p)\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "# print(training_paths, '\\n')\n",
    "# print(interpolate_paths, '\\n')\n",
    "# print(extrapolate_paths)\n",
    "\n",
    "arithmetic_train_paths = []\n",
    "probability_train_paths = []\n",
    "for p in training_paths:\n",
    "    if 'probability' in p:\n",
    "        probability_train_paths.append(p)\n",
    "    else:\n",
    "        arithmetic_train_paths.append(p)\n",
    "print(\"TRAINING\", '\\n')\n",
    "print(arithmetic_train_paths, '\\n')\n",
    "print(probability_train_paths, '\\n')\n",
    "\n",
    "        \n",
    "arithmetic_interpolate_paths = []\n",
    "probability_interpolate_paths = []\n",
    "for p in interpolate_paths:\n",
    "    if 'probability' in p:\n",
    "        probability_interpolate_paths.append(p)\n",
    "    else:\n",
    "        arithmetic_interpolate_paths.append(p)\n",
    "print(\"INTERPOLATION\", '\\n')\n",
    "print(arithmetic_interpolate_paths, '\\n')\n",
    "print(probability_interpolate_paths, '\\n')\n",
    "        \n",
    "        \n",
    "arithmetic_extrapolate_paths = []\n",
    "probability_extrapolate_paths = []\n",
    "for p in extrapolate_paths:\n",
    "    if 'probability' in p:\n",
    "        probability_extrapolate_paths.append(p)\n",
    "    else:\n",
    "        arithmetic_extrapolate_paths.append(p)\n",
    "print(\"EXTRAPOLATION\", '\\n')\n",
    "print(arithmetic_extrapolate_paths, '\\n')\n",
    "print(probability_extrapolate_paths, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f41d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.15.0\n",
    "!pip install torch==0.4.1.post2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from time import time\n",
    "import spacy\n",
    "import gensim\n",
    "import re\n",
    "from gensim.models import Word2Vec, Doc2Vec, FastText\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('/kaggle/input/deepmindmath/mathematics_dataset-v1.0/train-hard/probability__swr_p_sequence.txt', 'r')\n",
    "lines = file1.readlines()\n",
    "questions = []\n",
    "answers = []\n",
    "for lin_num, line in enumerate(lines):\n",
    "    if lin_num % 2 == 0:\n",
    "        questions.append(line)\n",
    "    else:\n",
    "        answers.append(line)\n",
    "df = pd.DataFrame()\n",
    "df['questions'] = pd.Series(questions)[:1000]\n",
    "df['answers'] = pd.Series(answers)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['questions'].isnull().sum(), df['answers'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84492607",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/olsenmatthew/LBF.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebcc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0488db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv LBF/* ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf LBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d41d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_and_evaluate import *\n",
    "from src.models import *\n",
    "import time\n",
    "import torch.optim\n",
    "from src.expressions_transfer import *\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('--model', default='fix', type=str, choices=['fix','ma-fix','reinforce','mapo'], help='training method')\n",
    "# parser.add_argument('--nstep', default=50, type=int, help='m-fix')\n",
    "# parser.add_argument('--name', default='fix', type=str, help='model name')\n",
    "\n",
    "# options = parser.parse_args()\n",
    "# model = options.model\n",
    "# n_step = options.nstep \n",
    "# model_name = options.name\n",
    "model = 'ma-fix'\n",
    "n_step = 50\n",
    "model_name = 'ma-fix'\n",
    "\n",
    "batch_size = 64\n",
    "embedding_size = 128\n",
    "hidden_size = 512\n",
    "n_epochs = 25#0 # TODO configure, original was 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "beam_size = 5\n",
    "n_layers = 2\n",
    "\n",
    "data = load_raw_data(\"data/Math_23K.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fce033",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmm = []\n",
    "for id in range(len(df.index)):\n",
    "    dmm.append({'id': id, \n",
    "                'original_text': df.loc[id]['questions'], \n",
    "                'ans': df.loc[id]['answers'].replace('\\n', ''),\n",
    "                'segmented_text': df.loc[id]['questions'],\n",
    "                'equation': ''\n",
    "               })\n",
    "    \n",
    "#     if id > 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = transfer_num(dmm)\n",
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4451bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pairs = []\n",
    "for p in pairs:\n",
    "    temp_pairs.append((p[0], p[1], p[2], p[3], p[4]))\n",
    "pairs = temp_pairs\n",
    "fold_size = int(len(pairs) * 0.2)\n",
    "fold_pairs = []\n",
    "for split_fold in range(4):\n",
    "    fold_start = fold_size * split_fold\n",
    "    fold_end = fold_size * (split_fold + 1)\n",
    "    fold_pairs.append(pairs[fold_start:fold_end])\n",
    "fold_pairs.append(pairs[(fold_size * 4):])\n",
    "\n",
    "best_acc_fold = []\n",
    "\n",
    "fold = 1 #we can also iterate all the folds like GTS\n",
    "pairs_tested = []\n",
    "pairs_trained = []\n",
    "for fold_t in range(5):\n",
    "    if fold_t == fold:\n",
    "        pairs_tested += fold_pairs[fold_t]\n",
    "    else:\n",
    "        pairs_trained += fold_pairs[fold_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang, output_lang, train_pairs, test_pairs = prepare_data(pairs_trained, pairs_tested, 5)\n",
    "# Initialize models\n",
    "encoder = EncoderSeq(input_size=input_lang.n_words, embedding_size=embedding_size, hidden_size=hidden_size,\n",
    "                        n_layers=n_layers)\n",
    "predict = Prediction(hidden_size=hidden_size, op_nums=5,\n",
    "                        input_size=2)\n",
    "generate = GenerateNode(hidden_size=hidden_size, op_nums=5,\n",
    "                        embedding_size=embedding_size)\n",
    "merge = Merge(hidden_size=hidden_size, embedding_size=embedding_size)\n",
    "\n",
    "# predict.load_state_dict(torch.load('./pretrain/predict'))\n",
    "# encoder.load_state_dict(torch.load('pretrain/encoder'))\n",
    "# generate.load_state_dict(torch.load('pretrain/generate'))\n",
    "# merge.load_state_dict(torch.load('pretrain/merge'))\n",
    "# # the embedding layer is  only for generated number embeddings, operators, and paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "predict_optimizer = torch.optim.Adam(predict.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "generate_optimizer = torch.optim.Adam(generate.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "merge_optimizer = torch.optim.Adam(merge.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=20, gamma=0.5)\n",
    "predict_scheduler = torch.optim.lr_scheduler.StepLR(predict_optimizer, step_size=20, gamma=0.5)\n",
    "generate_scheduler = torch.optim.lr_scheduler.StepLR(generate_optimizer, step_size=20, gamma=0.5)\n",
    "merge_scheduler = torch.optim.lr_scheduler.StepLR(merge_optimizer, step_size=20, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beae445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    predict.cuda()\n",
    "    generate.cuda()\n",
    "    merge.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_batches = [[] for i in range (len(train_pairs))]\n",
    "buffer_batches_exp = [[] for i in range (len(train_pairs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bde954",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "        'loss': [],\n",
    "        'test_epoch': [],\n",
    "        'test_result_acc3': [],\n",
    "        'test_result_acc1': [],\n",
    "        'test_result_acc5':[],\n",
    "        'iteration': []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbe50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "for epoch in range(n_epochs):\n",
    "    encoder_scheduler.step()\n",
    "    predict_scheduler.step()\n",
    "    generate_scheduler.step()\n",
    "    merge_scheduler.step()\n",
    "    loss_total = 0\n",
    "    input_batches, input_lengths, nums_batches, num_pos_batches, num_size_batches, num_ans_batches, num_id_batches = prepare_train_batch(train_pairs, batch_size)\n",
    "    print(\"fold:\", fold + 1)\n",
    "    print(\"epoch:\", epoch + 1)\n",
    "    start = time.time()\n",
    "    mask_flag = False\n",
    "    pos = 0\n",
    "    epo_iteration = 0\n",
    "    for idx in range(len(input_lengths)): #batch\n",
    "\n",
    "        if idx < 2 and epoch == 0:\n",
    "            mask_flag = True\n",
    "        buffer_batches_train = buffer_batches[pos : pos + len(input_lengths[idx])]\n",
    "        buffer_batches_train_exp = buffer_batches_exp[pos : pos + len(input_lengths[idx])]\n",
    "\n",
    "        loss, buffer_batch_new, iterations, buffer_batch_exp = train_tree(\n",
    "            input_batches[idx], input_lengths[idx], \n",
    "            num_size_batches[idx], encoder, predict, generate, merge,\n",
    "            encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos_batches[idx], num_ans_batches[idx], nums_batches[idx], buffer_batches_train, buffer_batches_train_exp, epoch, model, n_step, mask_flag)\n",
    "        loss_total += loss\n",
    "        iteration += iterations\n",
    "        epo_iteration += iterations\n",
    "        buffer_batches[pos : pos+len(input_lengths[idx])] = buffer_batch_new\n",
    "        buffer_batches_exp[pos : pos+len(input_lengths[idx])] = buffer_batch_exp\n",
    "        pos += len(input_lengths[idx])\n",
    "    \n",
    "    loss_total = loss_total if epo_iteration == 0 else loss_total/epo_iteration\n",
    "    stats['loss'].append(loss_total)\n",
    "    stats['iteration'].append(iteration)\n",
    "    print(\"loss:\", loss_total)\n",
    "    print(\"training time\", time_since(time.time() - start))\n",
    "    print(\"--------------------------------\")\n",
    "    if epoch % 1 == 0 or epoch > n_epochs - 5:\n",
    "        buffer_dict = {\n",
    "        'id': [],\n",
    "        'original_text': [],\n",
    "        'segmented_text': [],\n",
    "        'gt_equation': [],\n",
    "        'ans':[],\n",
    "        'gen_equations': []\n",
    "        }\n",
    "\n",
    "        value_ac3 = 0\n",
    "        eval_total3 = 0\n",
    "        value_ac1 = 0\n",
    "        eval_total1 = 0\n",
    "        value_ac5 = 0\n",
    "        eval_total5 = 0\n",
    "        start = time.time()\n",
    "        for k in range(len(test_pairs)):\n",
    "            test_batch = test_pairs[k]\n",
    "            test_exps = []\n",
    "            test_results = evaluate_tree(test_batch[0], test_batch[1], encoder, predict, generate,\n",
    "                                        merge, output_lang, test_batch[3], beam_size=beam_size)\n",
    "            #test_res = test_results[0]\n",
    "            for i in range (0, len(test_results)):\n",
    "                test_res = test_results[i]\n",
    "                val_ac, test_exp = compute_prefix_tree_result(test_res, test_batch[4], output_lang, test_batch[2])\n",
    "\n",
    "                if val_ac:\n",
    "                    test_exps.append(test_exp)\n",
    "                if val_ac:\n",
    "                    value_ac5 += 1\n",
    "                eval_total5 += 1\n",
    "\n",
    "                if i < 3:\n",
    "                    if val_ac:\n",
    "                        value_ac3 += 1\n",
    "                    eval_total3 += 1\n",
    "\n",
    "                if i == 0:\n",
    "                    if val_ac:\n",
    "                        value_ac1 += 1\n",
    "                    eval_total1 += 1\n",
    "\n",
    "            # id2 = int(test_pairs[k][7])\n",
    "            # buffer_dict['id'].append(id2)\n",
    "            # id2 = id2 - 1\n",
    "            # buffer_dict['original_text'].append(data[id2]['original_text'])\n",
    "            # buffer_dict['segmented_text'].append(data[id2]['segmented_text'])\n",
    "            # buffer_dict['ans'].append(data[id2]['ans'])\n",
    "            # buffer_dict['gt_equation'].append(data[id2]['equation'])\n",
    "            # buffer_dict['gen_equations'].append(test_exps)\n",
    "\n",
    "        stats['test_epoch'].append (epoch)\n",
    "        stats['test_result_acc3'].append(float(value_ac3) / eval_total3)\n",
    "        stats['test_result_acc1'].append(float(value_ac1) / eval_total1)\n",
    "        stats['test_result_acc5'].append(float(value_ac5) / eval_total5)\n",
    "\n",
    "        print(value_ac1, eval_total1)\n",
    "        print(\"test_answer_acc5\", float(value_ac5) / eval_total5)\n",
    "        print(\"test_answer_acc3\", float(value_ac3) / eval_total3)\n",
    "        print(\"test_answer_acc1\", float(value_ac1) / eval_total1)\n",
    "        print(\"testing time\", time_since(time.time() - start))\n",
    "        print(\"------------------------------------------------------\")\n",
    "        torch.save(encoder.state_dict(), \"models/encoder\")\n",
    "        torch.save(predict.state_dict(), \"models/predict\")\n",
    "        torch.save(generate.state_dict(), \"models/generate\")\n",
    "        torch.save(merge.state_dict(), \"models/merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b30407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3eba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = []\n",
    "y_ax = []\n",
    "for x,y in enumerate(stats['loss']):\n",
    "    x_ax.append(x)\n",
    "    y_ax.append(y)\n",
    "plt.scatter(x_ax,y_ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in enumerate(stats['test_result_acc1']):\n",
    "    x_ax.append(x)\n",
    "    y_ax.append(y)\n",
    "plt.scatter(x_ax,y_ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in enumerate(stats['test_result_acc3']):\n",
    "    x_ax.append(x)\n",
    "    y_ax.append(y)\n",
    "plt.scatter(x_ax,y_ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in enumerate(stats['test_result_acc5']):\n",
    "    x_ax.append(x)\n",
    "    y_ax.append(y)\n",
    "plt.scatter(x_ax,y_ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
