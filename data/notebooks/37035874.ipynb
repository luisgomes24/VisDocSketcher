{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import missingno\n",
    "import matplotlib.pyplot as plt\n",
    "import eli5\n",
    "import catboost\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4c626",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data available\n",
    "train_data = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/spaceship-titanic/sample_submission.csv')\n",
    "\n",
    "print('train_data', train_data.shape, \n",
    "      'test_data', test_data.shape, \n",
    "      'sample_submission',sample_submission.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect together train and test data to process all columns in a same way\n",
    "data = pd.concat([train_data, test_data])\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65beb0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.matrix(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94651e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fill NaN in series wth random non NaN value\n",
    "def fill_with_random(series: pd.Series):\n",
    "    \n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    \n",
    "    series2 = series.copy()\n",
    "    series2 = series2.apply(\n",
    "        lambda x: rng.choice(series2.dropna().values) if x!=x or x is None else x)\n",
    "    return series2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66621cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill empties with first value within group column\n",
    "def fill_group_forward(data, column, group):\n",
    "    \n",
    "    grp = data.groupby(group)[column].first()\n",
    "    \n",
    "    def fill(row):\n",
    "        if row[column] is None:\n",
    "            return grp[grp.index==row[group]].values[0]\n",
    "        else:\n",
    "            return row[column] \n",
    "    \n",
    "    data[column] = data.apply(fill, axis=1)\n",
    "    data[column] = fill_with_random(data[column])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d25ac",
   "metadata": {},
   "source": [
    "---\n",
    "**Passenger ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split passenger id to its group id and place in a group\n",
    "data['group_id'] = data['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n",
    "data['num_in_group'] = data['PassengerId'].apply(lambda x: x.split('_')[1]).astype(int)\n",
    "\n",
    "print('Groups total:', data['group_id'].nunique())\n",
    "print('Persons in group:')\n",
    "sns.histplot(data['num_in_group']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850126ff",
   "metadata": {},
   "source": [
    "---\n",
    "**Home Planet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill home planet with random values in a same percentage, as existing data\n",
    "print(data['HomePlanet'].value_counts(dropna=False), data['HomePlanet'].shape)\n",
    "data = fill_group_forward(data, column='HomePlanet', group='group_id') # fill_with_random(data['HomePlanet'])\n",
    "sns.histplot(data['HomePlanet']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af7d94",
   "metadata": {},
   "source": [
    "---\n",
    "**Cabin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf528d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fill_group_forward(data, column='Cabin', group='group_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cabin_code(x: str,n):\n",
    "    try:\n",
    "        split = x.split('/')\n",
    "    except:\n",
    "        return None \n",
    "\n",
    "    return split[n]\n",
    "\n",
    "\n",
    "# extract specific featires from cabin description\n",
    "data['cabin_deck'] = data['Cabin'].apply(lambda x: split_cabin_code(x,0)) \n",
    "data['cabin_num'] = data['Cabin'].apply(lambda x: split_cabin_code(x,1)).astype(int)\n",
    "data['cabin_side'] = data['Cabin'].apply(lambda x: split_cabin_code(x,2)) \n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "sns.histplot(data['cabin_deck'], ax=axs[0])\n",
    "sns.histplot(data['cabin_num'], ax=axs[1])\n",
    "sns.histplot(data['cabin_side'], ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eae444a",
   "metadata": {},
   "source": [
    "---\n",
    "**CryoSleep**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# share of sleepers at dufferent decks and ship sides\n",
    "sns.heatmap(data.pivot_table(index='cabin_deck', columns='cabin_side', values='CryoSleep', aggfunc='mean') ,annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['CryoSleep'].value_counts(dropna=False))\n",
    "data = fill_group_forward(data, column='CryoSleep', group='group_id')\n",
    "sns.histplot(data['CryoSleep']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee5972",
   "metadata": {},
   "source": [
    "---\n",
    "**Destination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Destination'].value_counts(dropna=False))\n",
    "data = fill_group_forward(data, column='Destination', group='group_id')\n",
    "sns.histplot(data['Destination']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7e976",
   "metadata": {},
   "source": [
    "---\n",
    "**Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill age randomly\n",
    "print(data['Age'].value_counts(dropna=False))\n",
    "data['Age']= fill_with_random(data['Age'])\n",
    "sns.histplot(data['Age']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f516bf",
   "metadata": {},
   "source": [
    "---\n",
    "**VIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9cf2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['VIP'].value_counts(dropna=False))\n",
    "\n",
    "# check percentage of VIP on different decks and cabin sides\n",
    "deck_to_vip = data.pivot_table(index='cabin_deck', values='VIP', aggfunc='mean')\n",
    "sns.heatmap(deck_to_vip)\n",
    "plt.show()\n",
    "\n",
    "# fill VIP status randomly\n",
    "for deck in data['cabin_deck'].unique():\n",
    "    # fill subset of passengers in deck/side\n",
    "    data.loc[(data['cabin_deck'] == deck), 'VIP'] = (\n",
    "            fill_with_random(data.loc[(data['cabin_deck'] == deck)]['VIP'])\n",
    "        )\n",
    "    \n",
    "data['VIP']=data['VIP'].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110bc560",
   "metadata": {},
   "source": [
    "---\n",
    "**RoomService, FoodCourt, ShoppingMall, Spa, VRDeck**\n",
    "\n",
    "`RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck` - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']:\n",
    "    fig = plt.figure(figsize=(15,0.5))\n",
    "    sns.boxplot(data.loc[data[col]>0][[col]], x=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "# check some correlations between bills and different possibly affecting factors\n",
    "deck_to_bill = data.pivot_table(index='cabin_deck', \n",
    "                                values=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'], aggfunc='mean')\n",
    "sns.heatmap(deck_to_bill, annot=True, fmt=\".0f\", ax=axs[0])\n",
    "\n",
    "vip_to_bill = data.pivot_table(index='VIP', values=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'], aggfunc='mean')\n",
    "sns.heatmap(vip_to_bill, annot=True, fmt=\".0f\", ax=axs[1])\n",
    "\n",
    "age_to_bill = data.pivot_table(index='Age', values=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'], aggfunc='mean')\n",
    "sns.heatmap(age_to_bill, ax=axs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca538c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna({'RoomService':0, 'FoodCourt':0, 'ShoppingMall':0, 'Spa':0, 'VRDeck':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491fec8f",
   "metadata": {},
   "source": [
    "---\n",
    "**Name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acbe3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_full_name(x: str,n):\n",
    "    try:\n",
    "        split = x.split(' ')\n",
    "    except:\n",
    "        return None \n",
    "\n",
    "    return split[n]\n",
    "\n",
    "# extract name and family name\n",
    "data['first_name'] = data['Name'].apply(lambda x: split_full_name(x,0)) \n",
    "data['last_name'] = data['Name'].apply(lambda x: split_full_name(x,1))\n",
    "\n",
    "# fill gaps with \n",
    "data['first_name'] = fill_with_random(data['first_name'])\n",
    "data = fill_group_forward(data, column='last_name', group='group_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d54a8",
   "metadata": {},
   "source": [
    "**Finalize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('PassengerId', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6364c5",
   "metadata": {},
   "source": [
    "## Make baseline submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad337e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, target = 'Transported'):\n",
    "    \n",
    "    # prepare data for train, validation and submission\n",
    "    x = data.drop(columns=target)  \n",
    "    y = data[target]  \n",
    "    \n",
    "    # drop text columns\n",
    "    for c in x.columns:\n",
    "        if x[c].dtype =='object':\n",
    "            x.drop(columns=c, inplace=True)\n",
    "\n",
    "    # extract train data\n",
    "    x_train = x[~y.isna()]\n",
    "    y_train = y[~y.isna()].astype(int)\n",
    "\n",
    "    #extract ubmission data\n",
    "    x_test = x[y.isna()]\n",
    "\n",
    "    return x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(model, data, \n",
    "                      new_feature_names=None, folds=10, scoring='accuracy',\n",
    "                      top_n_features_to_show=30, submission_file_name='submission.csv', silent=False):\n",
    "    \n",
    "    (x_train, y_train, x_test) = data\n",
    "    \n",
    "    cv = StratifiedKFold(folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # make cross-validation\n",
    "    cv_scores = cross_val_score(model, x_train, y_train, cv=cv, scoring=scoring, n_jobs=4)\n",
    "    if not silent: print('CV scores', cv_scores)\n",
    "    if not silent: print(f'CV mean:{cv_scores.mean():.4f}, CV std:{cv_scores.std():.4f}')\n",
    "    \n",
    "    # train model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # show feature importances\n",
    "    if not silent:\n",
    "        display(eli5.show_weights(estimator=model, \n",
    "                  feature_names=x_train.columns.to_list(), top=top_n_features_to_show))\n",
    " \n",
    "    # print new features stats\n",
    "    if new_feature_names:\n",
    "        print('New feature weights:')\n",
    "        try:\n",
    "            print(pd.DataFrame({'feature': new_feature_names, \n",
    "                        'coef': model.coef_.flatten()[-len(new_feature_names):]}))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # make submission\n",
    "    preds = model.predict(x_test)\n",
    "    preds = pd.DataFrame(preds, index=x_test.index).astype(bool)\n",
    "    preds.columns=['Transported']\n",
    "    \n",
    "    # save submission file\n",
    "    submission = sample_submission.drop(columns='Transported').\\\n",
    "        merge(preds.reset_index(), how='left', on='PassengerId')\n",
    "    \n",
    "    submission.to_csv(submission_file_name, index=False)\n",
    "    \n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "catreg = catboost.CatBoostClassifier(random_state=42, verbose=False)\n",
    "\n",
    "cv_scores1 = train_and_predict(catreg, split_data(data), submission_file_name='submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db349c",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d612ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cv_scores(cv_score_old, cv_score_new):\n",
    "    \n",
    "    folds_compare = cv_score_new > cv_score_old\n",
    "    print('\\nFolds compare:', folds_compare, end='\\n\\n')\n",
    "    \n",
    "    if cv_score_new.mean() > cv_score_old.mean():\n",
    "        print('Score increased \\t[GOOD]', end='')\n",
    "    else:\n",
    "        print('Score decreased \\t[BAD]', end='')\n",
    "    print(f'\\t{cv_score_old.mean():.4f} -> {cv_score_new.mean():.4f}',\n",
    "          f'{cv_score_new.mean() - cv_score_old.mean():.4f}')\n",
    "        \n",
    "    if cv_score_new.std()>cv_score_old.std():\n",
    "        print('Variation increased \\t[BAD]', end='')\n",
    "    else:\n",
    "        print('Variation decreased \\t[GOOD]', end='')    \n",
    "    print(f'\\t{cv_score_old.std():.4f} -> {cv_score_new.std():.4f}',\n",
    "          f'{cv_score_new.std() - cv_score_old.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode home planet\n",
    "def add_homeplanet_one(data):\n",
    "    data = data.join(pd.get_dummies(data['HomePlanet'], prefix='home', drop_first=True))\n",
    "    return data\n",
    "\n",
    "data = add_homeplanet_one(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e866a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode cabin side \n",
    "def add_cabin_side(data):\n",
    "    data['cabin_side'] = data['cabin_side'].map({'S': 1, 'P': 0}).astype(int)\n",
    "    return data\n",
    "\n",
    "data = add_cabin_side(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c741aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if person single or not\n",
    "def add_group_size(data):\n",
    "    group_sizes = data['group_id'].value_counts().reset_index()\n",
    "    group_sizes.columns=['group_id','group_size']\n",
    "    data = data.reset_index().merge(group_sizes, how='left', on='group_id').set_index('PassengerId')\n",
    "    \n",
    "    def categorize_size(x):\n",
    "        if x<=1: # single\n",
    "            return 1\n",
    "        elif x<=2: #couple\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    data['group_size'] = data['group_size'].apply(categorize_size).astype(int)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = add_group_size(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_deck_bill(data):\n",
    "    # mean bill on a deck\n",
    "    data['deck_mean_bill']=data['cabin_deck'].map({'A': 3331, 'B': 2927, 'C': 3937, 'D': 2296, \n",
    "                                                   'E': 1343, 'F': 1001, 'G': 408, 'T': 5916,  }).astype(int) \n",
    "\n",
    "    return data\n",
    "\n",
    "data = add_deck_bill(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14699675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weighted_bills(data):\n",
    "    money_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "    data['total_bill'] = data[['RoomService', 'FoodCourt', \n",
    "                               'ShoppingMall', 'Spa', \n",
    "                               'VRDeck']].apply(lambda x: 0.1 if x.sum()==0 else x.sum(), axis=1).astype(int)\n",
    "    for col in money_cols:\n",
    "        data[col+'_w'] = data[col]/data['total_bill']\n",
    "    \n",
    "    \n",
    "    data['total_bill'] = (data['total_bill'] - data['total_bill'].mean())/ data['total_bill'].std()\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = add_weighted_bills(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d334d",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03563720",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores2 = train_and_predict(catreg, split_data(data), submission_file_name='submission.csv' )\n",
    "compare_cv_scores(cv_scores1, cv_scores2)\n",
    "cv_scores1 = cv_scores2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029aa5e0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**0.80827** Public score\n",
    "    \n",
    "    Folds compare: [ True  True  True  True False  True  True  True  True  True]\n",
    "\n",
    "    Score increased        [GOOD]  0.7996 -> 0.8107  0.0110\n",
    "    Variation decreased    [GOOD]  0.0166 -> 0.0138 -0.0028"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
