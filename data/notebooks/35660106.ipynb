{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b6b98a",
   "metadata": {},
   "source": [
    "In this notebook, we'll use a pre-trained machine learning model to generate a submission to the [BirdClef2023 competition](https://www.kaggle.com/c/birdclef-2023).  The goal of the competition is to identify Eastern African bird species by sound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a6bcf",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f09af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob\n",
    "\n",
    "import csv\n",
    "import io\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f0474",
   "metadata": {},
   "source": [
    "## Step 2: Explore the training data\n",
    "\n",
    "We'll start by loading a couple of training examples and using the IPython.display.Audio module to play them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913350b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample audio files from two different species\n",
    "audio_abe, sr_abe = librosa.load(\"/kaggle/input/birdclef-2023/train_audio/abethr1/XC128013.ogg\")\n",
    "audio_abh, sr_abh = librosa.load(\"/kaggle/input/birdclef-2023/train_audio/abhori1/XC127317.ogg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe00806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the audio\n",
    "Audio(data=audio_abe, rate=sr_abe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the audio\n",
    "Audio(data=audio_abh, rate=sr_abh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c9e34a",
   "metadata": {},
   "source": [
    "## Step 3: Match the model's output with the bird species in the competition\n",
    "\n",
    "The competition includes 264 classes of birds, 261 of which exist in this model. We'll set up a way to map the model's output logits to our competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load('https://kaggle.com/models/google/bird-vocalization-classifier/frameworks/tensorFlow2/variations/bird-vocalization-classifier/versions/1')\n",
    "labels_path = hub.resolve('https://kaggle.com/models/google/bird-vocalization-classifier/frameworks/tensorFlow2/variations/bird-vocalization-classifier/versions/1') + \"/assets/label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7438d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the name of the class with the top score when mean-aggregated across frames.\n",
    "def class_names_from_csv(class_map_csv_text):\n",
    "    \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n",
    "    with open(labels_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        class_names = [mid for mid, desc in csv_reader]\n",
    "        return class_names[1:]\n",
    "\n",
    "## note that the bird classifier classifies a much larger set of birds than the\n",
    "## competition, so we need to load the model's set of class names or else our \n",
    "## indices will be off.\n",
    "classes = class_names_from_csv(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(\"/kaggle/input/birdclef-2023/train_metadata.csv\")\n",
    "train_metadata.head()\n",
    "competition_classes = sorted(train_metadata.primary_label.unique())\n",
    "\n",
    "forced_defaults = 0\n",
    "competition_class_map = []\n",
    "for c in competition_classes:\n",
    "    try:\n",
    "        i = classes.index(c)\n",
    "        competition_class_map.append(i)\n",
    "    except:\n",
    "        competition_class_map.append(0)\n",
    "        forced_defaults += 1\n",
    "        \n",
    "## this is the count of classes not supported by our pretrained model\n",
    "## you could choose to simply not predict these, set a default as above,\n",
    "## or create your own model using the pretrained model as a base.\n",
    "forced_defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec3cd9",
   "metadata": {},
   "source": [
    "## Step 4: Preprocess the data\n",
    "\n",
    "The following functions are one way to load the audio provided and break it up into the five-second samples with a sample rate of 32,000 required by the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_audio(\n",
    "      audio_array: np.ndarray,\n",
    "      window_size_s: float = 5.0,\n",
    "      hop_size_s: float = 5.0,\n",
    "      sample_rate = 32000,\n",
    "      ) -> np.ndarray:\n",
    "    \n",
    "    \"\"\"Helper function for framing audio for inference.\"\"\"\n",
    "    \"\"\" using tf.signal \"\"\"\n",
    "    if window_size_s is None or window_size_s < 0:\n",
    "        return audio_array[np.newaxis, :]\n",
    "    frame_length = int(window_size_s * sample_rate)\n",
    "    hop_length = int(hop_size_s * sample_rate)\n",
    "    framed_audio = tf.signal.frame(audio_array, frame_length, hop_length, pad_end=True)\n",
    "    return framed_audio\n",
    "\n",
    "def ensure_sample_rate(waveform, original_sample_rate,\n",
    "                       desired_sample_rate=32000):\n",
    "    \"\"\"Resample waveform if required.\"\"\"\n",
    "    if original_sample_rate != desired_sample_rate:\n",
    "        waveform = tfio.audio.resample(waveform, original_sample_rate, desired_sample_rate)\n",
    "    return desired_sample_rate, waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaacf8c",
   "metadata": {},
   "source": [
    "Below we load one training sample - use the Audio function to listen to the samples inside the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sample_rate = librosa.load(\"/kaggle/input/birdclef-2023/train_audio/afghor1/XC156639.ogg\")\n",
    "sample_rate, wav_data = ensure_sample_rate(audio, sample_rate)\n",
    "Audio(wav_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a065449",
   "metadata": {},
   "source": [
    "## Step 5: Make predictions\n",
    "\n",
    "Each test sample is cut into 5-second chunks. We use the pretrained model to return probabilities for all 10k birds included in the model, then pull out the classes used in this competition to create a final submission row. Note that we are NOT doing anything special to handle the 3 missing classes; those will need fine-tuning / transfer learning, which will be handled in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26172a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_tm = frame_audio(wav_data)\n",
    "logits, embeddings = model.infer_tf(fixed_tm[:1])\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "argmax = np.argmax(probabilities)\n",
    "print(f\"The audio is from the class {classes[argmax]} (element:{argmax} in the label.csv file), with probability of {probabilities[0][argmax]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_sample(filename, sample_submission, frame_limit_secs=None):\n",
    "    file_id = filename.split(\".ogg\")[0].split(\"/\")[-1]\n",
    "    \n",
    "    audio, sample_rate = librosa.load(filename)\n",
    "    sample_rate, wav_data = ensure_sample_rate(audio, sample_rate)\n",
    "    \n",
    "    fixed_tm = frame_audio(wav_data)\n",
    "    \n",
    "    frame = 5\n",
    "    all_logits, all_embeddings = model.infer_tf(fixed_tm[:1])\n",
    "    for window in fixed_tm[1:]:\n",
    "        if frame_limit_secs and frame > frame_limit_secs:\n",
    "            continue\n",
    "        \n",
    "        logits, embeddings = model.infer_tf(window[np.newaxis, :])\n",
    "        all_logits = np.concatenate([all_logits, logits], axis=0)\n",
    "        frame += 5\n",
    "    \n",
    "    frame = 5\n",
    "    all_probabilities = []\n",
    "    for frame_logits in all_logits:\n",
    "        probabilities = tf.nn.softmax(frame_logits).numpy()\n",
    "        \n",
    "        ## set the appropriate row in the sample submission\n",
    "        sample_submission.loc[sample_submission.row_id == file_id + \"_\" + str(frame), competition_classes] = probabilities[competition_class_map]\n",
    "        frame += 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633152b",
   "metadata": {},
   "source": [
    "## Step 6: Generate a submission\n",
    "\n",
    "Now we process all of the test samples as discussed above, creating output rows, and saving them in the provided `sample_submission.csv`. Finally, we save these rows to our final output file: `submission.csv`. This is the file that gets submitted and scored when you submit the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa499b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = list(glob.glob(\"/kaggle/input/birdclef-2023/test_soundscapes/*.ogg\"))\n",
    "test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81911e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(\"/kaggle/input/birdclef-2023/sample_submission.csv\")\n",
    "sample_sub[competition_classes] = sample_sub[competition_classes].astype(np.float32)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563dfb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_limit_secs = 15 if sample_sub.shape[0] == 3 else None\n",
    "for sample_filename in test_samples:\n",
    "    predict_for_sample(sample_filename, sample_sub, frame_limit_secs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d3dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911113c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
