{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3750e440",
   "metadata": {},
   "source": [
    "Hello everyone and welcome. \n",
    "\n",
    "In this notebook I will explain the convolutional neural networks, how it's work and why we use it in computer vision problems \n",
    "instead of using the traditional Multi-Layer Perceptron (MLP). \n",
    "\n",
    "First we will try to classify the image using traditional MLP (The results will be bad ‚òπÔ∏è) and explain why the results was \n",
    "bas when we use the MLP in image classification. \n",
    "\n",
    "Next we will see the components of Convolutional Neural Network CNN and see how it works and why it works better than the MLP\n",
    "in image classification problems. \n",
    "\n",
    "Finally we will see an example of CNN and compaire its result the the MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds\n",
    "import pathlib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b749ce0",
   "metadata": {},
   "source": [
    "# 1- Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d101980",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(r\"../input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of pathes for all images \n",
    "all_images = list(data_path.glob(r'*/*/*.jpg')) + list(data_path.glob(r'*/*/*.png'))\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "#looping through the pathes to extract pathes and labels\n",
    "for item in all_images:\n",
    "    \n",
    "    path = os.path.normpath(item)\n",
    "    splits = path.split(os.sep)\n",
    "    \n",
    "    if 'GT' not in splits[-2]:\n",
    "    \n",
    "        images.append(item)\n",
    "    \n",
    "        label = splits[-2]\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with two columns: image_path, label \n",
    "image_pathes = pd.Series(images).astype(str)\n",
    "labels = pd.Series(labels)\n",
    "\n",
    "dataframe = pd.concat([image_pathes, labels], axis=1)\n",
    "\n",
    "dataframe.columns = ['images', 'labels']\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f299a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15,10), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(plt.imread(dataframe.images[i]))\n",
    "    ax.set_title(dataframe.labels[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataframe rows and split it to train, val, test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataframe = dataframe.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c482f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train, test = train_test_split(shuffled_dataframe, test_size=0.15, random_state=0)\n",
    "train, val = train_test_split(all_train, test_size=0.17, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255.0,\n",
    "                                                                   rotation_range=40,\n",
    "                                                                   zoom_range=0.2,\n",
    "                                                                   width_shift_range=0.2,\n",
    "                                                                   height_shift_range=0.2,\n",
    "                                                                   shear_range=0.2,\n",
    "                                                                   horizontal_flip=True,\n",
    "                                                                   vertical_flip=True)\n",
    "\n",
    "training_generator = training_data_gen.flow_from_dataframe(dataframe=train,\n",
    "                                                           x_col='images', y_col='labels',\n",
    "                                                           target_size=(224, 224),\n",
    "                                                           color_mode='rgb',\n",
    "                                                           class_mode='categorical',\n",
    "                                                           batch_size=64)\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "val_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255.0)\n",
    "validation_generator = val_data_gen.flow_from_dataframe(dataframe=val,\n",
    "                                                        x_col='images', y_col='labels',\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        color_mode='rgb',\n",
    "                                                        class_mode='categorical',\n",
    "                                                        batch_size=64)\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "test_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255.0)\n",
    "test_generator = test_data_gen.flow_from_dataframe(dataframe=test,\n",
    "                                                   x_col='images', y_col='labels',\n",
    "                                                   target_size=(224, 224),\n",
    "                                                   color_mode='rgb',\n",
    "                                                   class_mode='categorical',\n",
    "                                                  batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e9776",
   "metadata": {},
   "source": [
    "# 2- Image classification using Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ccf10",
   "metadata": {},
   "source": [
    "We will first discuss the use of MLP in image classification problems. \n",
    "\n",
    "- MLP consists of **Input Layer** and the input must be 1D vector so the first thing we shoud do is to flatten the input images form 2D matrix to 1D vector to train the network. We will use keras Flatten layer that takes 2D image matrix and convert it  into a 1D vector. <br>\n",
    "\n",
    "\n",
    "- The second thing we add is the **hidden layers**, you can add as much as you want, but generally it depends on the complexity of the data and the task you have, and we eill choose the activation function, Relu function performs the best in Hidden layers. <br> \n",
    "\n",
    "\n",
    "- Finally the **output layer**. The number of neurons is equal to the number of classes we have, and the activation function  is Softmax since we have a multi-class classification  problem. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed6684",
   "metadata": {},
   "source": [
    "You can learn more about MLP at: \n",
    "    \n",
    "- <a href=\"https://www.kaggle.com/general/265058\">Multilayer Perceptron (MLP).</a>  <br>\n",
    "\n",
    "\n",
    "- <a href=\"https://www.kaggle.com/general/265073\">summary for Multilayer Perceptron (MLP) and backpropagation algorithm.</a> <br>\n",
    "\n",
    "\n",
    "- <a href=\"https://www.kaggle.com/general/265279\">Activation function, why do we need activation function in neural networks.</a> <br>\n",
    "\n",
    "\n",
    "- <a href=\"https://www.kaggle.com/general/265294\">Loss Function & Optimization.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db14228",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = tf.keras.models.Sequential()\n",
    "\n",
    "#Flatten layer\n",
    "mlp_model.add(tf.keras.layers.Flatten(input_shape=(224, 224, 3)))\n",
    "\n",
    "# 3 Hidden Layers with (256, 256, 128) neurons and relu activation function\n",
    "mlp_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "# dropout layer to reduce the overfitting \n",
    "mlp_model.add(tf.keras.layers.Dropout(0.4))\n",
    "mlp_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "mlp_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "# output layer with 9 neurons and softmax activation function\n",
    "mlp_model.add(tf.keras.layers.Dense(9, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee007c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can se the network using: \n",
    "\n",
    "tf.keras.utils.plot_model(mlp_model,\n",
    "                          show_shapes=True,\n",
    "                          show_dtype=True,\n",
    "                          show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59730537",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f46b85e",
   "metadata": {},
   "source": [
    "We can see the Trainable params which they are the parameters that will be adjusted and learned during the training process. <br>\n",
    "\n",
    "\n",
    "- The first hidden layer has (224 * 224*3 nodes in the input layer) * (256 nodes in this hidden layer) + 256 bias terms = 38535424 learnable parameters. <br>\n",
    "\n",
    "\n",
    "- The second Hidden layer has (256 nodes in the previous hidden layer) * (256 nodes in this hidden layer) + 256 bias terms =  **65792 learnable parameters.** <br>\n",
    "\n",
    "\n",
    "- The last hidden layer has (256 nodes in the previous hidden layer) * (128 nodes in this hidden layer) + 128 bias terms =  **32896 learnable parameters.**  <br>\n",
    "\n",
    "\n",
    "- Finally the output layer has (128 nodes in the previous hidden layer) * (9 nodes in the output layer) + 9 bias terms =  **1161 learnable parameters.** <br>\n",
    "\n",
    "\n",
    "The total number of Trainable params: **24,983,305** and this is a huge number üíî to such small neural network. What will happen if we add more and more layers or we resize the images to larger sizes, there will be tens of millions of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00257a1e",
   "metadata": {},
   "source": [
    "Compile the model you can learn more about loss function and optimization at: <a href=\"https://www.kaggle.com/general/265294\">Loss Function & Optimization.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ddb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='rmsprop',\n",
    "                 metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.fit(training_generator, \n",
    "             steps_per_epoch=24, \n",
    "             validation_data=validation_generator,\n",
    "             validation_steps=20,\n",
    "             epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f3348",
   "metadata": {},
   "source": [
    "We will loss the spatial features of the image  when we flattening the image to 2D vector, we will loss \n",
    "a lot of information and the network does not relate the pixel values to each other when it is trying to find patterns \n",
    "thats why we get a very bad accuracy when we use MLP in such problem. \n",
    "\n",
    "**Why??** \n",
    "\n",
    "\n",
    "**loss of information** ü§ï\n",
    "\n",
    "when we Flatten the image to be a 1D vector, the pixel values that present the fish üêü will be distributed in a certain way \n",
    "in the vector lets say in the left side of the image, if we have a new image that has the same object but in different \n",
    "location in the image, the neural network will not recognize it because different neurons need to fires in order to recognize \n",
    "the fish, the neural network will have no idea that this is the same fish. But why it does better than that on the MNIST data set, because MNIST data are well prepared  for this task. The MLP will not learn the fish shape. \n",
    "\n",
    "\n",
    "**very large number of parameters** ü§ï\n",
    "\n",
    "Another problem with the MLP is that it is an Fully connected layer, where every node in the layer is connected to all nodes \n",
    "of the previous layer and all nodes in the next layer. You saw that with this simple network we have more that 24 million parameters to learn, with more complex network and larger image size we will end up with billions of parameters to train and it is very computationally  expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bfe86",
   "metadata": {},
   "source": [
    "**Next we will use Convolutional neural networks to train the classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba361737",
   "metadata": {},
   "source": [
    "# 3- Image classification using Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0f21b",
   "metadata": {},
   "source": [
    "## 3.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a03c91",
   "metadata": {},
   "source": [
    "**Convolutional Neural Networks (CNN)** are locally connected, each node is connected to amall subset of the previous units.\n",
    "And it can learn the object features such as a car wheel or a cat nose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRwed5zvnSDt0zrFd_gf-kUIMoF7Nm6FXIwDw&usqp=CAU',\n",
    "      width=750,\n",
    "      height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71fd56",
   "metadata": {},
   "source": [
    "The Convolutional Neural Networks (CNN) has an input layer and a stack of hidden layers then the output layer just like the Fully Connected Network, the weights are randomly inialized, activation function is applied, we use back propagation and optimization algorithms to update the weights. The CNN does not need FLatten layer, it takes the whole image matrix as an input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f26a1c",
   "metadata": {},
   "source": [
    "The network consist of 2 main parts: \n",
    "\n",
    "1- Feature extraction part <br>\n",
    "2- classification part <br>\n",
    "\n",
    "- The image is passed to the CNN layers to extract features that called **feature map**, through this process the image dimension will shrinks and the number of feature  maps will increase. \n",
    "\n",
    "- The output of the final layer in the CNN is the feature that was extracted form the image and it represent the image and also this is what we are going to classify. Flatten layer used to flatten the feature map and then pass it to a fully connected layer for classification. \n",
    "\n",
    "- The earlier layers in the CNN will learn basic features such as edges and the later layers will learn more complex features such as a wheel of a car or the nose of a cat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e19520",
   "metadata": {},
   "source": [
    "So **to summarize** what we learn until now, we have an input image that passed directly to the network without flattening it, the the first part (the feature extractor) extract features that represent the input image and give us what we call it a feature map , and finally this feature map to the classifier that consists of Fully connected layers that perform the classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1959e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://www.researchgate.net/profile/Lavender-Jiang-2/publication/343441194/figure/fig2/AS:921001202311168@1596595206463/Basic-CNN-architecture-and-kernel-A-typical-CNN-consists-of-several-component-types.ppm',\n",
    "      width=750,\n",
    "      height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da341c1",
   "metadata": {},
   "source": [
    "So as I saied the network consists of 2 main parts: the CNN and the FCN \n",
    "    \n",
    "The CNN consist of **two main components:**\n",
    "    \n",
    "    1- Convolutional Layer (CONV)\n",
    "    2- Pooling Layer  (POOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996edb1c",
   "metadata": {},
   "source": [
    "**INPUT   >   CONV   >   POOL   >   CONV   >   POOL   >   CONV   >   POOL >  FC >   FC (softmax)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927a37d",
   "metadata": {},
   "source": [
    "So the full architecture consists of Convolutional Layers , Pooling Layers and Fully Connected Layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8c064",
   "metadata": {},
   "source": [
    "## 3.2- Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28545b1",
   "metadata": {},
   "source": [
    "Convolutional Layer is the basis of the Convolutional Networks, the units here is the **filters** and also called kernels and they works by sliding the filter over the input image, apply some processing at each location and store the result in the new processed image as we see in the figure below. \n",
    "\n",
    "In CNN these filters are the weights, their values are intialized randomly and learned during the training process. The area of the image that the filter process on is called receptive field. \n",
    "\n",
    "\n",
    "The math here is very simple, at each location the filter stop we multiply each pixel value in the receptive field by the corresponding pixel in the filter and then sum them to get the value of the center pixel in the new image as we see in the figure below. \n",
    "\n",
    "If the images are colored lets say with size of (520, 520, 3) where 3 is the number of channels (Red, Green and Blue (RGB)) the filter also will be (3, 3, 3) where the last 3 is number of channels in the input image. We have filter of size (3,3) for each channel in the input image and for each channel we do the same calculation then add the results of the filter processing on the 3 channels. \n",
    "\n",
    "In sime application colors are very important to identify objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a75b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://i.stack.imgur.com/CQtHP.gif',\n",
    "      width=750,\n",
    "      height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cb66c",
   "metadata": {},
   "source": [
    "Filters are parameters that learned during the training process. some filters can detect vertical edges while other might \n",
    "detect horizontal edges and other detect different things. \n",
    "\n",
    "so we can see that each filter produce its own feature map, the conv layer has many of those filters so the number of filters\n",
    "in the conv layer determine the number of feature maps that produced after applying this layer on the input. And the number \n",
    "of filters represent the depth of the output of the layer. As we increase the number of filters, the complexity of the network\n",
    "increase and enable use to detect more complex features. \n",
    "\n",
    "\n",
    "This kernel is just a matrix of weights that  are learned during the training process this filter is works by sliding over\n",
    "the image to extract features. kernels are almost always squares and can have different sizes (3x3), (5x5), (7x7) it is an\n",
    "hyperparameter that you can tune and the performance will be different based on the problem that you are solving. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883c5df",
   "metadata": {},
   "source": [
    "**Strides and Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://miro.medium.com/max/658/0*jLoqqFsO-52KHTn9.gif',\n",
    "      width=750,\n",
    "      height=500)\n",
    "\n",
    "#Source: https://towardsdatascience.com/cnn-part-i-9ec412a14cb1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2188b",
   "metadata": {},
   "source": [
    "As we can see from the figure above the conv operation output is smaller that the input, we can control the shape \n",
    "of the output of this operation  by the **stride and padding**\n",
    "\n",
    "Stride is the amount by which the filter slides  over the image, for example if stride = 1 then the filter will slide one pixel at a time, and if stride = 2 it will slide 2 pixel at a time. \n",
    "\n",
    "\n",
    "padding: We add zeros around the border of the image to preserve the spatial size of the image so we can train a deeper network and prevent losing  information from the edges of the image. \n",
    "\n",
    "the values for padding is either  2: 'same' where we add zeros around the image border in which the size of the output image is the same as the input image. 2: 'valid' which means without padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://res.cloudinary.com/practicaldev/image/fetch/s--nUoflRuG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.ibb.co/kG5vPdn/final-cnn.png',\n",
    "      width=750,\n",
    "      height=500)\n",
    "\n",
    "#Source: https://towardsdatascience.com/cnn-part-i-9ec412a14cb1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73160c29",
   "metadata": {},
   "source": [
    "So **to summarize** what we learn until now, the first component  of the CNN is the Conv layer, in this layer we have a lot of hyperparameters that we can tune to control the output of it. The Conv layers contain filters (kernels) that contain the weights that learned during training, these filters works by sliding them over the image and each filter detect a certain  feature, where the filters in the first layers detect simple things like edges and when we go deeper in the network, the filters will detect more complex features. Each filter in the layer produce a feature map and the feature maps of all filters in the layer are concatenated and this is the output of the layer. to control the shape of th output of the layer we have two additional hyperparameters, the stride which control the amount by which the filter slides  over the image and the padding where we add zeros around the border of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65db61",
   "metadata": {},
   "source": [
    "Example: if the input image is 224,224,3 to a Conv layer with 64 filters of size (3,3), stride of 1, and no padding ('valid'). The output will be of size {(224 + 0 -3 /1) +1 = 222} (222, 222, 64) where the 3 in the input image size is the RGB channels and the 64 in the output means 64 feature map produced by the 64 filters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641d2cd",
   "metadata": {},
   "source": [
    "## 3.3- Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145bc11",
   "metadata": {},
   "source": [
    "As we go deeper in the ConvNet the number of the feature maps (depth) is increase which lead to the increase to the number \n",
    "of parameters that will increase the computational  power and memory needed. \n",
    "\n",
    "Pooling layer reduce the size of the feature map (not the depth it reduce the height and the width) with two types of pooling\n",
    "max pooling and Average pooling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://nico-curti.github.io/NumPyNet/NumPyNet/images/maxpool.gif',\n",
    "      width=750,\n",
    "      height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808f522",
   "metadata": {},
   "source": [
    "In pooling we also have a window with specified size that sliding on the feature map but without weights, the window take the \n",
    "max pixel value and ignore the rest values in max pooling, or take the average of the values in average pooling, so the pooling\n",
    "layer help us to keep the important information and pass them to the next layer. \n",
    "\n",
    "**Where we put the pooling layers in the CNN?**\n",
    "\n",
    "usually pooling layer are placed after one or two convolutional  layers.\n",
    "\n",
    "the output of the final pooling layer after a sequence  of conv and pool layers will be for example (7, 7, 120) where the 120 id the depth (number of feature maps) now this output is ready for classification but first we need to flatten it then feeding it to the fully connected layers. \n",
    "\n",
    "Pooling layers don't  have any parameters to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c407972",
   "metadata": {},
   "source": [
    "## 3.4- Fully Conected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db3699",
   "metadata": {},
   "source": [
    "We pass the image through our feature extractor that composed of convolutional and pooling layer and give us the feature \n",
    "that we will pass through the FCL for classification. \n",
    "\n",
    "in This stage, we flatten the output of the feature extractor lets say for example its (7, 7, 40) wen we flattening it we get a vector\n",
    "of size 7*7*40 = 1960, then we feed this vector to a fully connected  layers (we can use for example one layer with 256 units\n",
    "with relu activation function and other layer with 9 units with softmax activation function for classification)\n",
    "\n",
    "the number of units in the last layer will equal to the number of classes where each node represents  the probability of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c13b80",
   "metadata": {},
   "source": [
    "## 3.5 Image classification using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model \n",
    "cnn_model = tf.keras.models.Sequential()\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "#Conv layer: 32 filters of size (3, 3), withstrides = 1 and relu activation\n",
    "cnn_model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=1, \n",
    "                                     activation='relu', input_shape=(224,224,3)))\n",
    "#max-poolig layer with pool_size of (2,2)\n",
    "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "#Conv layer: 64 filters of size (3, 3), withstrides = 1 and relu activation\n",
    "cnn_model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=1, \n",
    "                                    activation='relu'))\n",
    "#max-poolig layer with pool_size of (2,2)\n",
    "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "#Conv layer: 128 filters of size (3, 3), withstrides = 1 and relu activation\n",
    "cnn_model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=1, \n",
    "                                    activation='relu'))\n",
    "#max-poolig layer with pool_size of (2,2)\n",
    "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "#Conv layer: 128 filters of size (3, 3), withstrides = 1 and relu activation\n",
    "cnn_model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=1, \n",
    "                                    activation='relu'))\n",
    "#max-poolig layer with pool_size of (2,2)\n",
    "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "#Conv layer: 128 filters of size (3, 3), withstrides = 1 and relu activation\n",
    "cnn_model.add(tf.keras.layers.Conv2D(256, kernel_size=(3, 3), strides=1, \n",
    "                                    activation='relu'))\n",
    "#max-poolig layer with pool_size of (2,2)\n",
    "cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "#Flattening the output of the last pooling layer \n",
    "cnn_model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#cnn_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "\n",
    "#Fully connected layer with 256 units and relu activation\n",
    "cnn_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "#Dropout layer to lower the overfitting with dropuot rate of rate 0.4\n",
    "cnn_model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "#Fully connected layer with 9 units and softmax activation\n",
    "cnn_model.add(tf.keras.layers.Dense(9, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df120e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379747c1",
   "metadata": {},
   "source": [
    "As we can see the number of parameters is reduced from 38,635,273 in the MLP to 2,066,313 in CNN. reducing the parameters to 5.2% of the number of parameters in the MLP !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81608719",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='rmsprop',\n",
    "                 metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_model.fit(training_generator, \n",
    "                         steps_per_epoch=99, \n",
    "                         validation_data=validation_generator,\n",
    "                         validation_steps=20,\n",
    "                         epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360a23b",
   "metadata": {},
   "source": [
    "As we can see we get a much better accuracy when we use the convolutional network \n",
    "\n",
    "we can get much better accuracy when we use the transfer learning which will be the nect topic to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6799a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_loss = history.history['loss']\n",
    "cnn_val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "plt.plot(history.epoch, cnn_train_loss, label='Training Loss')\n",
    "plt.plot(history.epoch, cnn_val_loss, label='Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(history.epoch, train_acc, label='Training Accuracy')\n",
    "plt.plot(history.epoch, val_acc, label='Validation Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f75cc",
   "metadata": {},
   "source": [
    "**In the next notebook I will explain the transfer learning in computer vision problems.**\n",
    "\n",
    "\n",
    "**Thank you for reading, I hope you enjoyed and benefited from it.**\n",
    "\n",
    "**If you have any questions or notes please leave it in the commont section.**\n",
    "\n",
    "**If you like this notebook please press upvote and thanks again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a75577",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
