{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036f1b13",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a670ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a947ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import spatial\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from timm.utils import AverageMeter\n",
    "import sys\n",
    "sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import wandb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abfb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models('convnext*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39497afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models('vit_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models('swin*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8bd53c",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'vit_large_patch14_224_clip_laion2b', 'swin_large_patch4_window12_384', 'vit_base_patch16_224'\n",
    "\n",
    "class CFG:\n",
    "    model_name = 'swinv2_cr_large_224' #'vit_huge_patch14_224_clip_laion2b'\n",
    "    input_size = 224\n",
    "    batch_size = 64\n",
    "    num_epochs = 3\n",
    "    lr = 1e-4 #5e-5\n",
    "    seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3666997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75853f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_api = user_secrets.get_secret(\"wandb_api\")\n",
    "wandb.login(key=wandb_api)\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "run = wandb.init(project='Stable Diffusion ViT Baseline Train', \n",
    "             name=CFG.model_name,\n",
    "             config=class2dict(CFG),\n",
    "             group=CFG.model_name,\n",
    "             job_type=\"train\",\n",
    "             anonymous=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fc28e",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['filepath'])\n",
    "        image = self.transform(image)\n",
    "        prompt = row['prompt']\n",
    "        return image, prompt\n",
    "\n",
    "\n",
    "class DiffusionCollator:\n",
    "    def __init__(self):\n",
    "        self.st_model = SentenceTransformer(\n",
    "            '/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2',\n",
    "            device='cpu'\n",
    "        )\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        images, prompts = zip(*batch)\n",
    "        images = torch.stack(images)\n",
    "        prompt_embeddings = self.st_model.encode(\n",
    "            prompts, \n",
    "            show_progress_bar=False, \n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        return images, prompt_embeddings\n",
    "    \n",
    "    \n",
    "def get_dataloaders(\n",
    "    trn_df,\n",
    "    val_df,\n",
    "    input_size,\n",
    "    batch_size\n",
    "):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #transforms.RandomRotation(degrees=10),\n",
    "        #transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    trn_dataset = DiffusionDataset(trn_df, transform)\n",
    "    val_dataset = DiffusionDataset(val_df, transform)\n",
    "    collator = DiffusionCollator()\n",
    "    \n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = DataLoader(\n",
    "        dataset=trn_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True,\n",
    "        collate_fn=collator\n",
    "    )\n",
    "    dataloaders['val'] = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False,\n",
    "        collate_fn=collator\n",
    "    )\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561d365",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(y_trues, y_preds):\n",
    "    return np.mean([\n",
    "        1 - spatial.distance.cosine(y_true, y_pred) \n",
    "        for y_true, y_pred in zip(y_trues, y_preds)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    trn_df,\n",
    "    val_df,\n",
    "    model_name,\n",
    "    input_size,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    lr\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataloaders = get_dataloaders(\n",
    "        trn_df,\n",
    "        val_df,\n",
    "        input_size,\n",
    "        batch_size\n",
    "    )\n",
    "\n",
    "    _model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=384\n",
    "    )\n",
    "    _model.set_grad_checkpointing()\n",
    "    model = _model\n",
    "    #model = nn.DataParallel(_model)\n",
    "    model.to(device)\n",
    "    \n",
    "    wandb.watch(model, log_freq=100)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    ttl_iters = num_epochs * len(dataloaders['train'])\n",
    "    #scheduler = CosineAnnealingLR(optimizer, T_max=ttl_iters, eta_min=1e-7)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=ttl_iters, eta_min=1e-6)\n",
    "    criterion = nn.CosineEmbeddingLoss()\n",
    "    \n",
    "    save_start_step = 1000\n",
    "    save_step = 200\n",
    "    best_score_trn = -1.0\n",
    "    best_score = -1.0\n",
    "    step = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        train_meters = {\n",
    "            'loss': AverageMeter(),\n",
    "            'cos': AverageMeter(),\n",
    "        }\n",
    "        model.train()\n",
    "        for X, y in tqdm(dataloaders['train'], leave=False):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            X_out = model(X)\n",
    "            target = torch.ones(X.size(0)).to(device)\n",
    "            loss = criterion(X_out, y, target)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            trn_loss = loss.item()\n",
    "            trn_cos = cosine_similarity(\n",
    "                X_out.detach().cpu().numpy(), \n",
    "                y.detach().cpu().numpy()\n",
    "            )\n",
    "            \n",
    "            wandb.log({\n",
    "                \"step\": step, \n",
    "                \"loss\": trn_loss, \n",
    "                \"cos\": trn_cos\n",
    "            })\n",
    "\n",
    "            train_meters['loss'].update(trn_loss, n=X.size(0))\n",
    "            train_meters['cos'].update(trn_cos, n=X.size(0))\n",
    "            step += 1\n",
    "            \n",
    "            if step >= save_start_step and step % save_step == 0 and best_score_trn < train_meters['cos'].avg:\n",
    "                best_score_trn = train_meters['cos'].avg\n",
    "                torch.save(model.state_dict(), f'{model_name}_trn_{step}.pth')\n",
    "\n",
    "        print('Epoch {:d} / trn/loss={:.4f}, trn/cos={:.4f}'.format(\n",
    "            epoch + 1,\n",
    "            train_meters['loss'].avg,\n",
    "            train_meters['cos'].avg))\n",
    "\n",
    "        val_meters = {\n",
    "            'loss': AverageMeter(),\n",
    "            'cos': AverageMeter(),\n",
    "        }\n",
    "        model.eval()\n",
    "        for X, y in tqdm(dataloaders['val'], leave=False):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                X_out = model(X)\n",
    "                target = torch.ones(X.size(0)).to(device)\n",
    "                loss = criterion(X_out, y, target)\n",
    "\n",
    "                val_loss = loss.item()\n",
    "                val_cos = cosine_similarity(\n",
    "                    X_out.detach().cpu().numpy(), \n",
    "                    y.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "            val_meters['loss'].update(val_loss, n=X.size(0))\n",
    "            val_meters['cos'].update(val_cos, n=X.size(0))\n",
    "\n",
    "        print('Epoch {:d} / val/loss={:.4f}, val/cos={:.4f}'.format(\n",
    "            epoch + 1,\n",
    "            val_meters['loss'].avg,\n",
    "            val_meters['cos'].avg))\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch + 1, \n",
    "            \"train_loss\": train_meters['loss'].avg, \n",
    "            \"train_cos\": train_meters['cos'].avg,\n",
    "            \"val_loss\": val_meters['loss'].avg,\n",
    "            \"val_cos\": val_meters['cos'].avg\n",
    "        })\n",
    "        \n",
    "        if val_meters['cos'].avg > best_score:\n",
    "            best_score = val_meters['cos'].avg\n",
    "            torch.save(model.state_dict(), f'{model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/diffusiondb-data-cleansing/diffusiondb.csv')\n",
    "trn_df, val_df = train_test_split(df, test_size=0.1, random_state=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5befc309",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(trn_df, val_df, CFG.model_name, CFG.input_size, CFG.batch_size, CFG.num_epochs, CFG.lr)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
