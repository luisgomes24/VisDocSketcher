{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf323a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "global_start_t = time.time()\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54187f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge\n",
    "!cp -r ../input/mrc-baidu-pytorch-baseline-ga/pytorch_baseline_ga/*  ./\n",
    "!cp -r ../input/baidu-mrc-2021-competition-data/data  ./\n",
    "!mkdir ./log\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_train_dev_json():    \n",
    "    print('in detect_train_dev_json()')\n",
    "    \n",
    "    json_file_path = '../input/baidu-mrc-2021-competition-data/data/train.json'\n",
    "    train_fileJson = json.load(open(json_file_path))\n",
    "    json_file_path = '../input/baidu-mrc-2021-competition-data/data/dev.json'\n",
    "    dev_fileJson = json.load(open(json_file_path))\n",
    "    train_fileJson['data'][0]['paragraphs'] += dev_fileJson['data'][0]['paragraphs']\n",
    "    train_fileJson['data'][0]['paragraphs'] += dev_fileJson['data'][1]['paragraphs']\n",
    "    print('len of merged data: ', len(train_fileJson['data'][0]['paragraphs']))\n",
    "    \n",
    "    paragraphs_lst = train_fileJson['data'][0]['paragraphs']\n",
    "    ids_lst = []\n",
    "    for para in paragraphs_lst:\n",
    "        qid = para['qas'][0]['id']\n",
    "        if qid in ids_lst:\n",
    "            print('found duplicate qid is', qid)\n",
    "        ids_lst.append(qid)\n",
    "\n",
    "    print('merge_train_dev_json finished!')\n",
    "    \n",
    "detect_train_dev_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from dataset.dataset import MrcDataset\n",
    "from dataset.dataset import collate_fn\n",
    "from models.model import MRC_model, MRC_model_tsg\n",
    "from metric.metric4merge import compute_prediction_checklist_for_merge\n",
    "from run import evaluate\n",
    "from utils.finetuning_argparse import get_argparse\n",
    "from utils.utils import seed_everything, init_logger, logger\n",
    "\n",
    "from utils.evaluate_baidu import evaluate as evaluate_baidu\n",
    "from utils.evaluate_baidu import read_mrc_dataset, read_model_prediction\n",
    "\n",
    "def write_to_json(json_data, file_name):\n",
    "    # 写入data文件\n",
    "    objects = json.dumps(json_data, indent=4, ensure_ascii=False)\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write(objects)\n",
    "\n",
    "def softmax_func(x, return_list=False):\n",
    "    if type(x)==list:\n",
    "        x = np.array(x)\n",
    "    exp_x = np.exp(x)\n",
    "    x = exp_x / np.sum(exp_x)\n",
    "    if return_list:\n",
    "        return list(x)\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def merge_dev_outcome(args):\n",
    "    print('in merge_dev_outcome()')\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "        \n",
    "    dev_json_files = [\n",
    "        '../input/mrc-baidu-baseline-roberta-fold4-cv0-2027/roberta_large_output/dev_cv0_predictions_best.json',\n",
    "        '../input/mrc-baidu-baseline-roberta-fold4-cv1-2027/roberta_large_output/dev_cv1_predictions_best.json',\n",
    "        '../input/mrc-baidu-baseline-roberta-fold4-cv2-2027/roberta_large_output/dev_cv2_predictions_best.json',\n",
    "        '../input/mrc-baidu-baseline-roberta-fold4-cv3-2027/roberta_large_output/dev_cv3_predictions_best.json',\n",
    "    ]\n",
    "    dev_outcome_merged = {}\n",
    "    for dev_json_file in dev_json_files:\n",
    "        dev_fileJson = json.load(open(dev_json_file))\n",
    "        dev_outcome_merged.update(dev_fileJson)\n",
    "    print('len of dev_outcome_merged is ', len(dev_outcome_merged))\n",
    "    \n",
    "    dev_outcome_merged_file = os.path.join(args.output_dir, 'dev_predictions_merged.json')\n",
    "    with open(dev_outcome_merged_file, \"w\", encoding='utf-8') as writer:\n",
    "        writer.write(json.dumps(dev_outcome_merged, ensure_ascii=False, indent=4) + \"\\n\")\n",
    "    \n",
    "    ref_ans = read_mrc_dataset(os.path.join(args.output_dir, 'tain_dev_merged.json'))\n",
    "    pred_ans = read_model_prediction(dev_outcome_merged_file)\n",
    "    #print('before F1, EM, TOTAL, SKIP = base_evaluate(ref_ans, pred_ans, False)')\n",
    "    baidu_F1, baidu_EM, TOTAL, SKIP = evaluate_baidu(ref_ans, pred_ans, False)\n",
    "    \n",
    "    print(f'baidu_F1: {baidu_F1:.5f}, baidu_EM: {baidu_EM:.5f}')\n",
    "\n",
    "def merge_train_dev_json(args):    \n",
    "    print('in merge_train_dev_json()')\n",
    "    \n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "        \n",
    "    json_file_path = '../input/baidu-mrc-2021-competition-data/data/train.json'\n",
    "    train_fileJson = json.load(open(json_file_path))\n",
    "    json_file_path = '../input/baidu-mrc-2021-competition-data/data/dev.json'\n",
    "    dev_fileJson = json.load(open(json_file_path))\n",
    "    train_fileJson['data'][0]['paragraphs'] += dev_fileJson['data'][0]['paragraphs']\n",
    "    train_fileJson['data'][0]['paragraphs'] += dev_fileJson['data'][1]['paragraphs']\n",
    "    print('len of merged data: ', len(train_fileJson['data'][0]['paragraphs']))\n",
    "    \n",
    "    write_to_json(train_fileJson, os.path.join(args.output_dir, 'tain_dev_merged.json'))\n",
    "    print('merge_train_dev_json finished!')\n",
    "    \n",
    "def merge_outcome(data_loader, args, prefix=\"\"):\n",
    "    print('from tusonggao, in merge_outcome()')\n",
    "\n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "    all_cls_logits = []\n",
    "\n",
    "    pkl_files = [\n",
    "                 '../input/mrc-baidu-baseline-roberta-fold4-cv0-2027/roberta_large_output/test1_predictions_detail.pkl',\n",
    "                 '../input/mrc-baidu-baseline-roberta-fold4-cv1-2027/roberta_large_output/test1_predictions_detail.pkl',\n",
    "                 '../input/mrc-baidu-baseline-roberta-fold4-cv2-2027/roberta_large_output/test1_predictions_detail.pkl',\n",
    "                 '../input/mrc-baidu-baseline-roberta-fold4-cv3-2027/roberta_large_output/test1_predictions_detail.pkl',\n",
    "    ]\n",
    "    print('len of pkl_files is ', len(pkl_files))\n",
    "    \n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    print('from tusonggao, starts now!')\n",
    "    all_start_logits, all_end_logits, all_cls_logits = [], [], []\n",
    "    for pkl_file in pkl_files:\n",
    "        dump_detial = pickle.load(open(pkl_file, \"rb\"))\n",
    "        all_start_logits_part = dump_detial['all_start_logits']\n",
    "        all_end_logits_part = dump_detial['all_end_logits']\n",
    "        all_cls_logits_part = dump_detial['all_cls_logits']\n",
    "        print('for pkl_file: ', pkl_file, 'len of all_start_logits_part: ', len(all_start_logits_part))\n",
    "        assert len(all_start_logits_part)==len(all_end_logits_part)==len(all_cls_logits_part), 'should all be the same length'\n",
    "        all_start_logits_part = [softmax_func(all_start_logits_part[i]) for i in range(len(all_start_logits_part))]\n",
    "        all_end_logits_part = [softmax_func(all_end_logits_part[i]) for i in range(len(all_end_logits_part))]\n",
    "        all_cls_logits_part = [softmax_func(all_cls_logits_part[i]) for i in range(len(all_cls_logits_part))]\n",
    "\n",
    "        if all_start_logits==[]:\n",
    "            all_start_logits = all_start_logits_part\n",
    "            all_end_logits = all_end_logits_part\n",
    "            all_cls_logits = all_cls_logits_part\n",
    "        else:\n",
    "            for i in range(len(all_start_logits)):\n",
    "                assert all_start_logits[i].shape==all_start_logits_part[i].shape, 'shape should be the same'\n",
    "                all_start_logits[i] = all_start_logits[i] + all_start_logits_part[i]\n",
    "                all_end_logits[i] = all_end_logits[i] + all_end_logits_part[i]\n",
    "                all_cls_logits[i] = all_cls_logits[i] + all_cls_logits_part[i]\n",
    "\n",
    "    for i in range(len(all_start_logits)):\n",
    "        all_start_logits[i] = all_start_logits[i] / len(pkl_files)\n",
    "        all_end_logits[i] = all_end_logits[i] / len(pkl_files)\n",
    "        all_cls_logits[i] = all_cls_logits[i] / len(pkl_files)\n",
    "        \n",
    "    print('from tusonggao, ends now!')\n",
    "    \n",
    "    merged_predictions_detail = {'all_start_logits': all_start_logits,\n",
    "                                 'all_end_logits': all_end_logits,\n",
    "                                 'all_cls_logits': all_cls_logits}\n",
    "    merged_detail_dump_filename = os.path.join(args.output_dir, prefix+'_predictions_detail_merged.pkl')\n",
    "    pickle.dump(merged_predictions_detail, open(merged_detail_dump_filename, 'wb'), protocol=4)\n",
    "    print('dump merged_predictions_detail file succeed now!')\n",
    "\n",
    "    print('from tusonggao len of all_start_logits: ', len(all_start_logits))\n",
    "    print('from tusonggao len of all_end_logits: ', len(all_end_logits))\n",
    "    print('from tusonggao len of all_cls_logits: ', len(all_cls_logits))\n",
    "    assert len(all_start_logits)==len(all_end_logits)==len(all_cls_logits), 'should all be the same length 222'\n",
    "\n",
    "    print('from tusonggao args.max_answer_length is ', args.max_answer_length) \n",
    "    all_predictions, all_nbest_json, all_cls_predictions = compute_prediction_checklist_for_merge(\n",
    "        data_loader.dataset.examples, \n",
    "        data_loader.dataset.tokenized_examples,\n",
    "        (all_start_logits, all_end_logits,all_cls_logits), \n",
    "        True, \n",
    "        args.n_best_size,\n",
    "        args.max_answer_length, \n",
    "        args.cls_threshold\n",
    "    )\n",
    "\n",
    "    print('from tusonggao dump merged file now')\n",
    "    with open(os.path.join(args.output_dir,prefix+'_predictions_merged.json'), \"w\", encoding='utf-8') as writer:\n",
    "        writer.write(\n",
    "            json.dumps(\n",
    "                all_predictions, ensure_ascii=False, indent=4) + \"\\n\")\n",
    "    print('from tusonggao dump merged file succeed now!')\n",
    "\n",
    "def main(args):\n",
    "    print('in main()')\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    seed_everything(args.seed)\n",
    "\n",
    "    print('from tusonggao, args.cls_threshold: ', args.cls_threshold)\n",
    "    print('from tusonggao, args.cls_threshold: ', args.test_file_name)\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "    print('from tusonggao got test_iter start now!')\n",
    "    # Dataset & Dataloader\n",
    "    test_dataset = MrcDataset(args,\n",
    "                              json_path=args.test_file_name,\n",
    "                              tokenizer=tokenizer)\n",
    "    test_iter = DataLoader(test_dataset,\n",
    "                           shuffle=False,\n",
    "                           batch_size=args.per_gpu_eval_batch_size,\n",
    "                           collate_fn=collate_fn,\n",
    "                           num_workers=24)\n",
    "    print('from tusonggao got test_iter succeed now!')\n",
    "\n",
    "    logger.info(\"The nums of the test_dataset examples is {}\".format(len(test_dataset.examples)))\n",
    "    logger.info(\"The nums of the test_dataset features is {}\".format(len(test_dataset)))\n",
    "\n",
    "    merge_outcome(test_iter, args, prefix=\"test\")\n",
    "\n",
    "class CFG:\n",
    "    cls_threshold = 0.6\n",
    "    test_file_name = './data/test1.json'\n",
    "    model_name_or_path = 'hfl/chinese-roberta-wwm-ext-large'\n",
    "    per_gpu_eval_batch_size = 2\n",
    "    output_dir = './merged_outcome'\n",
    "    max_len = 512\n",
    "    max_answer_length = 150\n",
    "    seed = 66\n",
    "    n_best_size = 5\n",
    "    stride = 100\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    args = CFG()\n",
    "    \n",
    "    merge_train_dev_json(args)\n",
    "    merge_dev_outcome(args)\n",
    "    main(args)\n",
    "    print('finished now!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c72895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./merged_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nfinished now, total cost time: ', time.time() - global_start_t)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
