{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecae631",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I will add some simple features I checked in my **data analysis kernel [Google QUEST: First data introduction](https://www.kaggle.com/corochann/google-quest-first-data-introduction)**.\n",
    "\n",
    "Except that, most of the code is just copy from [https://www.kaggle.com/hukuda222/tfidf-swem-approach](https://www.kaggle.com/hukuda222/tfidf-swem-approach) by @hukuda222.\n",
    "This kernel is based on TFIDF+NN model(https://www.kaggle.com/ryches/tfidf-benchmark ).\n",
    "\n",
    "> I will add new information to TFIDF+NN model(https://www.kaggle.com/ryches/tfidf-benchmark ).<br>\n",
    "> TFIDF can create features based on actual vocabulary, but it can't handle well when there is another word of close meaning.<br>\n",
    "> Therefore, I thought that adding SWEM(https://arxiv.org/abs/1805.09843) using learned word2vec as a feature value would increase the score.\n",
    "\n",
    "Since I only added some codes from forked kernel, **please upvote original kernel as well :)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d27155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "from scipy.stats import spearmanr\n",
    "from nltk.corpus import wordnet as wn\n",
    "import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f538564",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\n",
    "test = pd.read_csv(\"../input/google-quest-challenge/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45542ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b000f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11156d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['question_asker_intent_understanding',\n",
    "       'question_body_critical', 'question_conversational',\n",
    "       'question_expect_short_answer', 'question_fact_seeking',\n",
    "       'question_has_commonly_accepted_answer',\n",
    "       'question_interestingness_others', 'question_interestingness_self',\n",
    "       'question_multi_intent', 'question_not_really_a_question',\n",
    "       'question_opinion_seeking', 'question_type_choice',\n",
    "       'question_type_compare', 'question_type_consequence',\n",
    "       'question_type_definition', 'question_type_entity',\n",
    "       'question_type_instructions', 'question_type_procedure',\n",
    "       'question_type_reason_explanation', 'question_type_spelling',\n",
    "       'question_well_written', 'answer_helpful',\n",
    "       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "       'answer_satisfaction', 'answer_type_instructions',\n",
    "       'answer_type_procedure', 'answer_type_reason_explanation',\n",
    "       'answer_well_written']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8db8c8",
   "metadata": {},
   "source": [
    "# Adding simple feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_count(s):\n",
    "    return len(s)\n",
    "\n",
    "def word_count(s):\n",
    "    return s.count(' ')\n",
    "\n",
    "train['question_title_n_chars'] = train['question_title'].apply(char_count)\n",
    "train['question_title_n_words'] = train['question_title'].apply(word_count)\n",
    "train['question_body_n_chars'] = train['question_body'].apply(char_count)\n",
    "train['question_body_n_words'] = train['question_body'].apply(word_count)\n",
    "train['answer_n_chars'] = train['answer'].apply(char_count)\n",
    "train['answer_n_words'] = train['answer'].apply(word_count)\n",
    "\n",
    "test['question_title_n_chars'] = test['question_title'].apply(char_count)\n",
    "test['question_title_n_words'] = test['question_title'].apply(word_count)\n",
    "test['question_body_n_chars'] = test['question_body'].apply(char_count)\n",
    "test['question_body_n_words'] = test['question_body'].apply(word_count)\n",
    "test['answer_n_chars'] = test['answer'].apply(char_count)\n",
    "test['answer_n_words'] = test['answer'].apply(word_count)\n",
    "\n",
    "train['question_body_n_chars'].clip(0, 5000, inplace=True)\n",
    "test['question_body_n_chars'].clip(0, 5000, inplace=True)\n",
    "train['question_body_n_words'].clip(0, 1000, inplace=True)\n",
    "test['question_body_n_words'].clip(0, 1000, inplace=True)\n",
    "\n",
    "train['answer_n_chars'].clip(0, 5000, inplace=True)\n",
    "test['answer_n_chars'].clip(0, 5000, inplace=True)\n",
    "train['answer_n_words'].clip(0, 1000, inplace=True)\n",
    "test['answer_n_words'].clip(0, 1000, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aaffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_question = train['question_user_name'].value_counts()\n",
    "num_answer = train['answer_user_name'].value_counts()\n",
    "\n",
    "train['num_answer_user'] = train['answer_user_name'].map(num_answer)\n",
    "train['num_question_user'] = train['question_user_name'].map(num_question)\n",
    "test['num_answer_user'] = test['answer_user_name'].map(num_answer)\n",
    "test['num_question_user'] = test['question_user_name'].map(num_question)\n",
    "\n",
    "# map is done by train data, we need to fill value for user which does not appear in train data...\n",
    "test['num_answer_user'].fillna(1, inplace=True)\n",
    "test['num_question_user'].fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_feature_cols = [\n",
    "    'question_title_n_chars', 'question_title_n_words', 'question_body_n_chars', 'question_body_n_words',\n",
    "    'answer_n_chars', 'answer_n_words', 'num_answer_user', 'num_question_user'\n",
    "]\n",
    "simple_engineered_feature = train[simple_feature_cols].values\n",
    "simple_engineered_feature_test = test[simple_feature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "simple_engineered_feature = scaler.fit_transform(simple_engineered_feature)\n",
    "simple_engineered_feature_test = scaler.transform(simple_engineered_feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cdc0ff",
   "metadata": {},
   "source": [
    "# other feature engineering\n",
    "\n",
    "Below is just a copy of https://www.kaggle.com/hukuda222/tfidf-swem-approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_prepro(s):\n",
    "    return [w for w in s.replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n",
    "            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n",
    "            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_prepro_tfidf(s):\n",
    "    return \" \".join([w for w in s.lower().replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n",
    "            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n",
    "            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a3ece",
   "metadata": {},
   "source": [
    "This is basic preprocessing. This time, symbols and words are attached, so they are separated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_max = max([len(simple_prepro(l)) for l in list(train[\"question_title\"].values)])\n",
    "qb_max = max([len(simple_prepro(l))  for l in list(train[\"question_body\"].values)])\n",
    "an_max = max([len(simple_prepro(l))  for l in list(train[\"answer\"].values)])\n",
    "print(\"max lenght of question_title is\",qt_max)\n",
    "print(\"max lenght of question_body is\",qb_max)\n",
    "print(\"max lenght of question_answer is\",an_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa98f2a",
   "metadata": {},
   "source": [
    "The text is so long that it is difficult to apply RNN to all series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e3fef",
   "metadata": {},
   "source": [
    "Here we use a trained word2vec model that is easily available with nltk.<br>\n",
    "We used SWEM with max pooling.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16481ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(text):\n",
    "    np.random.seed(abs(hash(text)) % (10 ** 8))\n",
    "    words = simple_prepro(text)\n",
    "    vectors = np.zeros((len(words),100))\n",
    "    if len(words)==0:\n",
    "        vectors = np.zeros((1,100))\n",
    "    for i,word in enumerate(simple_prepro(text)):\n",
    "        try:\n",
    "            vectors[i]=w2v_model[word]\n",
    "        except:\n",
    "            vectors[i]=np.random.uniform(-0.01, 0.01,100)\n",
    "            #np.array([len(text)/5000,len(words)/1000,text.count(\"\\n\")/10])]\n",
    "    return np.max(np.array(vectors), axis=0)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f93b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_title = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_title\"].values)]\n",
    "question_title_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_title\"].values)]\n",
    "\n",
    "question_body = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_body\"].values)]\n",
    "question_body_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_body\"].values)]\n",
    "\n",
    "answer = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"answer\"].values)]\n",
    "answer_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"answer\"].values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809b4f8",
   "metadata": {},
   "source": [
    "From here on, I'm quite referring to https://www.kaggle.com/ryches/tfidf-benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ccadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 3))\n",
    "tsvd = TruncatedSVD(n_components = 50)\n",
    "tfidf_question_title = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_title\"].values)])\n",
    "tfidf_question_title_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_title\"].values)])\n",
    "tfidf_question_title = tsvd.fit_transform(tfidf_question_title)\n",
    "tfidf_question_title_test = tsvd.transform(tfidf_question_title_test)\n",
    "\n",
    "tfidf_question_body = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_body\"].values)])\n",
    "tfidf_question_body_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_body\"].values)])\n",
    "tfidf_question_body = tsvd.fit_transform(tfidf_question_body)\n",
    "tfidf_question_body_test = tsvd.transform(tfidf_question_body_test)\n",
    "\n",
    "tfidf_answer = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"answer\"].values)])\n",
    "tfidf_answer_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"answer\"].values)])\n",
    "tfidf_answer = tsvd.fit_transform(tfidf_answer)\n",
    "tfidf_answer_test = tsvd.transform(tfidf_answer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type2int = {type:i for i,type in enumerate(list(set(train[\"category\"])))}\n",
    "cate = np.identity(5)[np.array(train[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)\n",
    "cate_test = np.identity(5)[np.array(test[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.concatenate([question_title, question_body, answer,\n",
    "                                 tfidf_question_title, tfidf_question_body, tfidf_answer, \n",
    "                                 cate, simple_engineered_feature\n",
    "                                ], axis=1)\n",
    "test_features = np.concatenate([question_title_test, question_body_test, answer_test, \n",
    "                               tfidf_question_title_test, tfidf_question_body_test, tfidf_answer_test,\n",
    "                                cate_test, simple_engineered_feature_test\n",
    "                                ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ba0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "fold_scores = []\n",
    "kf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\n",
    "test_preds = np.zeros((len(test_features), len(target_cols)))\n",
    "for train_index, val_index in kf.split(train_features):\n",
    "    gc.collect()\n",
    "    train_X = train_features[train_index, :]\n",
    "    train_y = train[target_cols].iloc[train_index]\n",
    "    \n",
    "    val_X = train_features[val_index, :]\n",
    "    val_y = train[target_cols].iloc[val_index]\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(512, input_shape=(train_features.shape[1],)),\n",
    "        Activation('relu'),\n",
    "        Dense(128),\n",
    "        Activation('relu'),\n",
    "        Dense(len(target_cols)),\n",
    "        Activation('sigmoid'),\n",
    "    ])\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy')\n",
    "    \n",
    "    model.fit(train_X, train_y, epochs = 100, validation_data=(val_X, val_y), callbacks = [es])\n",
    "    preds = model.predict(val_X)\n",
    "    overall_score = 0\n",
    "    for col_index, col in enumerate(target_cols):\n",
    "        overall_score += spearmanr(preds[:, col_index], val_y[col].values).correlation/len(target_cols)\n",
    "        print(col, spearmanr(preds[:, col_index], val_y[col].values).correlation)\n",
    "    fold_scores.append(overall_score)\n",
    "    print(overall_score)\n",
    "\n",
    "    test_preds += model.predict(test_features)/num_folds\n",
    "    \n",
    "print(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ee890",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\n",
    "for col_index, col in enumerate(target_cols):\n",
    "    sub[col] = test_preds[:, col_index]\n",
    "sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f304b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ec9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f65b7d",
   "metadata": {},
   "source": [
    "# Check prediction\n",
    "\n",
    "Compare train ground truth and test prediction for the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9deca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(6, 5, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "bins = np.linspace(0, 1, 20)\n",
    "\n",
    "for i, col in enumerate(target_cols):\n",
    "    ax = axes[i]\n",
    "    sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n",
    "    sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n",
    "    # ax.set_title(col)\n",
    "    ax.set_xlim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a238291",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
