{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248d09bc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "*This is my first time using keras* (with tensorflow backend) hands-on to practice DL. I am using a video from youtube to guide myself and created this notebook as a memo and note to myself.\n",
    "\n",
    "You can find the video playlist [here](https://www.youtube.com/watch?v=wQ8BIBpya2k&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=2&t=0s).\n",
    "\n",
    "- Being a newbie at DL, this is very much a very very basic model without any augmentations done. The validation scores and accuracy will be pretty horrible.\n",
    "\n",
    "- Having said that, this kernel will be helpful in giving the most BASIC BASELINE model to improve upon, so it can be used to practice DL and other improvement methods like tranfer learning, using data generators, data augmentation etc.\n",
    "\n",
    "- **I have deleted the unzipped files at the end, so that after the predictions are made, you can easily see your output .csv, or else it'll be filled with cat and dog images which were unzipped on output directory.**\n",
    "\n",
    "- Incase, you are not able to get rid of cat and dog images, you'll find your csv in the output folder in \"Data\" section to right, in the editable kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889d34b",
   "metadata": {},
   "source": [
    "# Unzipping the data onto Kaggle itself\n",
    "The following code will unzip the image data. Although they will show up as normal test and train with \".zip\", you should be able to use os and walkthrough inside the zip files to access each image after this step without any warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e263c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ../input/dogs-vs-cats-redux-kernels-edition/train.zip -d train\n",
    "!unzip ../input/dogs-vs-cats-redux-kernels-edition/test.zip -d test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../working/train/train/'\n",
    "TEST_DIR = '../working/test/test/'\n",
    "\n",
    "train_images_filepaths = [TRAIN_DIR + last_file_name for last_file_name in os.listdir(TRAIN_DIR)]\n",
    "test_images_filepaths = [TEST_DIR + last_file_name for last_file_name in os.listdir(TEST_DIR)]\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749b774",
   "metadata": {},
   "source": [
    "We have training data containing both cat as well as dog image. Let us make a list of cat and dog images from the train data and store them separately.\n",
    "\n",
    "Getting the dogs and cats data sorted from the training dataset using list comprehension will do the trick for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd051c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dogs_filepaths = [TRAIN_DIR+ dog_file_name for dog_file_name in os.listdir(TRAIN_DIR) if 'dog' in dog_file_name]\n",
    "train_cats_filepaths = [TRAIN_DIR+ cat_file_name for cat_file_name in os.listdir(TRAIN_DIR) if 'cat' in cat_file_name]\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b350eb0",
   "metadata": {},
   "source": [
    "## Seeing a sample image\n",
    "\n",
    "Each entry in the `train_dogs` list is a file path to one individual image of dog in jpg format.\n",
    "We will be converting each of this photo into an array so that each individual image can be represented as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f68ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing a \"color\" image\n",
    "test_img_file_path = train_dogs_filepaths[0]\n",
    "img_array = cv2.imread(test_img_file_path,cv2.IMREAD_COLOR) #The last parameter can be switched with cv2.IMREAD_GRAYSCALE too\n",
    "plt.imshow(img_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unhide the output to see how the image looks like in array form\n",
    "print(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf680b",
   "metadata": {},
   "source": [
    "The 3 at the end signifies that the image has 3 channels, each for\n",
    "* red\n",
    "* green\n",
    "* blue\n",
    "\n",
    "Incase of grayscale images, there is no need for such three channels. Below is a quick implementation of it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array_gray = cv2.imread(test_img_file_path,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.imshow(img_array_gray, cmap = \"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(img_array_gray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c36af1",
   "metadata": {},
   "source": [
    "## Resizing the photos\n",
    "\n",
    "Each image in the file needn't be the same in it's dimensions. A snapshot of the images as given below.\n",
    "\n",
    "NOTE : In the snapshot, the filenames don't match with that of Kaggle's data. Kaggle has each image filename with 'dog' or 'cat' mentioned in it (only for train data). This dataset used in the YT tutorial is downloaded from microsoft store, which has both the cats and dog files separated into two respective folders.\n",
    "\n",
    "![Dog Snapshot](https://i.imgur.com/0NiPett.jpg)\n",
    "\n",
    "Hence, we will set a global image size, which will serve as a global dimesnion standard for all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1edac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_DIMENSION = 60\n",
    "COLUMN_DIMENSION = 60\n",
    "CHANNELS = 3 #For greyscale images put it to 1; put it to 3 if you want color image data\n",
    "\n",
    "new_array = cv2.resize(img_array_gray,(ROW_DIMENSION,COLUMN_DIMENSION)) #A squarish compression on it's width will take place\n",
    "plt.imshow(new_array,cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e8847",
   "metadata": {},
   "source": [
    "# Prep train and test images\n",
    "\n",
    "Now, we need to prep all the images in the datasets, ie, assigning them with the same global dimensions and other configurations so they stay uniform. \n",
    "\n",
    "We will also add a read_converted_img function that will take an image array as argument and display it back in the converted format, incase we want to see any image in future.\n",
    "\n",
    "`prep_img` does exactly the same, except it returns the modified resized array.\n",
    "\n",
    "![](http://)We will return preped_data which will contain all the modified image arrays while the original filepaths linking to the original image files remain unchanged inside `train_dogs` and `train_cats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_converted_img(to_read_img_array):\n",
    "    plt.imshow(to_read_img_array,cmap = 'gray')\n",
    "    plt.show()\n",
    "    \n",
    "def prep_img(single_image_path):\n",
    "    img_array_to_resize = cv2.imread(single_image_path,cv2.IMREAD_COLOR)\n",
    "    resized = cv2.resize(img_array_to_resize,(ROW_DIMENSION,COLUMN_DIMENSION),interpolation = cv2.INTER_CUBIC)\n",
    "    return resized\n",
    "\n",
    "def prep_data(list_of_image_paths):\n",
    "    \n",
    "    size = len(list_of_image_paths)\n",
    "    \n",
    "    #preped_data = np.ndarray((size, ROW_DIMENSION, COLUMN_DIMENSION,CHANNELS), dtype=np.uint8)\n",
    "    preped_data = []\n",
    "    \n",
    "    '''\n",
    "    for i in range(size):\n",
    "        list_of_image_paths[i] = prep_img(list_of_image_paths)\n",
    "    '''\n",
    "    \n",
    "    for i, image_file_path in enumerate(list_of_image_paths):\n",
    "        '''\n",
    "        image = prep_img(image_file_path)\n",
    "        #preped_data[i] = image.T\n",
    "        preped_data.append(image)\n",
    "        '''\n",
    "        preped_data.append(cv2.resize(cv2.imread(image_file_path), (ROW_DIMENSION,COLUMN_DIMENSION), interpolation=cv2.INTER_CUBIC))\n",
    "        \n",
    "        if(i%1000==0):\n",
    "            print(\"Processed\",i,\"of\",size)\n",
    "        \n",
    "        #print(image.shape)\n",
    "        #print(preped_data.shape)\n",
    "        \n",
    "    return preped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efaba6",
   "metadata": {},
   "source": [
    "The two variables, `train_data` and `test_data` will be used for storing the modified prep data generated from `train_images` and `test_images` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PREPING TRAINING SET\")\n",
    "train_data = prep_data(train_images_filepaths)\n",
    "print(\"\\nPREPING TEST SET\")\n",
    "test_data = prep_data(test_images_filepaths)\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183feb88",
   "metadata": {},
   "source": [
    "Since train_data is a list, let us convert it into a numpy array. I will name it X_train for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85799f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_data)\n",
    "\n",
    "print(X_train.shape)\n",
    "#print(train_data.shape)\n",
    "#print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18077922",
   "metadata": {},
   "source": [
    "Let us try visualising a photo from the converted training set. We'll use the `read_converted_img` method we defined above for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a81aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_converted_img(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04ffa4",
   "metadata": {},
   "source": [
    "# Preparing Y_train\n",
    "\n",
    "Unlike the MNIST dataset, which has a separate column `label` depicting the outcome, we have no such column in this case. However, in the filepath names, each file in train.zip folder has 'dog' or 'cat' being written in it's filename.\n",
    "\n",
    "The same is not true for testing images for obvious reasons. You can confirm this with the code below.\n",
    "\n",
    "Let's start making the `y_train`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images_filepaths[:3])\n",
    "print(\"\\n\")\n",
    "print(test_images_filepaths[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4544fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing y_train\n",
    "\n",
    "y_train = []\n",
    "\n",
    "for path_name in train_images_filepaths:\n",
    "    if('dog' in path_name):\n",
    "        y_train.append(1)\n",
    "    else:\n",
    "        y_train.append(0)\n",
    "\n",
    "print(\"Percentage of dogs is\",sum(y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f836e",
   "metadata": {},
   "source": [
    "We have equal number of cats and dog photos to train from. This is good.\n",
    "Let's convert this list to a numpy array too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f103ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e5d46",
   "metadata": {},
   "source": [
    "# Choosing a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout\n",
    "\n",
    "print(\"Import Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa37cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc_classifier = Sequential()\n",
    "\n",
    "dvc_classifier.add(Conv2D(32,kernel_size = (3,3),\n",
    "                         activation = 'relu',\n",
    "                         input_shape = (ROW_DIMENSION,COLUMN_DIMENSION,3)))\n",
    "\n",
    "dvc_classifier.add(Conv2D(32,kernel_size = (3,3),\n",
    "                        activation = 'relu'))\n",
    "\n",
    "dvc_classifier.add(Conv2D(64,kernel_size = (3,3),\n",
    "                        activation = 'relu'))\n",
    "\n",
    "dvc_classifier.add(Flatten())\n",
    "\n",
    "dvc_classifier.add(Dense(128,activation = 'relu'))\n",
    "\n",
    "dvc_classifier.add(Dropout(0.5))\n",
    "\n",
    "dvc_classifier.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "dvc_classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13fca81",
   "metadata": {},
   "source": [
    "## Syntax Observations : Important for absolute Begineers\n",
    "\n",
    "- **NOTE** : We passed a dense layer of size 1 at the end as this is a binary classification problem and one prediction is enough to find the other prediction. \n",
    "Eg, if a picture is 80% likely to not be a cat, it is 80% likely to be a dog.\n",
    "\n",
    "This is not the case with MNIST dataset that is tasked with recognising the digits. The digits can be any one of the 10 types, and binary classification isn't a viable option.\n",
    "\n",
    "This is highlighted by the shape of our target arrays or the `y_train`\n",
    "\n",
    "The shape of `y_train` in most tutorials for Digit Recognition such as [this one](https://www.kaggle.com/poonaml/deep-neural-network-keras-way), has shape of `y_train` as [num_of_testcases,classifiaction_categories] or simply. [num_of_testcases,10]\n",
    "\n",
    "In this case, `y_train` is of shape (25000,)\n",
    "\n",
    "Simply put, in MNIST Digit Recognition, a number 8 would have it's `y_train` row as\n",
    "[0,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "In this excercise, if a picture is cat, it's `y_train` is given as\n",
    "[0]\n",
    "\n",
    "- **NOTE 2** : If you decide to use same syntax as MNIST digit prediction, you need to show a cat image in it's y_train representation as [1,0] where 0th index is for Cats and 1th index is for Dogs.\n",
    "\n",
    "Doing so also allows you to use the 'softmax' activation function at the last Dense Layer.\n",
    "The respective code changes to\n",
    "\n",
    "`dvc_classifier.add(Dense(num_of_classes,activation = 'softmax'))`\n",
    "where num_of_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dda970",
   "metadata": {},
   "source": [
    "Compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53690f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc_classifier.compile(loss = keras.losses.binary_crossentropy,\n",
    "                      optimizer = 'adam',\n",
    "                      metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b1d56",
   "metadata": {},
   "source": [
    "Fit the model.\n",
    "Since train_data has no need of splitting into X_train and y_train (due to there being no labels in train_data), we can safely conclude `X_train` would be = `train_data`.\n",
    "For readability, we'll copy the elements of it anyway in a new varible `X_train` and use it for fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc_classifier.fit(X_train,y_train,\n",
    "               batch_size = 128,\n",
    "               epochs = 3,\n",
    "               validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37788b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to save a model\n",
    "model_json = dvc_classifier.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dvc_classifier.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90130213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_test = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities = dvc_classifier.predict(arr_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb54611",
   "metadata": {},
   "source": [
    "Let us visualize some of the predictions the model made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,11):\n",
    "    if prediction_probabilities[i, 0] >= 0.5: \n",
    "        print('I am {:.2%} sure this is a Dog'.format(prediction_probabilities[i][0]))\n",
    "    else: \n",
    "        print('I am {:.2%} sure this is a Cat'.format(1-prediction_probabilities[i][0]))\n",
    "        \n",
    "    plt.imshow(arr_test[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10817ec",
   "metadata": {},
   "source": [
    "# Generating output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5595381",
   "metadata": {},
   "source": [
    "We will first remove the zipped files, as they fill up the output section and we are able to see no .csv file.\n",
    "We later make the .csv file 'submissions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deletig the folders containing unzipped data so output section is free of pictures\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Get directory name\n",
    "mydir = \"/kaggle/working\"\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(mydir)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8336bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals = [float(probability) for probability in prediction_probabilities ]\n",
    "\n",
    "submissions = pd.DataFrame({\"id\": list(range(1,len(prediction_probabilities)+1)),\n",
    "                         \"label\": pred_vals})\n",
    "\n",
    "submissions.to_csv(\"dogvcat_1.csv\", index=False, header=True)\n",
    "\n",
    "print(\"Time to submit the baseline model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bbea58",
   "metadata": {},
   "source": [
    "## Improve upon the model from here\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d610ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
