{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77b097d",
   "metadata": {},
   "source": [
    "## Score With Vs Without Outliers (Dramatic Increase):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c5e23",
   "metadata": {},
   "source": [
    "Hello kagglers .. \n",
    "\n",
    "in this notebook we are going to see the difference in score with and without outliers.\n",
    "\n",
    "**Note:** You can skip the processing steps like: Data Cleaning, Data Engineering, ... And focus on the Modeling part.\n",
    "\n",
    "If you find this notebook helpful, Please press the **UPVOTE** button up there, This help me a lot ^-^."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# Importing the Libraries:\n",
    "# =================================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# Importing the Data:\n",
    "# =================================================================================================\n",
    "\n",
    "train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
    "IDtest = test[\"PassengerId\"]\n",
    "## Join train and test datasets in order to obtain the same number of features during categorical conversion\n",
    "train_len = len(train)\n",
    "dataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n",
    "print(\"Data imported successfully!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfac44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# Data Cleaning:\n",
    "# =================================================================================================\n",
    "# You can Skip this cell !!\n",
    "# ---------------------------------------------\n",
    "\n",
    "#Fill Fare missing values with the median value\n",
    "dataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())\n",
    "\n",
    "# Apply log to Fare to reduce skewness distribution\n",
    "dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n",
    "\n",
    "#Fill Embarked nan values of dataset set with 'S' most frequent value\n",
    "dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# convert Sex into categorical value 0 for male and 1 for female\n",
    "dataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})\n",
    "\n",
    "dataset.drop(columns = [\"Cabin\" , \"Ticket\"] , inplace = True)\n",
    "# Filling missing value of Age \n",
    "\n",
    "## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n",
    "# Index of NaN age rows\n",
    "index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_age :\n",
    "    age_med = dataset[\"Age\"].median()\n",
    "    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n",
    "    if not np.isnan(age_pred) :\n",
    "        dataset['Age'].iloc[i] = age_pred\n",
    "    else :\n",
    "        dataset['Age'].iloc[i] = age_med\n",
    "        \n",
    "print(\"Data cleaned successfully!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# Data Engineering:\n",
    "# =================================================================================================\n",
    "# You can Skip this cell !!\n",
    "# ---------------------------------------------\n",
    "\n",
    "\n",
    "# Get Title from Name\n",
    "dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\n",
    "dataset[\"Title\"] = pd.Series(dataset_title)\n",
    "\n",
    "# Convert to categorical values Title \n",
    "dataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "dataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n",
    "dataset[\"Title\"] = dataset[\"Title\"].astype(int)\n",
    "\n",
    "# Drop Name variable\n",
    "dataset.drop(labels = [\"Name\"], axis = 1, inplace = True)\n",
    "\n",
    "# Create a family size descriptor from SibSp and Parch\n",
    "dataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\n",
    "\n",
    "# Create new feature of family size\n",
    "dataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\n",
    "dataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\n",
    "dataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "dataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)\n",
    "\n",
    "# convert to indicator values Title and Embarked \n",
    "dataset = pd.get_dummies(dataset, columns = [\"Title\"])\n",
    "dataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")\n",
    "\n",
    "# Create categorical values for Pclass\n",
    "dataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\n",
    "dataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")\n",
    "\n",
    "# Drop useless variables \n",
    "dataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)\n",
    "\n",
    "print(\"The operation is done successfully!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a6003",
   "metadata": {},
   "source": [
    "## Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313a9f8",
   "metadata": {},
   "source": [
    "#### Train With Outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9666f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# Separate train dataset and test dataset\n",
    "# ===========================================================================\n",
    "\n",
    "# Train:\n",
    "train_with_outliers = dataset[:train_len]\n",
    "train_with_outliers[\"Survived\"] = train[\"Survived\"].astype(int)\n",
    "\n",
    "Y_train_with_outliers = train_with_outliers[\"Survived\"]\n",
    "\n",
    "X_train_with_outliers = train_with_outliers.drop(columns = [\"Survived\"])\n",
    "\n",
    "# Test:\n",
    "test = dataset[train_len:]\n",
    "test.drop(labels=[\"Survived\"],axis = 1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_with_outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_with_outliers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb9ef3",
   "metadata": {},
   "source": [
    "We will use these models:\n",
    "- Random Forest\n",
    "- SVC\n",
    "- GradientBoosting \n",
    "- AdaBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375ef83",
   "metadata": {},
   "source": [
    "#### Parameters tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate model with Kfold stratified cross val\n",
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0680670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# SVC classifier\n",
    "# =====================================================================================\n",
    "def Best_SVM(X_train , Y_train):\n",
    "    SVMC = SVC(probability=True)\n",
    "    svc_param_grid = {'kernel': ['rbf'], \n",
    "                      'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "                      'C': [1, 10, 50, 100,200,300, 1000]}\n",
    "    gsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "    gsSVMC.fit(X_train,Y_train)\n",
    "    return gsSVMC.best_estimator_ , gsSVMC.best_score_\n",
    "# =====================================================================================\n",
    "# RFC Parameters tunning \n",
    "# =====================================================================================\n",
    "def Best_Random_Forest(X_train , Y_train):\n",
    "    RFC = RandomForestClassifier()\n",
    "    rf_param_grid = {\"max_depth\": [None],\n",
    "                  \"max_features\": [1, 3, 10],\n",
    "                  \"min_samples_split\": [2, 3, 10],\n",
    "                  \"min_samples_leaf\": [1, 3, 10],\n",
    "                  \"bootstrap\": [False],\n",
    "                  \"n_estimators\" :[100,300],\n",
    "                  \"criterion\": [\"gini\"]}\n",
    "    gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "    gsRFC.fit(X_train,Y_train)\n",
    "    return  gsRFC.best_estimator_ , gsRFC.best_score_\n",
    "# =====================================================================================\n",
    "# Adaboost\n",
    "# =====================================================================================\n",
    "def Best_ADABoosting(X_train , Y_train):\n",
    "    DTC = DecisionTreeClassifier()\n",
    "    adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "    ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "                  \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "                  \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "                  \"n_estimators\" :[1,2],\n",
    "                  \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
    "    gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "    gsadaDTC.fit(X_train,Y_train)\n",
    "    return gsadaDTC.best_estimator_ , gsadaDTC.best_score_\n",
    "# =====================================================================================\n",
    "# Gradient boosting tunning\n",
    "# =====================================================================================\n",
    "def Best_Gradient_Boosting(X_train , Y_train):\n",
    "    GBC = GradientBoostingClassifier()\n",
    "    gb_param_grid = {'loss' : [\"deviance\"],\n",
    "                  'n_estimators' : [100,200,300],\n",
    "                  'learning_rate': [0.1, 0.05, 0.01],\n",
    "                  'max_depth': [4, 8],\n",
    "                  'min_samples_leaf': [100,150],\n",
    "                  'max_features': [0.3, 0.1] \n",
    "                  }\n",
    "    gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "    gsGBC.fit(X_train,Y_train)\n",
    "    return gsGBC.best_estimator_ , gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3bc680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*100 , \"\\n\" , \"SVM:\" )\n",
    "SVMC_best_with_outlier , SVMC_score_with_outlier = Best_SVM(X_train_with_outliers , Y_train_with_outliers)\n",
    "print(\"SVM Score: \" , SVMC_score_with_outlier)\n",
    "print(\"-\"*100 , \"\\n\" , \"AdaBoost:\" )\n",
    "ada_best_with_outlier , ada_score_with_outlier = Best_ADABoosting(X_train_with_outliers , Y_train_with_outliers)\n",
    "print(\"ADABoost Score: \" , ada_score_with_outlier)\n",
    "print(\"-\"*100 , \"\\n\" , \"Random Forest:\" )\n",
    "RFC_best_with_outlier , RFC_score_with_outlier = Best_Random_Forest(X_train_with_outliers , Y_train_with_outliers)\n",
    "print(\"Random Forest Score: \" , RFC_score_with_outlier)\n",
    "print(\"-\"*100 , \"\\n\" , \"Gradient Boosting:\" )\n",
    "GBC_best_with_outlier , GBC_score_with_outlier = Best_Gradient_Boosting(X_train_with_outliers , Y_train_with_outliers)\n",
    "print(\"Gradient Boosting Score: \" , GBC_score_with_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11bc1c",
   "metadata": {},
   "source": [
    "#### Train without Outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ec1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection \n",
    "\n",
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    \n",
    "    return multiple_outliers   \n",
    "\n",
    "# detect outliers from Age, SibSp , Parch and Fare\n",
    "Outliers_to_drop = detect_outliers(X_train_with_outliers,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5472ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_with_outliers.loc[Outliers_to_drop] # Show the outliers rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac497f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers\n",
    "train_without_outliers = train_with_outliers.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n",
    "Y_train_without_outliers = train_without_outliers[\"Survived\"]\n",
    "X_train_without_outliers = train_without_outliers.drop(columns = [\"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3252121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*100 , \"\\n\" , \"SVM:\" )\n",
    "SVMC_best_without_outlier , SVMC_score_without_outlier = Best_SVM(X_train_without_outliers , Y_train_without_outliers)\n",
    "print(\"SVM Score: \" , SVMC_score_without_outlier)\n",
    "print(\"-\"*100 , \"\\n\" , \"AdaBoost:\" )\n",
    "ada_best_without_outlier , ada_score_without_outlier = Best_ADABoosting(X_train_without_outliers , Y_train_without_outliers)\n",
    "print(\"ADABoost Score: \" , ada_score_without_outlier)\n",
    "print(\"-\"*100 , \"\\n\" , \"Random Forest:\" )\n",
    "RFC_best_without_outlier , RFC_score_without_outlier = Best_Random_Forest(X_train_without_outliers , Y_train_without_outliers)\n",
    "print(\"Random Forest Score: \" , RFC_score_without_outlier)\n",
    "print(\"-\"*100 , \"\\n\" , \"Gradient Boosting:\" )\n",
    "GBC_best_without_outlier , GBC_score_without_outlier = Best_Gradient_Boosting(X_train_without_outliers , Y_train_without_outliers)\n",
    "print(\"Gradient Boosting Score: \" , GBC_score_without_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\"Algorithm\":[\"SVM\" , \"AdaBoost\" , \"Random Forest\" , \"Geadient Boosting\"],\n",
    "           \"With outliers\" : [SVMC_score_with_outlier , ada_score_with_outlier , RFC_score_with_outlier , GBC_score_with_outlier], \n",
    "           \"Without outliers\":[SVMC_score_without_outlier , ada_score_without_outlier , RFC_score_without_outlier , GBC_score_without_outlier],\n",
    "          })\n",
    "result[\"difference\"] = result[\"Without outliers\"] - result[\"With outliers\"]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a828ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
