{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import os,time,sys,re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import efficientnet.tfkeras as efn\n",
    "print(\"Running Tensorflow version: \" + str(tf.__version__))\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting accelerator hardware\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "    gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\n",
    "#Select appropriate distribution strategy for hardware\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    print(\"Running on TPU: \" + str(tpu.master()))\n",
    "elif len(gpus) > 0:\n",
    "    strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "    print(\"Running on \",len(gpus),\" GPU(s)\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de304ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring useful variables\n",
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "IMG_SIZE = 512\n",
    "GCS_BASE_PATH = KaggleDatasets().get_gcs_path()\n",
    "print(GCS_BASE_PATH)\n",
    "print(\"GCS BASE PATH: \" + str(GCS_BASE_PATH))\n",
    "\n",
    "GCS_PATHS = {\n",
    "    192 : GCS_BASE_PATH + '/tfrecords-jpeg-192x192',\n",
    "    224 : GCS_BASE_PATH + '/tfrecords-jpeg-224x224',\n",
    "    391 : GCS_BASE_PATH + '/tfrecords-jpeg-331x331',\n",
    "    512 : GCS_BASE_PATH + '/tfrecords-jpeg-512x512'\n",
    "}\n",
    "\n",
    "GCS_PATH = GCS_PATHS[IMG_SIZE]\n",
    "\n",
    "IMAGE_SIZE = [IMG_SIZE,IMG_SIZE]\n",
    "EPOCHS = 20\n",
    "\n",
    "#Classes for flowers to be classified into\n",
    "CLASSES = ['Pink Primrose', 'Hard-leaved Pocket Orchid', 'Canterbury Bells', 'Sweet Pea', 'Wild Geranium',\n",
    "           'Tiger Lily', 'Moon Orchid', 'Bird Of Paradise', 'Monkshood', 'Globe Thistle',\n",
    "           'Snapdragon', \"Colt's Foot\", 'King Protea', 'Spear Thistle', 'Yellow Iris',\n",
    "           'Globe-flower', 'Purple Coneflower', 'Peruvian Lily', 'Balloon Flower', 'Giant White Arum Lily',\n",
    "           'Fire Lily', 'Pincushion Flower', 'Fritillary', 'Red Ginger', 'Grape Hyacinth',\n",
    "           'Corn Poppy', 'Prince Of Wales Feathers', 'Stemless Gentian', 'Artichoke', 'Sweet William',\n",
    "           'Carnation', 'Garden Phlox', 'Love In The Mist', 'Cosmos', 'Alpine Sea Holly',\n",
    "           'Ruby-lipped Cattleya', 'Cape Flower', 'Great Masterwort', 'Siam Tulip', 'Lenten Rose',\n",
    "           'Barberton Daisy', 'Daffodil', 'Sword Lily', 'Poinsettia', 'Bolero Deep Blue',\n",
    "           'Wallflower', 'Marigold', 'Buttercup', 'Daisy', 'Common Dandelion', 'Petunia', 'Wild Pansy',\n",
    "           'Primula', 'Sunflower', 'Lilac Hibiscus', 'Bishop Of Llandaff', 'Gaura', 'Geranium', 'Orange Dahlia',\n",
    "           'Pink-yellow Dahlia', 'Cautleya Spicata', 'Japanese Anemone', 'Black-eyed Susan', 'Silverbush',\n",
    "           'Californian Poppy', 'Osteospermum', 'Spring Crocus', 'Iris', 'Windflower', 'Tree Poppy', 'Gazania',\n",
    "           'Azalea', 'Water Lily', 'Rose', 'Thorn Apple', 'Morning Glory', 'Passion Flower', 'Lotus', 'Toad Lily',\n",
    "           'Anthurium', 'Frangipani', 'Clematis', 'Hibiscus', 'Columbine', 'Desert-rose', 'Tree Mallow',\n",
    "           'Magnolia', 'Cyclamen ', 'Watercress', 'Canna Lily', 'Hippeastrum ', 'Bee Balm', 'Pink Quill',\n",
    "           'Foxglove', 'Bougainvillea', 'Camellia', 'Mallow', 'Mexican Petunia', 'Bromelia', 'Blanket Flower',\n",
    "           'Trumpet Creeper', 'Blackberry Lily', 'Common Tulip', 'Wild Rose']\n",
    "\n",
    "#Learning rate scheduling variables\n",
    "num_units = strategy.num_replicas_in_sync\n",
    "if num_units == 8:\n",
    "    BATCH_SIZE = 16 * num_units\n",
    "    VALIDATION_BATCH_SIZE = 16 * num_units\n",
    "    start_lr = 0.00001\n",
    "    min_lr = 0.00001\n",
    "    max_lr = 0.00005 * num_units\n",
    "    rampup_epochs = 8\n",
    "    sustain_epochs = 0\n",
    "    exp_decay = 0.8\n",
    "elif num_units == 1:\n",
    "    BATCH_SIZE = 16\n",
    "    VALIDATION_BATCH_SIZE = 16\n",
    "    start_lr = 0.00001\n",
    "    min_lr = 0.00001\n",
    "    max_lr = 0.0002\n",
    "    rampup_epochs = 8\n",
    "    sustain_epochs = 0\n",
    "    exp_decay = 0.8\n",
    "else:\n",
    "    BATCH_SIZE = 8 * num_units\n",
    "    VALIDATION_BATCH_SIZE = 8 * num_units\n",
    "    start_lr = 0.00001\n",
    "    min_lr = 0.00001\n",
    "    max_lr = 0.00002 * num_units\n",
    "    rampup_epochs = 11\n",
    "    sustain_epochs = 0\n",
    "    exp_decay = 0.8\n",
    "    \n",
    "TRAINING_FILENAMES   = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\n",
    "VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\n",
    "TEST_FILENAMES       = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec')\n",
    "TRAIN_STEPS = count_data_items(TRAINING_FILENAMES) // BATCH_SIZE\n",
    "print(\"TRAINING IMAGES: \", count_data_items(TRAINING_FILENAMES), \", STEPS PER EPOCH: \", TRAIN_STEPS)\n",
    "print(\"VALIDATION IMAGES: \", count_data_items(VALIDATION_FILENAMES))\n",
    "print(\"Total training files: \" + str(len(TRAINING_FILENAMES)))\n",
    "\n",
    "print(\"Total validation files: \" + str(len(VALIDATION_FILENAMES)))\n",
    "print(\"Total test files: \" + str(len(TEST_FILENAMES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c47f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Utilities\n",
    "def dataset_to_numpy_util(dataset,batch_size):\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    for images,labels in dataset:\n",
    "        numpy_images = images.numpy()\n",
    "        numpy_labels = labels.numpy()\n",
    "        break\n",
    "    \n",
    "    return numpy_images,numpy_labels\n",
    "\n",
    "def get_title(label,correct_label):\n",
    "    if label.lower() == correct_label.lower():\n",
    "        return ((label + ' [CORRECT]'),False)\n",
    "    else:\n",
    "        return ((label + ' wrong [Should be ' + correct_label + ']'),True)\n",
    "\n",
    "def display_flower(image,title,subplot,red=False):\n",
    "    plt.subplot(*subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.title(title,fontsize=16,color = 'red' if red else 'black')\n",
    "    return (subplot[0],subplot[1],subplot[2] + 1)\n",
    "\n",
    "def display_16_flowers_from_dataset(dataset,test=False):\n",
    "    subplot = (4,4,1)\n",
    "    plt.figure(figsize=(13,13))\n",
    "    images,labels = dataset_to_numpy_util(dataset,16)\n",
    "    for i,image in enumerate(images):\n",
    "        title = CLASSES[labels[i]] if not test else ''\n",
    "        subplot = display_flower(image,title,subplot)\n",
    "        if i >= 15:\n",
    "            break\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace = 0.1,hspace = 0.1)\n",
    "    plt.show()\n",
    "    \n",
    "def display_16_images_with_predictions(images,predictions,labels):\n",
    "    subplot = (4,4,1)\n",
    "    plt.figure(figsize=(13,13))\n",
    "    classes = np.argmax(predictions, axis=-1)\n",
    "    for i,image in enumerate(images):\n",
    "        title,fault = get_title(classes[i],labels[i])\n",
    "        subplot = display_flower(image,title,subplot,fault)\n",
    "        if i>=15:\n",
    "            break\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "def display_training_curves(training,validation,title,subplot):\n",
    "    if subplot%10 == 1:\n",
    "        plt.subplots(figsize = (10,10),facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for reading images and labels from dataset\n",
    "\n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image,channels = 3)\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image = image/255.0\n",
    "    image = tf.reshape(image,[*IMAGE_SIZE,3])\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    labeled_features = {\n",
    "        'image' : tf.io.FixedLenFeature([],tf.string),\n",
    "        'class' : tf.io.FixedLenFeature([],tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example,labeled_features)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class'],tf.int32)\n",
    "    return image,label\n",
    "\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    unlabeled_features = {\n",
    "        'image' : tf.io.FixedLenFeature([],tf.string),\n",
    "        'id'    : tf.io.FixedLenFeature([],tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example,unlabeled_features)\n",
    "    image = decode_image(example['image'])\n",
    "    idnum = example['id']\n",
    "    return image,idnum\n",
    "\n",
    "def load_dataset(filenames,labeled = True):\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    if labeled:\n",
    "        dataset = dataset.map(read_labeled_tfrecord,num_parallel_calls = AUTO)\n",
    "    else:\n",
    "        dataset = dataset.map(read_unlabeled_tfrecord,num_parallel_calls = AUTO)\n",
    "    return dataset\n",
    "\n",
    "def data_augment(image,label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_saturation(image,0,2)\n",
    "    return image,label\n",
    "\n",
    "def get_batched_dataset(filenames,labeled = True, train = False,validation=False):\n",
    "    dataset = load_dataset(filenames,labeled)\n",
    "    dataset = dataset.cache()\n",
    "    if train:\n",
    "        dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "def learningrate_function(epoch):\n",
    "    if epoch < rampup_epochs:\n",
    "        lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
    "    elif epoch < rampup_epochs + sustain_epochs:\n",
    "        lr = max_lr\n",
    "    else:\n",
    "        lr = (max_lr - min_lr) * exp_decay**(epoch - rampup_epochs - sustain_epochs) + min_lr\n",
    "    return lr\n",
    "\n",
    "def learning_rate_callback():\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch : learningrate_function(epoch),verbose = True)\n",
    "    rng = [i for i in range(EPOCHS)]\n",
    "    y = [learningrate_function(x) for x in range(EPOCHS)]\n",
    "    plt.plot(rng,y)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ca212",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = get_batched_dataset(TRAINING_FILENAMES,labeled = True, train = True)\n",
    "validation_dataset = get_batched_dataset(VALIDATION_FILENAMES,labeled = True, train = False)\n",
    "test_dataset = get_batched_dataset(TEST_FILENAMES,labeled = False, train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5789799",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_16_flowers_from_dataset(load_dataset(VALIDATION_FILENAMES,labeled=True),test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aab3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_callback = learning_rate_callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25649ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "with strategy.scope():\n",
    "    #pretrained_model = tf.keras.applications.Xception(input_shape = [*IMAGE_SIZE,3],include_top = False)\n",
    "    #pretrained_model.trainable = True\n",
    "    enet = efn.EfficientNetB0(\n",
    "    input_shape = (512,512,3),\n",
    "    weights = 'imagenet',\n",
    "    include_top = False)\n",
    "    enet.trainable = True\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        enet,\n",
    "        #pretrained_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(len(CLASSES),activation = 'softmax',dtype=tf.float32)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        optimizer = 'adam',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tuning the model by training\n",
    "filepath = \"model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks = [checkpoint, lr_callback]\n",
    "start_time = time.time()\n",
    "hist = model.fit(training_dataset, validation_data = validation_dataset,steps_per_epoch = TRAIN_STEPS, epochs = EPOCHS, callbacks = callbacks )\n",
    "final_accuracy = hist.history[\"val_accuracy\"][-5:]\n",
    "print(\"FINAL ACCURACY MEAN-5: \", np.mean(final_accuracy))\n",
    "print(\"TRAINING TIME: \", time.time() - start_time, \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing training curves\n",
    "display_training_curves(hist.history['loss'], hist.history['val_loss'], 'loss', 211)\n",
    "display_training_curves(hist.history['accuracy'], hist.history['val_accuracy'], 'accuracy', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter_q = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter_q.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()\n",
    "Q_tflite_model = converter_q.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "open('plant_model.tflite','wb').write(tflite_model)\n",
    "open('plant_model_q.tflite','wb').write(Q_tflite_model)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
