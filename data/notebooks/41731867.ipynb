{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ba490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for work with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# graphs\n",
    "import matplotlib.pyplot as plt\n",
    "# for training and training's preparing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# other\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26072d08",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105fa7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s3e20/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s3e20/test.csv')\n",
    "samsub = pd.read_csv('/kaggle/input/playground-series-s3e20/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465d22c",
   "metadata": {},
   "source": [
    "I don't see any obvious point in adding other columns because sometimes 20% of the rows are missing, so I'll leave only \"clean\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e61209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take just necessary columns\n",
    "train_df = train[['latitude', 'longitude', 'year', 'week_no', 'emission']]\n",
    "test_df = test[['latitude', 'longitude', 'year', 'week_no']]\n",
    "\n",
    "# display the datasets\n",
    "display(train_df.head(3))\n",
    "display(test_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96462a0",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b7e93",
   "metadata": {},
   "source": [
    "Obviously, the graphs are very similar, it is necessary to check the trend of this time series in order to understand how different these data are over the years, and this in turn will help us choose the optimal model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68124599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new figure for 3 plots\n",
    "fig, ax = plt.subplots(3)\n",
    "# create 3 graphs for every year in data\n",
    "ax[0].plot(train_df[train_df.year == 2019]['emission']) \n",
    "ax[1].plot(train_df[train_df.year == 2020]['emission']) \n",
    "ax[2].plot(train_df[train_df.year == 2021]['emission']) \n",
    "# show plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02739f3b",
   "metadata": {},
   "source": [
    "The MSE for each year is no more than 400, which means that the series are really similar not only visually, but also numerically. It should be mentioned right away that there was a crisis in 2020 due to the coronavirus, which is why the MSE is so large this year, so this fact confirms that there are no special fluctuations from year to year. Therefore, in the training data I will use both 2019 and 2021, or only 2021 - I will rely on the results of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE(2019/2020): ', mean_squared_error(train_df[train_df.year == 2019]['emission'], train_df[train_df.year == 2020]['emission']))\n",
    "print('MSE(2019/2021): ', mean_squared_error(train_df[train_df.year == 2019]['emission'], train_df[train_df.year == 2021]['emission']))\n",
    "print('MSE(2020/2021): ', mean_squared_error(train_df[train_df.year == 2020]['emission'], train_df[train_df.year == 2021]['emission']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd52faa",
   "metadata": {},
   "source": [
    "There are also week numbers in the source data, but there are fewer weeks in the training sample, as can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59912526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig, ax = plt.subplots()\n",
    "# take week's columns from train and test datasets\n",
    "ax.bar(['Train', 'Test'], height=[len(train_df['week_no'].unique()), len(test_df['week_no'].unique())])\n",
    "ax.set_title('How many weeks in dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fffce3e",
   "metadata": {},
   "source": [
    "So we will leave the values for all weeks except 49, 50, 51 and 52. This way we will improve the predictions for the remaining weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take data just for 2021 year\n",
    "train_df = train_df[train_df.year == 2021]\n",
    "# drop useless raws for the training\n",
    "train_df = train_df[train_df['week_no'] != 49][train_df['week_no'] != 50][train_df['week_no'] != 51][train_df['week_no'] != 52]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581f981",
   "metadata": {},
   "source": [
    "Now we are preparing the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a803c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data on train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df.drop(['emission'], axis=1), train_df['emission'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856f58f",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d2b9e",
   "metadata": {},
   "source": [
    "For training, I will choose RandomForestRegressor, because: firstly, half of the columns in the data are categorical data, and secondly, RFG is great for values with trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new regression model\n",
    "reg = RandomForestRegressor()\n",
    "# train the regression model\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a60ec",
   "metadata": {},
   "source": [
    "# 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3255af3",
   "metadata": {},
   "source": [
    "To test the model before predicting the test sample, we use X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e455cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions\n",
    "preds = reg.predict(X_test)\n",
    "# print MSE between the datasets\n",
    "print(mean_squared_error(preds, y_test))\n",
    "# plot the preds\n",
    "pd.Series(preds).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4c96d",
   "metadata": {},
   "source": [
    "As we can see from the chart: the short-term trend and periodicity are preserved. Now let's prepare a table with predictions and save them to the file 'submission.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final predictions\n",
    "sub = reg.predict(test_df)\n",
    "# rewrite the 'emission' column with the predictions\n",
    "samsub['emission'] = np.around(sub+1, 6)\n",
    "# save the file\n",
    "samsub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35090a56",
   "metadata": {},
   "source": [
    "### Thanks for your attemption!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
