{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe9d7fe",
   "metadata": {},
   "source": [
    "# <center><u> Face Detection using MTCNN </u></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b4f48",
   "metadata": {},
   "source": [
    "<center><img src=\"https://play-lh.googleusercontent.com/_TPNZQF4zJ8XWvPsNxVAQk4ogODPhcwojnD1YFzIaLT675mW_DtT-5pgav2tO0WHIbs=w240-h480-rw\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a46668",
   "metadata": {},
   "source": [
    "## why Face detection and why only MTCNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00636d44",
   "metadata": {},
   "source": [
    "Facial recognition task requires the creation of face embeddings for the faces that we want to recognize. All models that are capable of generating face embeddings will generate them properly if and only if the image that contains only the face to it is passed. So, most of the face recogntion pipe lines will have the face detection and extraction as the preliminary task. There are various models available in the community that are capable of detecting the faces in the image. MTCNN(Multi Task Cascaded Nueral Network) is one of the start of the art techniques for face detection. Though the execution time is a bit high when compared to other models, the performance of the model is best and is capable of detecting faces well in different light conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a42f7",
   "metadata": {},
   "source": [
    "### Installing MTCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb1aba",
   "metadata": {},
   "source": [
    "<center><img src=\"https://miro.medium.com/v2/resize:fit:490/1*mH7AABb6oha9g6v9jB4gjw.png\"> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mtcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71762f",
   "metadata": {},
   "source": [
    "### Installing few other required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60545389",
   "metadata": {},
   "source": [
    "1. **shutil** - Used for creating, deleting and moving files and directores.\n",
    "2. **tensorflow** - Framework for building deep learning networks.\n",
    "3. **mtcnn** - contains the architectue of the model and helps in inferencing MTCNN.\n",
    "4. **cv2(opencv)** - Used for performing operations on images like colour format conversion, resizing and many more.\n",
    "5. **tqdm** - Instantly make loops show a smart progress meter.\n",
    "6. **matplotlib** - Used for plotting. Sometimes can also be used for displaying images.\n",
    "7. **glob** - Used for listing files in a directory recursivley using wildcards.\n",
    "8. **random** - Used for random number generation with different distributions.\n",
    "9. **os** - Used for accessing files and moving across file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bdfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tensorflow as tf\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb1d8c",
   "metadata": {},
   "source": [
    "## Lets have a look at the folder structure of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7045250",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tree -d \"/kaggle/input/indian-actor-images-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a19eed",
   "metadata": {},
   "source": [
    "We have 135 directories with 135 unique indian actor names. Each directory will have 50 unique images of that corresponding actor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655a88c",
   "metadata": {},
   "source": [
    "### Displaying one of the image in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/kaggle/input/indian-actor-images-dataset/Bollywood Actor Images/Bollywood Actor Images/deepika_padukone/03fcc8cb3a.jpg')\n",
    "plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac245b47",
   "metadata": {},
   "source": [
    "## What is the color format of above image and why is it differnet to our original image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c513a3d",
   "metadata": {},
   "source": [
    "We have read the above image using opencv. The opencv reads the image in BGR(Blue Green Red) colour format by default. Our original image is in the RGB(Red Green Blue) colour format. To look at the original image, we can change the colour format of the image to RGB from BGR using ```cvtColor()``` of Opencv.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f250b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a105788",
   "metadata": {},
   "source": [
    "### Generating a grid of random images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_creation(num_rows, num_columns, image_paths):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "     - num_rows : number of rows to be present in the grid of images.\n",
    "     - num_columns :  number of columns to be present in the grid of images.\n",
    "     - image_path : list with paths to all the images in the dataset.\n",
    "    Functionality:\n",
    "     Generates a grid of images according to the input dimensions from the randomly selected images\n",
    "     availbale in the dataset.\n",
    "    \"\"\"\n",
    "    fig,axes = plt.subplots(num_rows, num_columns, figsize=(18,11), sharex=True, sharey=True)\n",
    "    for i,axes in enumerate(axes.flat):\n",
    "        random_image_path = random.choice(image_paths)\n",
    "        random_image = cv2.imread(random_image_path)\n",
    "        random_image = cv2.resize(random_image, (250,250))\n",
    "        random_image = cv2.cvtColor(random_image, cv2.COLOR_BGR2RGB)\n",
    "        axes.set_xlabel(random_image_path.split('/')[-2])\n",
    "        axes.set_yticks([])\n",
    "        axes.imshow(random_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '/kaggle/input/indian-actor-images-dataset/Bollywood Actor Images/Bollywood Actor Images'\n",
    "image_paths = glob.glob(directory_path+'/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_creation(4, 8, image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd8388",
   "metadata": {},
   "source": [
    "## Lets understand How to leverage MTCNN for face detection and its output format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c4a698",
   "metadata": {},
   "source": [
    "1. The results from MTCNN will be a list of dictonaries where each dictionary contains the information about each detected face in the image. So, the length of the result from MTCNN represents the total number of faces detected in the image.\n",
    "\n",
    "2. Each dictionary will have 3 keys. They are\n",
    "   ```\n",
    "   a. box : list of 4 numbers and each number has its own significance. The first and the second               number represents the X,Y co-ordinates of the top left corner of the bounding box that             covers the face. The third and the fourth numbers represent the height and widht of                 that bounding box.\n",
    "   Format -> 'box':[90,23,73,96]\n",
    "   \n",
    "   b. confidence : This value lies between a range of 0 and 1 representing the authenticity of the                    detected face.\n",
    "   Format -> 'confidence':0.9988888888\n",
    "   \n",
    "   c. keypoints : This is again a dictionary with 5 keys in it. This dictinary will contians the X,                   Y cordinates of left_eye, right_eye, nose and mouth.\n",
    "    Format -> 'keypoints':{'left_eye': (104, 63),'right_eye': (137, 58),'nose': (119,80),\n",
    "    'mouth_left':(110,96),'mouth_right': (144, 91)}}\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f825e",
   "metadata": {},
   "source": [
    "### Overlaying the results of MTCNN on one of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b35bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_path = random.choice(image_paths)\n",
    "random_image = cv2.cvtColor(cv2.imread(random_image_path), cv2.COLOR_BGR2RGB)\n",
    "detector = MTCNN()\n",
    "result = detector.detect_faces(random_image)\n",
    "if len(result)>1:\n",
    "    print('$'*50)\n",
    "    print(\"ReExute the cell once again.\")\n",
    "    print('$'*50)\n",
    "else:\n",
    "    x1, y1, width, height = result[0]['box'][0], result[0]['box'][1], result[0]['box'][2], result[0]['box'][3],\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # Displaying the bounding box over the face of the actor\n",
    "    random_image=cv2.rectangle(random_image, (x1,y1), (x2,y2),(255,0,0), 4)\n",
    "    \n",
    "    # Displaying the confidence value for the detected bounding box.\n",
    "    random_image=cv2.putText(random_image,f\"confidence:{round(result[0]['confidence'],2)}\",\n",
    "                             (x1-50,y2+10),cv2.FONT_HERSHEY_COMPLEX_SMALL,1,(0,255,255),1)\n",
    "    \n",
    "    # Displaying the detected keypoings in the face\n",
    "    for key in result[0]['keypoints']:\n",
    "        random_image=cv2.circle(random_image,result[0]['keypoints'][key],radius=0,color=(255,0,0),thickness=5)\n",
    "    plt.xlabel(random_image_path.split('/')[-2])\n",
    "    plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    plt.imshow(random_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40baa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_face(detector,img_path):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "     - detector : instance of MTCNN class.\n",
    "     - img_path : Path to the image\n",
    "    Functionality:\n",
    "     Functon detects the face in the image using MTCNN and extracts the pixels of the image and'\n",
    "     returns a new image that has only detected face in it.\n",
    "    \"\"\"\n",
    "    img=cv2.imread(img_path)\n",
    "    img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    face=detector.detect_faces(img)\n",
    "    if len(face)>1 or len(face)<1:\n",
    "        print(img_path)\n",
    "        return []\n",
    "    face_coords=face[0]['box']\n",
    "    x1, y1, width, height = face_coords[0],face_coords[1],face_coords[2],face_coords[3],\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    extracted_face=img[y1:y2,x1:x2]\n",
    "    return extracted_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9cb23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images(actor_name,base_dir,destination_dir,images):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "     - actor_name : Name of the actor. This is used to create a directory.\n",
    "     - base_dir : path to the directory that contains the original base images.\n",
    "     - destination_dir : Destination directory path that should contains the processed images\n",
    "     - images : list of images present in the directory of corresponding actor.\n",
    "    Functionality\n",
    "     Function works at an actor level. The function takes each actor images once, processes them and\n",
    "     saves the processed images in a new directory. The base directory structure is maintained in the\n",
    "     destination as well. \n",
    "    \"\"\"\n",
    "    excluded_image_paths=[]\n",
    "    actor_dir_path=destination_dir+actor_name\n",
    "    os.mkdir(actor_dir_path)\n",
    "    detector=MTCNN()\n",
    "    for image in images:\n",
    "        face=return_face(detector,base_dir+actor_name+'/'+image)\n",
    "        if len(face)==0:\n",
    "            excluded_image_paths.append(base_dir+actor_name+'/'+image)\n",
    "            continue\n",
    "        face=cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(actor_dir_path+'/'+image,face)\n",
    "    return excluded_image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4ffb4",
   "metadata": {},
   "source": [
    "## Extracting the face images from all the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf999ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='/kaggle/input/indian-actor-images-dataset/Bollywood Actor Images/Bollywood Actor Images/'\n",
    "destination_dir='/kaggle/working/bollywood_actors_dataset/'\n",
    "os.mkdir(destination_dir)\n",
    "excluded_images=[]\n",
    "dir_count=1\n",
    "for actor_dir in tqdm.tqdm(os.listdir(base_dir)):\n",
    "    print(f\"Directory Count : {dir_count}\")\n",
    "    actor_images=os.listdir(base_dir+actor_dir)\n",
    "    excluded_image_paths=move_images(actor_dir,base_dir,destination_dir,actor_images)\n",
    "    excluded_images.extend(excluded_image_paths)\n",
    "    dir_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac280d56",
   "metadata": {},
   "source": [
    "While detecing the faces in the images, we have left few images from processing. Those images include ones in which MTCNN did not detect atleast one face and the images in which MTCNN has detected more than one face in it. The reasons for excluding images that has more than one face in it was, there might be a chance where the image contains the face of some other actor. If we save both the faces in the one directory, we might end up having the wrong faces in each directory. Instead, we will just save the paths of the images that we have excluded. If the excluded images count is more, we can manually crop those images and if the count is less, we can exclude them from the dataset. For the images in which the MTCNN did not detect any face in it, we can go ahead an try some other face detectoin technique that would reduce this count a bit further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6653d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of Excluded Images: {len(excluded_images)}\")\n",
    "percentage_dropped=round((len(excluded_images)/len(os.listdir(base_dir)*50))*100,2)\n",
    "print(f\"Percentage of Images dropped: {percentage_dropped}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tree -d \"/kaggle/working/bollywood_actors_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c15f29",
   "metadata": {},
   "source": [
    "### Generating grid of images from the processed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '/kaggle/working/bollywood_actors_dataset'\n",
    "image_paths = glob.glob(directory_path+'/*/*')\n",
    "grid_creation(4, 4, image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb612d",
   "metadata": {},
   "source": [
    "### Downloading the created dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf890f6c",
   "metadata": {},
   "source": [
    "Downloading the generated images from the output section of kaggle notebook using the UI download option is not an smart idea to export our dataset. This is so because, we can only download one image at once via the UI download option. We have greater than 1500 images and we need to click the download button for 1500 times. Instead, we can zip our entire processed dataset and can download it with just one click. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r file.zip /kaggle/working/bollywood_actors_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also download the dataset using the below link\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'file.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ceb45",
   "metadata": {},
   "source": [
    "<center style=\"color:red;font-size:20px\">Upvote the notebook, if this has added any value to your learnings.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854fb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
