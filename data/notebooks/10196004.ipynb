{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec7ec70",
   "metadata": {},
   "source": [
    "In this notebook I have created a CNN model built in PyTorch to predict MNIST dataset. \n",
    "**I was able to achieve 99.3% accuracy**. I also tested my model using a random image for internet and the prediction was perfect.\n",
    "Please find my work below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms ## Hence torchvision is basically used as it has many datasets and also transform properities\n",
    "# that can be applied on these datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1cc3b",
   "metadata": {},
   "source": [
    "Initializing **GPU** **Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c40427",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # To use to cuba GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c155703",
   "metadata": {},
   "source": [
    "**Transformations**, this is really necessary as PyTorch works with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((28,28)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,))\n",
    "                               ])\n",
    "training_dataset = datasets.MNIST(root='./data_1', train=True, download=True, transform=transform)\n",
    "validation_dataset = datasets.MNIST(root='./data_1', train=False, download=True, transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=100, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = 100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516ea72",
   "metadata": {},
   "source": [
    "**NumPy Image Conversion Function **, I did this to display my images using plt. As matplotlib.pyplot do not work with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca887c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "  image = tensor.cpu().clone().detach().numpy() # Just use cpu in this case as it is not compatible with gpu\n",
    "  image = image.transpose(1, 2, 0)\n",
    "  image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
    "  image = image.clip(0, 1)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = dataiter.next()\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "for idx in np.arange(20):\n",
    "  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "  plt.imshow(im_convert(images[idx]))\n",
    "  ax.set_title([labels[idx].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.conv1 = nn.Conv2d(1, 20, 5, 1) # Conv layer1\n",
    "      self.conv2 = nn.Conv2d(20, 50, 5, 1) # Conv layer2\n",
    "      self.fc1 = nn.Linear(4*4*50, 500)    # Fully connected layer1\n",
    "      self.dropout1 = nn.Dropout(0.5)   # We use dropout layer between these both FCL as they have the highest number of parameters b/t them\n",
    "      self.fc2 = nn.Linear(500, 10)   # Fully connected layer2\n",
    "    def forward(self, x):\n",
    "      x = F.relu(self.conv1(x))  # Apply ReLu to the feature maps produced after Conv 1 layer\n",
    "      x = F.max_pool2d(x, 2, 2)  # Pooling layer after Conv 1 layer\n",
    "      x = F.relu(self.conv2(x))  # Apply ReLu to the feature maps produced after Conv 2 layer\n",
    "      x = F.max_pool2d(x, 2, 2)  # Pooling layer after Conv 2 layer\n",
    "      x = x.view(-1, 4*4*50)     # Flattening the output of CNN to feed it into Fully connected layer\n",
    "      x = F.relu(self.fc1(x))   # Fully connected layer 1 with Relu\n",
    "      x = self.dropout1(x)     # We use dropout layer between these both FCL as they have the highest number of parameters b/t them\n",
    "      x = self.fc2(x)         # Fully connected layer 2 with no activation funct as we need raw output from CrossEntropyLoss\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef51091",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1507a0",
   "metadata": {},
   "source": [
    "* I will be using the most popular Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45042cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc5e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "val_running_loss_history = []\n",
    "val_running_corrects_history = []\n",
    "\n",
    "for e in range(epochs):\n",
    "  \n",
    "  running_loss = 0.0\n",
    "  running_corrects = 0.0\n",
    "  val_running_loss = 0.0\n",
    "  val_running_corrects = 0.0\n",
    "  \n",
    "  for inputs, labels in training_loader: # As our train_loader is batch size of 100 and had input images and corresponding labels\n",
    "    inputs = inputs.to(device)  # Put our inputs and labels in the device as our model is also in the device\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs)   # giving input to our model to get corresponding output\n",
    "    loss = criterion(outputs, labels) # comparing out model's output to original labels\n",
    "    \n",
    "    optimizer.zero_grad()  ##sets the initial gradient to zero\n",
    "    loss.backward()  ## The whole calculated loss is then back propogated to the model\n",
    "    optimizer.step()  ## Then the weights are updated by doing their derivative w.r.t the Loss\n",
    "    \n",
    "    _, preds = torch.max(outputs, 1) # Then we select the max value of raw output and consider it as our prediction. We select it from 10 o/ps\n",
    "    running_loss += loss.item()  # total loss of 1 epoch\n",
    "    running_corrects += torch.sum(preds == labels.data) #total accuracy of 1 epoch\n",
    "\n",
    "  else:\n",
    "    with torch.no_grad(): # This we done to set no gradient as we do not need it for val as our model is already trained.\n",
    "      for val_inputs, val_labels in validation_loader:\n",
    "        val_inputs = val_inputs.to(device)  # Put our val_inputs and labels in the device as our model is also in the device\n",
    "        val_labels = val_labels.to(device)\n",
    "        val_outputs = model(val_inputs)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "        \n",
    "        _, val_preds = torch.max(val_outputs, 1)\n",
    "        val_running_loss += val_loss.item()\n",
    "        val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
    "      \n",
    "    epoch_loss = running_loss/len(training_loader)\n",
    "    epoch_acc = running_corrects.float()/ len(training_loader)\n",
    "    running_loss_history.append(epoch_loss)\n",
    "    running_corrects_history.append(epoch_acc)\n",
    "    \n",
    "    val_epoch_loss = val_running_loss/len(validation_loader)\n",
    "    val_epoch_acc = val_running_corrects.float()/ len(validation_loader)\n",
    "    val_running_loss_history.append(val_epoch_loss)\n",
    "    val_running_corrects_history.append(val_epoch_acc)\n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_loss_history, label='training loss')\n",
    "plt.plot(val_running_loss_history, label='validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4998041",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_corrects_history, label='training accuracy')\n",
    "plt.plot(val_running_corrects_history, label='validation accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b775089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = 'https://images.homedepot-static.com/productImages/007164ea-d47e-4f66-8d8c-fd9f621984a2/svn/architectural-mailboxes-house-letters-numbers-3585b-5-64_1000.jpg'\n",
    "response = requests.get(url, stream = True)\n",
    "img = Image.open(response.raw)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.ImageOps.invert(img)  # we use Image operations from PIL to invert(i.e. make white black and vice versa)\n",
    "img = img.convert('1') # we convert from RGB to Gray\n",
    "img = transform(img) # Apply the transform funct we defined earlier to make our downloaded img same as what we trained on\n",
    "plt.imshow(im_convert(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c888b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = img.to(device)  # As our model is in the device\n",
    "image = images[0].unsqueeze(0).unsqueeze(0)\n",
    "output = model(image)\n",
    "_, pred = torch.max(output, 1)\n",
    "print(pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45666c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(validation_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "output = model(images)\n",
    "_, preds = torch.max(output, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "for idx in np.arange(20):\n",
    "  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "  plt.imshow(im_convert(images[idx]))\n",
    "  ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab2230a",
   "metadata": {},
   "source": [
    "**As seen I got really good accuracy and my model predicted all the new images correctly, which it has never seen before.**\n",
    "**As you can see it amount of coding that has to be done is much more when using PyTorch over Tensorflow**\n",
    "**I prefer working with Tensorflow & Keras because its simple to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
