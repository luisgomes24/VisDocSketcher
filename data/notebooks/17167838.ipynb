{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245d1fbd",
   "metadata": {},
   "source": [
    "## Natural Language Processing - SMS Spam Detection\n",
    "\n",
    "**Let's train a model to predict spam messages!**\n",
    "\n",
    "Description of the data:\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
    "\n",
    "\n",
    "Dataset used: SMS Spam Collection Dataset, UCI\n",
    "\n",
    "### Importing Initial Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b159e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d278d1",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data\n",
    "df = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',encoding='latin1')\n",
    "\n",
    "#printing data information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01262528",
   "metadata": {},
   "source": [
    "**The 2nd, 3rd, and 4th columns are having too many null values.**\n",
    "\n",
    "**Let's explore these columns in detail.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b33cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring some parts of the data where messages are found in Unnamed:3 and Unnamed:4 columns.\n",
    "\n",
    "df[df.columns.drop(['v1','v2'])].dropna(how='all').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4720c75",
   "metadata": {},
   "source": [
    "**The columns other than v1 and v2 also have messages (shown above).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['v1'] == 'spam',df.columns.drop(['v1','v2'])].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5795a5",
   "metadata": {},
   "source": [
    "**None of the columns Unnamed: 2, Unnamed: 3 and Unnamed: 4 have any *Spam* messages.**\n",
    "\n",
    "**Hence we are labelling the messages in these columns as *ham***\n",
    "\n",
    "**Let's try to rearrage these columns to a two column dataframe, where one is for labels and the other is for  messages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569abce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning label ham to messages in Unnamed: 2, Unnamed: 3, Unnamed: 4 and concatenating them with v2\n",
    "temp1 = df[['v2','v1']]\n",
    "\n",
    "temp2 = df['Unnamed: 2'].to_frame()\n",
    "temp2.loc[:,'v1'] = 'ham'\n",
    "temp2 = temp2.rename(columns={'Unnamed: 2':'v2'})\n",
    "\n",
    "temp3 = df['Unnamed: 3'].to_frame()\n",
    "temp3.loc[:,'v1'] = 'ham'\n",
    "temp3 = temp3.rename(columns={'Unnamed: 3':'v2'})\n",
    "\n",
    "temp4 = df['Unnamed: 4'].to_frame()\n",
    "temp4.loc[:,'v1'] = 'ham'\n",
    "temp4 = temp4.rename(columns={'Unnamed: 4':'v2'})\n",
    "\n",
    "df = pd.concat([temp1,temp2,temp3,temp4],axis=0).dropna().reindex(columns=['v1','v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's print the last 10 messages in our dataframe\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d8e55",
   "metadata": {},
   "source": [
    "**Let's change the column labels of the dataframe for readability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns for readability\n",
    "df.columns = ['label','message']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8451d7",
   "metadata": {},
   "source": [
    "Let's have a look at the messages labelled as **spam**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e869144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['label'] == 'spam'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881ca73",
   "metadata": {},
   "source": [
    "**Spam** messages seem to be well structured and lengthy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674d531",
   "metadata": {},
   "source": [
    "Let's have a look at the countplot of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3035085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting frequency vs label name\n",
    "_=sns.countplot(data=df,x='label',palette='viridis')\n",
    "print(\"percentage of labels:\",f\"{100*df.groupby('label').count()/len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a6444",
   "metadata": {},
   "source": [
    "**ham** messages are high in number compared to **spam** messages. The dataset is slightly imbalanced or skewed. We have to take this imbalance into account while training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f54ca",
   "metadata": {},
   "source": [
    "Let's have a look at distribution of message sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a length column\n",
    "df['len'] = df['message'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b77171",
   "metadata": {},
   "source": [
    "Usually spam messages are larger than personal messages. Let's comapare the distribution of the lengths of **spam** and **ham** messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(12,8))\n",
    "check = 'ham'\n",
    "sns.histplot(df[df['label']==check],x='len',color='green',ax=ax,alpha=0.7)\n",
    "ymin, ymax = plt.gca().get_ylim()\n",
    "plt.vlines(x=df.loc[df['label']==check,'len'].median(),ymin=ymin,ymax=ymax,color='green',alpha=0.5,linestyles='dashed')\n",
    "check = 'spam'\n",
    "sns.histplot(df[df['label']==check],x='len',color='orange',ax=ax,alpha=0.9)\n",
    "plt.vlines(x=df.loc[df['label']==check,'len'].median(),ymin=ymin,ymax=ymax,color='orange',linestyles='dashed')\n",
    "plt.legend(['median ham lengths','median spam lengths'])\n",
    "_=plt.title('Distribution of message sizes')\n",
    "_=plt.ylabel('Count or Frequency')\n",
    "_=plt.xlabel('Message Length  (in characters)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baee59b",
   "metadata": {},
   "source": [
    "**The median of both the frequency distributions indicate that personal messages are mostly short and spam messages are lengthy! Hence we can use the length of messages as a feature while training the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864cf069",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b76efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing functions that helps in feature engineering\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting words and punctuations to be removed in a list\n",
    "remove_list = set(list(string.punctuation) + stopwords.words('english'))\n",
    "#defining a function that can tokenize the messages, remove unwanted words and punctuations\n",
    "porterstemmer = PorterStemmer()\n",
    "def message_cleaner(message):\n",
    "    cleaned_message = []\n",
    "    for word in word_tokenize(message.lower()):\n",
    "        word = re.sub('[^a-zA-Z]','',word)\n",
    "        if(word == ''):\n",
    "            continue\n",
    "        if(word not in remove_list):\n",
    "            cleaned_message.append(porterstemmer.stem(word))\n",
    "    return cleaned_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d5bf1",
   "metadata": {},
   "source": [
    "Let's test the message cleaner function using a dummy messsage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_cleaner('Hey, Are you attending that guitar competition?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4a948",
   "metadata": {},
   "source": [
    "Cool. The function seems to do it's job. Now let's calculate tf-idf features from the messages.\n",
    "\n",
    "**The TfidfVectorizer function will run the analyzer (message_cleaner) through each of the messages and create a sparse matrix of words and it's frequency. It then calculates the tf-idf features from this matrix. The length for each message is also calculated and appended to the matrix as a feature.**\n",
    "\n",
    "**We must split the dataset into train and test data for further processing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2620ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a2ab1",
   "metadata": {},
   "source": [
    "**Let's map labels *spam* to 1 and *ham* to 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].map(lambda x: 0 if(x=='ham') else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2060fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(df['message'],df['label'],test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion \n",
    "#FeatureUnion is used to concat feaatures obtained using different transform functions\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "#FunctionTransformer can create a transform function from any arbitrary or user defined function\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095b1b2",
   "metadata": {},
   "source": [
    "**Writing a function that can calculate and return the length of messages in sparse matrix format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acf582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(df):\n",
    "    df = df.apply(len)\n",
    "    return csr_matrix(df.to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ceac7b",
   "metadata": {},
   "source": [
    "**Let's unite the TfidfVectorizer and get_length features to a single matrix using a FeatureUnion pipeline. The features matrix can then be created using the fit and transform method on this FeatureUnion object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def533f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipe = FeatureUnion([\n",
    "    ('tfidf',TfidfVectorizer(analyzer=message_cleaner)),\n",
    "    ('length',FunctionTransformer(get_length))])\n",
    "tfidf_mat = feature_pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12729e9a",
   "metadata": {},
   "source": [
    "**Let's print the shape of the feature matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of the matrix:', tfidf_mat.shape) # This is a very sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28bab4",
   "metadata": {},
   "source": [
    "**Let's check out the tf-idf features of words in the first message (message_num=0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_num=0 #give the index value of message in DataFrame df\n",
    "\n",
    "#get the indices of the token words in any message and its length\n",
    "ind = tfidf_mat[message_num].nonzero()[1]\n",
    "#print the corresponding word and its tf-idf weights, and the length of the whole message\n",
    "for index in ind:\n",
    "    if(index == (tfidf_mat.shape[1]-1)):\n",
    "        print('\\nmessage length:',tfidf_mat[message_num,index])\n",
    "    else:\n",
    "        print('Word:',feature_pipe.transformer_list[0][1].get_feature_names()[index],'  tf-idf:',tfidf_mat[message_num,index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cbba4",
   "metadata": {},
   "source": [
    "## Training and tuning the model\n",
    "\n",
    "Let's use the Multinomial naive bayes classifier to train the model and GridSearchCV to tune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4290c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1b36d",
   "metadata": {},
   "source": [
    "**Since the labels are imbalanced, we choose recall as the scoring method. Tuning the model using recall score will improve predictability of the spam messages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee370dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define grid search\n",
    "params =  {'alpha':[0.001,0.01,0.1,0.25,0.3,0.35,0.5,0.6,0.7,0.8,0.9,1,2,5]}\n",
    "grid = GridSearchCV(MultinomialNB(),param_grid=params,cv=6,scoring='recall')\n",
    "grid.fit(tfidf_mat,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a82b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print accuracy score\n",
    "print(f'Accuracy score is: {grid.score(feature_pipe.transform(X_test),y_test):.2f}')\n",
    "#Print the best parameter\n",
    "print('Best parameter:',grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747e65c",
   "metadata": {},
   "source": [
    "**Let's print and plot the result scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2df1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform test data before predicting\n",
    "transformed_X_test = feature_pipe.transform(X_test)\n",
    "\n",
    "#classify test messages\n",
    "predictions = grid.predict(transformed_X_test)\n",
    "#print classification report\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436ab4e",
   "metadata": {},
   "source": [
    "**We have a good recall and f1-score for both labels!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test,predictions),annot=True,fmt='d')\n",
    "plt.xlabel(['ham predict','spam predict'])\n",
    "_=plt.ylabel(['spam actual','ham actual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f998bd6",
   "metadata": {},
   "source": [
    "## Thank You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
