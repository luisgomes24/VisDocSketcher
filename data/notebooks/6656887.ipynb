{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af15ada",
   "metadata": {},
   "source": [
    "**Site 1 is University College London**. They provide open data for energy for many buildings including the ones for this competition.\n",
    "https://platform.carbonculture.net/communities/ucl/30/apps/assets/list/place/\n",
    "\n",
    "Leak link is in  official external dataset thread:\n",
    "https://www.kaggle.com/c/ashrae-energy-prediction/discussion/112841#latest-675067\n",
    "\n",
    "I've done the mapping locally by basically applying a pearson correlation. I've also scraped meta-data available. Here is the results and kernel to download data.It only covers 50 buildings and site 1 has 51. I haven't found (yet) the missing one (number 152) but I'm sure it's here because another competitor found it.\n",
    "\n",
    "It's another data leak (and more are coming). However this competition is not over as organizers said that all leaked data will be removed from private LB scoring. So currently, you should use leaked data to improve your model. Public LB is totally biased with leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9370d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, gc, math, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io, timeit, os, gc, requests\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import requests, json, zipfile\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 4000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "path_data = \"/kaggle/input/ashrae-energy-prediction/\"\n",
    "TRAIN_FILE = path_data + \"train.csv\"\n",
    "TEST_FILE = path_data + \"test.csv\"\n",
    "TRAIN_BUILDING_FILE = path_data + \"building_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c193c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory optimization\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def df_optimization(df, use_float16=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"col: %s was %s and is %s\" % (col, col_type, df[col].dtype))\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read mapping (matching done locally simply with pearson correlation)\n",
    "site1_pd = pd.read_csv(\"/kaggle/input/ucl50buildings/site1_scrapped_50_buildings.csv\", encoding=\"UTF-8\")\n",
    "site1_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cdb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert UCL data to ASHREA format.\n",
    "# UCL data is by half hour so we have to sum per hour.\n",
    "def as_ashrae_format(df, coef=1.0):\n",
    "    df.rename(columns= {\"Unnamed: 0\":\"date\"}, inplace=True)\n",
    "    df = df.set_index('date').T.reset_index()\n",
    "    flat_pd = pd.melt(df, id_vars=[\"index\"], var_name=\"date\", value_name=\"meter_reading_scraped\")\n",
    "    flat_pd[\"timestamp\"] = flat_pd[\"date\"] + \" \"+ flat_pd[\"index\"]\n",
    "    flat_pd.drop(columns = [\"index\", \"date\"], inplace=True)\n",
    "    flat_pd[\"timestamp\"] = pd.to_datetime(flat_pd[\"timestamp\"])\n",
    "    flat_pd[\"meter\"] = 0\n",
    "    flat_pd[\"meter_reading_scraped\"] = flat_pd[\"meter_reading_scraped\"].astype(np.float64) * coef\n",
    "    flat_pd = flat_pd.set_index(\"timestamp\").sort_index().reset_index()\n",
    "    flat_pd[\"minute\"] = flat_pd[\"timestamp\"].dt.minute\n",
    "    flat_pd[\"hour\"] = flat_pd[\"timestamp\"].dt.hour\n",
    "    flat_pd[\"day\"] = flat_pd[\"timestamp\"].dt.day\n",
    "    flat_pd[\"month\"] = flat_pd[\"timestamp\"].dt.month\n",
    "    flat_pd[\"year\"] = flat_pd[\"timestamp\"].dt.year\n",
    "    flat_pd[\"meter_reading_scraped\"] = flat_pd.groupby([\"year\", \"month\", \"day\", \"hour\"])[\"meter_reading_scraped\"].transform(np.nansum)\n",
    "    flat_pd = flat_pd[flat_pd[\"minute\"] == 0].reset_index(drop=True)\n",
    "    flat_pd.drop(columns = [\"year\", \"month\", \"day\", \"hour\", \"minute\"], inplace=True)\n",
    "    return flat_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_building_data(download_url, building_id):\n",
    "    tmp_pd = None\n",
    "    r = requests.get(download_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "         if r.headers['Content-Disposition'].find(\"attachment\") >= 0:\n",
    "            result = re.search('attachment; filename=\"(.*)\"', r.headers['Content-Disposition'])\n",
    "            if result is not None:\n",
    "                filename = result.group(1)\n",
    "                print(\"Downloading %s: %s\" % (filename, download_url))\n",
    "                r.raw.decode_content = True\n",
    "                in_memory = BytesIO(r.content)\n",
    "                with zipfile.ZipFile(in_memory) as archive:\n",
    "                    files = archive.namelist()\n",
    "                    for file in files:\n",
    "                        if file.find(\"elec\") > 0:\n",
    "                            tmp_pd = as_ashrae_format(pd.read_csv(archive.open(file), skiprows=3), coef=1.0)\n",
    "                            tmp_pd[\"building_id\"] = int(building_id)\n",
    "                return tmp_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap site1 data\n",
    "site1_scraped_pd = None\n",
    "for idx, row in site1_pd.iterrows():\n",
    "    tmp_pd = download_building_data(row[\"url\"], row[\"building_id\"])\n",
    "    if site1_scraped_pd is None:\n",
    "        site1_scraped_pd = tmp_pd\n",
    "    else:\n",
    "        site1_scraped_pd = pd.concat([site1_scraped_pd, tmp_pd], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by building_id, timestamp\n",
    "site1_scraped_pd = site1_scraped_pd.set_index([\"building_id\", \"meter\", \"timestamp\"]).sort_index().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "site1_scraped_pd.to_pickle(\"site1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1bc3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building 152 is missing.\n",
    "scraped_buildings = site1_scraped_pd[\"building_id\"].unique()\n",
    "scraped_buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69fb572",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv(TRAIN_FILE)\n",
    "train_pd[\"timestamp\"] = pd.to_datetime(train_pd[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "train_pd = df_optimization(train_pd)\n",
    "print(train_pd.info())\n",
    "train_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d549b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_building_pd = pd.read_csv(TRAIN_BUILDING_FILE) \n",
    "train_building_pd = df_optimization(train_building_pd)\n",
    "train_building_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only site1\n",
    "train_pd = pd.merge(train_pd, train_building_pd[[\"site_id\", \"building_id\"]], on=\"building_id\")\n",
    "train_pd = train_pd.query(\"site_id == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join scraped data\n",
    "train_pd = pd.merge(train_pd, site1_scraped_pd, on=[\"building_id\", \"meter\", \"timestamp\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97754105",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd.query(\"building_id == 138 & meter == 0\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08eddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data to get timestamp for further join\n",
    "test_pd = pd.read_csv(TEST_FILE)\n",
    "test_pd[\"timestamp\"] = pd.to_datetime(test_pd[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "test_pd = df_optimization(test_pd)\n",
    "print(test_pd.info())\n",
    "test_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791fd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only site1\n",
    "test_pd = pd.merge(test_pd, train_building_pd[[\"site_id\", \"building_id\"]], on=\"building_id\")\n",
    "test_pd = test_pd.query(\"site_id == 1\")\n",
    "test_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727af78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join scraped data\n",
    "test_pd = pd.merge(test_pd, site1_scraped_pd, on=[\"building_id\", \"meter\", \"timestamp\"], how=\"left\").drop(columns=[\"row_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d3f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pd = pd.concat([train_pd, test_pd], axis=0).set_index([\"building_id\", \"meter\", \"timestamp\"]).sort_index().reset_index()\n",
    "full_pd.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building 152 is missing\n",
    "# Blue: Original data\n",
    "# Green: Scraped\n",
    "for bid in scraped_buildings:\n",
    "    fig, ax = plt.subplots(figsize=(24, 5))\n",
    "    t1 = full_pd.query(\"building_id == %d & timestamp >= '2016-01-01 00:00:00' & timestamp < '2019-01-01 00:00:00' & meter == 0\" % bid).set_index(\"timestamp\")\n",
    "    d = t1.plot(kind='line', y=\"meter_reading\", ax=ax, c='blue')\n",
    "    d = t1.plot(kind='line', y=\"meter_reading_scraped\", ax=ax, c='green', alpha=0.5, grid=True)\n",
    "    diff = np.sum(t1[\"meter_reading\"] - t1[\"meter_reading_scraped\"])\n",
    "    plt.title(\"Building id: %d (%d), diff=%.3f\" % (bid, len(t1), diff))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
