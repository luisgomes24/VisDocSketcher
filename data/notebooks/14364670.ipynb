{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e9468c9",
   "metadata": {},
   "source": [
    "# Exercise 2 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d2807",
   "metadata": {},
   "source": [
    "This lab is on performing some regression experiments and develop the understanding of Model Selection.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d11b1",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37176d03",
   "metadata": {},
   "source": [
    "From the Regression lecture we have seen that Linear Regression can be easily extended to Polynomial Regression just by augmenting the original data with extra nonlinear terms. This strategy is adopted by Sklearn to implement both regression techniques using the same Linear Regression engine, and introducing a pre-processing pipeline for Polynomial augmentation. This is shown in the following code cell:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# define a pipeline for polynomial regression\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689a645",
   "metadata": {},
   "source": [
    "Now we make some simple toy data - this is generate the X and y arrays we are to use for later experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377cdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some example data\n",
    "def make_data(N=30, err=0.8, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2           \n",
    "    y = 10 - 1. / (X.ravel() + 0.1)   \n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data()\n",
    "xf = np.linspace(0.0, 1.0, 1000)[:, None]       # use 1000 points in [-0.1, 1.0) to plot out prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5e5ee",
   "metadata": {},
   "source": [
    "Now we call out Pyplot to do some simple plotting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b.')     # plot the data points \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081c0ac0",
   "metadata": {},
   "source": [
    "As our first modeling attempt, let us try linear regression, i.e., a PolynomialRegression of degree 1. Let's see how the fitted model predict over the specified range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolynomialRegression(1).fit(X, y)\n",
    "plt.plot(X, y, 'b.') \n",
    "plt.plot(xf.ravel(), model.predict(xf), color='gray')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba691890",
   "metadata": {},
   "source": [
    "**TO-DO** Copy and paste the code above; modify it so that it generates prediction using degree 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for degree=3; fitting is much better!\n",
    "model = PolynomialRegression(3).fit(X, y)\n",
    "plt.plot(X, y, 'b.') \n",
    "plt.plot(xf.ravel(), model.predict(xf), color='gray')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35d05c",
   "metadata": {},
   "source": [
    "There are three major functions for us to focus on:\n",
    "- First we call the **fit(X, y)** to *train* the model with the training data X, y\n",
    "- Then do a **predict()** to produce predictions\n",
    "- Or, we can **score(X, y)** to check out the performance on data X and y.\n",
    "\n",
    "In our examples, to plot out the prediction curves we use a range of input values in [0.0, 1.0] (using numpy's linspace() function). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60d5ac",
   "metadata": {},
   "source": [
    "## Just Fit It\n",
    "Here's an example to show multiple prediction results using subplots. For polynomial orders from 1 to 8, we generate the fitting models, visualize them, and report the $R^2$ scores. It is easy to see which model gives the best $R^2$ score..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38280b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(16, 6))  # Create 4x2 subplots showing results\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.2, hspace=0.3) # adjust subplot layout a bit\n",
    "\n",
    "for deg in range(1,9):\n",
    "    model = PolynomialRegression(deg).fit(X, y)\n",
    "    r = int((deg-1)/4)\n",
    "    c = (deg-1)%4\n",
    "    ax[r, c].scatter(X.ravel(), y, s=40)\n",
    "    ax[r, c].plot(xf.ravel(), model.predict(xf), color='gray')\n",
    "    ax[r, c].axis([-0.1, 1.0, -2, 14])\n",
    "    ax[r, c].set_title('Degr.={}, $R^2$={}'.format(deg, model.score(X, y)), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a7b91",
   "metadata": {},
   "source": [
    "<font color=\"red\">*Your Answer*:</font> \n",
    "- $R^2$ score keeps increasing when the degree of the polynomial model increases.\n",
    "- The score is measured on the *training* data only. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db77a949",
   "metadata": {},
   "source": [
    "## Do It Properly\n",
    "Now, generate twenty testing data instances and re-assess the score trend with the new testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = make_data(20, rseed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a94ac",
   "metadata": {},
   "source": [
    "**TO-DO.** Follow the example given above. Plot out the prediction curve and display the $R^2$ score for each model being tested on the X2,y2 data. In other words, copy and paste the code, keep \".fit(X, y)\" unchanged, but change \".score(X, y)\" to \".score(X2, y2)\". What is now revealed by the new results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and modify...\n",
    "fig, ax = plt.subplots(2, 4, figsize=(16, 6))  # Create 4x2 subplots showing results\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.2, hspace=0.3) # adjust subplot layout a bit\n",
    "\n",
    "for deg in range(1,9):\n",
    "    model = PolynomialRegression(deg).fit(X, y)\n",
    "    r = int((deg-1)/4)\n",
    "    c = (deg-1)%4\n",
    "    ax[r, c].scatter(X2.ravel(), y2, s=40)    # note: plotting out testing data instead\n",
    "    ax[r, c].plot(xf.ravel(), model.predict(xf), color='gray')\n",
    "    ax[r, c].axis([-0.1, 1.0, -2, 14])\n",
    "    ax[r, c].set_title('Degr.={}, $R^2$={}'.format(deg, model.score(X2, y2)), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758d854",
   "metadata": {},
   "source": [
    "*Note* when plotting out the fit curves against the testing data points, quite some *misses* are revealed. By looking at the scores, the polynomial models with degrees higher than 6 actually start to drop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313dacce",
   "metadata": {},
   "source": [
    "We now collect all test scores for display and comparison. \n",
    "For the model degree ranged from 1 to 11 (inclusive), we retrain (using X and y) and test a model (using X2 and y2). Two lists are used to record the training score and testing score of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a964e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code with tesing score plotted out\n",
    "tr_score=[]      # create two empty lists to collect training and testing scores\n",
    "te_score=[]\n",
    "degrange=range(1,12)\n",
    "for deg in degrange:\n",
    "    model = PolynomialRegression(deg).fit(X, y)     # training on X, y\n",
    "    tr_score.append(model.score(X, y))              # score on training data  \n",
    "    te_score.append(model.score(X2, y2))         # score on testing data\n",
    "\n",
    "plt.xlabel('Degree'); plt.ylabel(\"$R^2$\")\n",
    "plt.plot(degrange, tr_score, 'g-', label='Training score')\n",
    "# also plotting test scores \n",
    "plt.plot(degrange, te_score, 'r-', label='Training score')\n",
    "plt.legend(); plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f4695",
   "metadata": {},
   "source": [
    "**TO-DO.** The testing score array is missing in the plot above. Add it in and re-run the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978a277",
   "metadata": {},
   "source": [
    "What we get is a so-called \"[run chart](https://en.wikipedia.org/wiki/Run_chart)\" frequently used in machine learning practice. \n",
    "\n",
    "**END**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ad4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
