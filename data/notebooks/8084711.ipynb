{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da546733",
   "metadata": {},
   "source": [
    "<h1 style=\"color:brown\">Supervised Learning: Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0afc3cf",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb29b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d6adb",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe7047",
   "metadata": {},
   "source": [
    "Logistic Regression is a very basic classification method in machine learning. Thanks to simplicity and efficiency, the algorithm has been widely used in practical scenarios. In this course, we will explore the principle of logical regression and its algorithm implementation, and use scikit-learn to construct a logistic regression classification prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3afbe0",
   "metadata": {},
   "source": [
    "### Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbb70d",
   "metadata": {},
   "source": [
    "- The relationship between logistic regression and linear regression<br>\n",
    "- Logistic regression model<br>\n",
    "- Logarithmic loss function<br>\n",
    "- Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7540bfa7",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79b474",
   "metadata": {},
   "source": [
    "- Python 3.6.6<br>\n",
    "- NumPy 1.18.1<br>\n",
    "- Matplotlib 3.0.3<br>\n",
    "- Pandas 0.25.3<br>\n",
    "- scikit-learn 0.21.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342d133",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9005a66",
   "metadata": {},
   "source": [
    "## Basics of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7b5ed",
   "metadata": {},
   "source": [
    "\"Logistic regression\", when you hear the name, the first thing you may notice is exactly the word 'regression'. You may be confused. Haven't the contents of regression been learned last week? Why do we put the logistic regression into the curriculum of this week's focus - classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfabe2d",
   "metadata": {},
   "source": [
    "Therefore, at the beginning of this course, it is necessary to emphasize: ** Logistic regression is a classification method, not a regression method. ** You need to keep this in mind afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c932b0",
   "metadata": {},
   "source": [
    "So, why does **logical regression take a name with the word 'regression'? Does it really have nothing to do with the regression methods mentioned earlier? **<br>\n",
    "<br>\n",
    "Regarding this question, we will answer it at the end of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657bed51",
   "metadata": {},
   "source": [
    "There is a video about Logistics Regression. We hope it can deepen your understanding of Supervised Learning: Classification Logistic Regression | Source: [Statquest.org](https://youtu.be/yIYKR4sgzI8/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c008a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('yIYKR4sgzI8', width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82fef92",
   "metadata": {},
   "source": [
    "### Linear Separability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998dfdb",
   "metadata": {},
   "source": [
    "First of all, we need to touch a concept first: linear separability. As shown in the figure below, **in a two-dimensional plane**, if you can separate samples by using only one line directly, the samples are called linearly separable, otherwise they are linearly inseparable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import os\n",
    "Image(\"../input/week-3-images/Logistic-Regression-Classification-1.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb822598",
   "metadata": {},
   "source": [
    "Of course, in a three-dimensional space, if you can separate the samples through a plane, it is also known as linear separability. Since this part will not be involved, we will not discuss it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a76f3f1",
   "metadata": {},
   "source": [
    "### Classification with Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328c436",
   "metadata": {},
   "source": [
    "In the previous section, we focused on the definition of linear regression. To sum it up, linear regression is a method to predict more continuous values by fitting a straight line. In fact, in addition to the regression problem, linear regression can also be used to deal with the classification problem **in special cases**. e.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728bbd3",
   "metadata": {},
   "source": [
    "We have the following dataset, which contains only `1` feature and `1` target. For example, we count the scores of students and determine whether the course is `PASS` through the length of study time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample data\n",
    "\"\"\"\n",
    "\n",
    "scores=[[1],[1],[2],[2],[3],[3],[3],[4],[4],[5],[6],[6],[7],[7],[8],[8],[8],[9],[9],[10]]\n",
    "passed= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb7839",
   "metadata": {},
   "source": [
    "**â˜ž Exercise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c9ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your code here-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1bbe6",
   "metadata": {},
   "source": [
    "In the above dataset, `passed` takes only `0` and `1`, which are numeric data. However, here if we denote `1` and `0` as `PASS` and `NOT PASS`, respectively, then it is converted into a classification problem. Moreover, this is a typical binary classification problem. The binary classification indicates that there are only two categories, which can also be called: `0-1` classification problem.<br>\n",
    "<br>\n",
    "For such a binary classification problem, how to solve it with linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19918a9b",
   "metadata": {},
   "source": [
    "Here we can define: The result of the linear fitting function $f(x)$ is $f(x) > 0.5$ (near 1) for `PASS` and $f(x) <= 0.5$ (near 0) for `NOT PASS`:<br>\n",
    "\n",
    "$$\n",
    "f_(x) \\gt 0.5 => y=1 \\\\\n",
    "f_(x) \\leq 0.5 => y=0\n",
    "\\tag1\n",
    "$$\n",
    "\n",
    "In this way, linear regression can be skillfully used to solve binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e247492",
   "metadata": {},
   "source": [
    "Below we will start the practice. First draw a scatter plot corresponding to the dataset in the 2D plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46dffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Draw a scatter plot with sample data\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "plt.scatter(scores, passed, color='r')\n",
    "plt.xlabel(\"scores\")\n",
    "plt.ylabel(\"passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0ec82",
   "metadata": {},
   "source": [
    "Then we use `sklearn` to complete the linear regression fitting. I believe that after studying the first week's contents you are familiar enough with the linear regression fitting process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12579aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear regression fitting\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(scores, passed)\n",
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d5d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d276c",
   "metadata": {},
   "source": [
    "Next draw the fitted line into the scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54890bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Draw the plot after fitting process\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-2,12,100)\n",
    "\n",
    "plt.plot(x, model.coef_[0] * x + model.intercept_)\n",
    "plt.scatter(scores, passed, color='r')\n",
    "plt.xlabel(\"scores\")\n",
    "plt.ylabel(\"passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81d19c",
   "metadata": {},
   "source": [
    "If you follow the above definition, the result of the linear fit function $f(x)$ is $f(x) > 0.5$ for `PASS`, and $f(x) <= 0.5$ for `NOT PASS`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3926d8",
   "metadata": {},
   "source": [
    "Then, as shown in the figure below, any part whose `scores` is larger than the $x$  coordinate value of the orange vertical line will be judged as `PASS`, which means the two points circled by the brown box are misclassified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls ../input/week-3-images/\n",
    "Image(\"../input/week-3-images/Untitled.jpg\", width=\"800\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea4d35",
   "metadata": {},
   "source": [
    "## Principle and Implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a404c",
   "metadata": {},
   "source": [
    "### Sigmoid Distribution Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fecdb2",
   "metadata": {},
   "source": [
    "As seen above, although we can use linear regression to solve a binary classification problem, its results are not ideal. Especially in the completion of the binary classification problem, linear regression may also produce negative values or numbers greater than $1$ during the calculation. So, here we can better solve the binary classification problem by a method called **logistic regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47518a",
   "metadata": {},
   "source": [
    "Here we introduce a function called **Sigmoid**, which is defined as follows:<br>\n",
    "\n",
    "$$\n",
    "f(z)=\\frac{1}{1+e^{-z}}\n",
    "\\tag2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84238e4d",
   "metadata": {},
   "source": [
    "You may feel some confusion: Why do we suddenly introduce such a function? <br>\n",
    "<br>\n",
    "Below we draw the curve of this function. Take a look. Perhaps you will understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sigmoid function\n",
    "\"\"\"\n",
    "\n",
    "z = np.linspace(-12, 12, 100) # Generate equidistant x values for easy drawing\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "plt.plot(z, sigmoid)\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabce945",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd68945",
   "metadata": {},
   "source": [
    "The figure above is of the Sigmoid function, and you will be surprised to find that it presents a perfect \"S\" shape (S = Sigmoid). Its value is only between `0` and `1`, and is symmetric about the center of the `z=0` axis. At the same time, larger the `z` is, the closer the `y` is to `1`; and smaller the `z` is, the closer the `y` is to `0`. If we use `0.5` as the demarcation point, we can divide the values `> 0.5` and the values `<= 0.5` into two categories. This is a perfect choice for solving binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136d9fa",
   "metadata": {},
   "source": [
    "Here we have to introduce a mathematical definition. That is, if a set of consecutive random variables conforms to the Sigmoid function distribution, it is called a logical distribution. **Logistic distribution is the theorem in probability theory, which is a _continuous probability distribution_**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a6622",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97cde0",
   "metadata": {},
   "source": [
    "In the example of the above section, we solve the classification problem by linear regression. `y` of the fitted linear function was found to be between $\\left ( - \\infty, + \\infty \\right )$. For the Sigmoid function, however, mentioned in the above section, its `y` is between $\\left ( 0,1 \\right )$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d021201",
   "metadata": {},
   "source": [
    "So, consider combining the concepts here, that is, the result of fitting the linear function is compressed to $\\left ( 0,1 \\right )$ using the Sigmoid function. If `y` of the linear function is larger, it means that the probability is closer to `1`, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0512996",
   "metadata": {},
   "source": [
    "So, in logistic regression, we define:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ac130",
   "metadata": {},
   "source": [
    "$$\n",
    "z_{i} = {w_0}{x_0} + {w_1}{x_1} + \\cdots + {w_i}{x_i} = {w^T}x \\\\\n",
    "f(z_{i})=\\frac{1}{1+e^{-z_{i}}}\n",
    "\\tag3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32129989",
   "metadata": {},
   "source": [
    "For equation $(3)$, in general: $w_0=b$, $x_0=1$, which corresponds to the intercept term in the linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68736a",
   "metadata": {},
   "source": [
    "In the above equation, we multiply each of the features denoted by $x$ by the coefficient $w$ and then calculate the $f(z)$ value by the Sigmoid function to get the probability. Here, $z$ can be regarded as a classification boundary. Therefore:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309cea73",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{w}(x) = f({w^T}x)\n",
    "\\tag4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9869c84a",
   "metadata": {},
   "source": [
    "Below we implement the above equation $(3)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b67a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic Regression Model\n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(z):\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca325b",
   "metadata": {},
   "source": [
    "### Logarithmic Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57697817",
   "metadata": {},
   "source": [
    "Next, we attempt to solve the parameter $w^T$ in equation $(3)$. Before that, we need to define the loss function. We recall that the loss function is a function that measures the difference between the predicted value and the ground truth. For example, in linear regression we use the squared loss function. Now, in logistic regression, we usually use a logarithmic loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274fcac9",
   "metadata": {},
   "source": [
    "$$\n",
    "J(w) = \\frac{1}{m} \\sum_{i=1}^m \\left [ - y_i \\log (h_{w}(x)) - (1-y_i) \\log(1-h_{w}(x_i))  \\right ]\\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94532bbd",
   "metadata": {},
   "source": [
    "You might consider why we use the squared loss function in linear regression? In fact, there is a mathematical basis. The purpose of setting the loss function is to find the minimum value of the loss function with the selected optimization method. The minimum loss assures that the model is optimal at that time. In the optimization solution, only the convex function can obtain the global minimum and the non-convex function tends to obtain the local optimum. In our case, the squared loss function, when used for logistic regression, will obtain a non-convex function, which means that global optimization will be missed in most of the cases. Therefore, we use logarithmic loss function to avoid this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10361d2",
   "metadata": {},
   "source": [
    "Of course, the above explanation involves a lot of mathematical knowledge. Especially like the optimization theory, it is the content involved in the postgraduate course and there might be some difficulties in understanding. If you can't get it, just remember that in logistic regression it is recommended to use the logarithmic loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b89424",
   "metadata": {},
   "source": [
    "There is a video about Logarithmic Loss Function. We hope it can deepen your understanding of Supervised Learning: Logarithmic Loss Function in Classification\n",
    "\n",
    "[Video] Logarithmic Loss Function | Source: [End-to-end Machine Learning](https://end-to-end-machine-learning.teachable.com/courses/polynomial-regression-optimization/lectures/9559488)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17040598",
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('fr7dfyfB7mI', width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17c594",
   "metadata": {},
   "source": [
    "Below, we implement the equation $(5)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091c9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logarithmic Loss Function\n",
    "\"\"\"\n",
    "\n",
    "def loss(h, y):\n",
    "    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7bbdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe2e29",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5013b",
   "metadata": {},
   "source": [
    "In the section above, we have successfully defined and implemented the logarithmic loss function. Up to now, it is only one step away from solving the optimal parameters, that is, to find the minimum value of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37941767",
   "metadata": {},
   "source": [
    "In order to find the minimum value of the equation $(5)$, a solution method called **gradient descent** is introduced here. Gradient descent is a very common and classical optimization algorithm, through which we can quickly find the minimum value of a function. The principle of the gradient descent method will be explained below. We hope that you can understand it carefully. Many of the following cases will apply the gradient descent method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ef605",
   "metadata": {},
   "source": [
    "To understand the gradient descent, first of all let's answer the question: What is 'gradient'? A gradient is a vector that indicates that the direction derivative of a function takes the maximum along this direction at that point, i.e., the function changes the fastest along the direction of this gradient (changes with the highest rate of change). In short, for a unary function a gradient is the derivative at a certain point. For a multivariate function, a gradient is a vector of partial derivatives at a certain point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395e890",
   "metadata": {},
   "source": [
    "Since the function changes the fastest along the gradient, the core of the gradient descent method is that we **seek the minimum value of the loss function along the direction of the gradient descent**. The process is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"../input/week-3-images/Logistic Regression Classification 4.jpeg\", width=\"800\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7ac86",
   "metadata": {},
   "source": [
    "Therefore, we find the partial derivative for equation $(5)$ and get the gradient:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11635c47",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X^T(h_{w}(x)-y) \\tag6\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce16cf",
   "metadata": {},
   "source": [
    "When we get the direction of the gradient and then multiply it by a constant $\\alpha$, we can get the step size of the gradient descent (the length of the arrow above). Finally, through multiple iterations, the point where the gradient change is small enough will be found, which corresponds to the minimum value of the loss function. Here the constant $\\alpha$ is often referred to as the learning rate. The process of performing a weight update is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065bcf2",
   "metadata": {},
   "source": [
    "$$\n",
    "w \\leftarrow w - \\alpha \\frac{\\partial{J}}{\\partial{w}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00dee0e",
   "metadata": {},
   "source": [
    "There is a video about Gradient Descent. We hope it can deepen your understanding of Supervised Learning: Gradient Descent in Classification\n",
    "\n",
    "[Video] Gradient Descent | Source: [Statquest.org](https://statquest.org/video-index/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('sDv4f4s2SB8', width=800, height=300) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00babea",
   "metadata": {},
   "source": [
    "Below we implement the equation $(5)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate gradient\n",
    "\"\"\"\n",
    "\n",
    "def gradient(X, h, y):\n",
    "    gradient = np.dot(X.T, (h - y)) / y.shape[0]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db1f832",
   "metadata": {},
   "source": [
    "### Logistic Regression Implementation with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef58d98",
   "metadata": {},
   "source": [
    "So far, we have the basic elements to implement logistic regression. Next, we will use a set of sample data to complete the classification task. First, download and load the sample data. The dataset name is: `Logistic-Regression-Classification-data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01963f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../input/week-3-dataset/Logistic-Regression-Classification-data.csv\", header=0) # Load dataset\n",
    "df.head() # Preview first 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56469707",
   "metadata": {},
   "source": [
    "As you can see, the dataset has two feature variables `X0` and `X1`, and a target value `Y`. Among them, the target value `Y` only contains `0` and `1` which is a typical binary classification problem. We try to plot the dataset into a graph and take a look at the distribution of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67962e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the data distribution\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['X0'],df['X1'], c=df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c057b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd60fb5",
   "metadata": {},
   "source": [
    "Above dark blue represents `0` and yellow represents `1`. Next, the logistic regression is used to complete the classification task, that is, the linear function in equation $(3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d5864",
   "metadata": {},
   "source": [
    "In order to facilitate the display of the code, the logistic regression model mentioned above, loss function and gradient descent code are collectively presented together. Now, let's use Python to implement a complete logistic regression process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165468a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete logistic regression process\n",
    "\"\"\"\n",
    "\n",
    "#Sigmoid distribution function\n",
    "def sigmoid(z):\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    return sigmoid\n",
    "\n",
    "#Loss function\n",
    "def loss(h, y):\n",
    "    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    return loss\n",
    "\n",
    "#Calculate the gradient\n",
    "def gradient(X, h, y):\n",
    "    gradient = np.dot(X.T, (h - y)) / y.shape[0]\n",
    "    return gradient\n",
    "\n",
    "#Logistic regression process\n",
    "def Logistic_Regression(x, y, lr, num_iter):\n",
    "    intercept = np.ones((x.shape[0], 1)) # Initialize intercept as 1\n",
    "    x = np.concatenate((intercept, x), axis=1)\n",
    "    w = np.zeros(x.shape[1]) # Initialize parameters as 1\n",
    "    \n",
    "    for i in range(num_iter): # Gradient descent iterations\n",
    "        z = np.dot(x, w) # Linear function\n",
    "        h = sigmoid(z) # Sigmoid function\n",
    "        \n",
    "        g = gradient(x, h, y) # Calculate the gradient\n",
    "        w -= lr * g # Calculate the step size and perform the gradient descent with lr\n",
    "        \n",
    "        z = np.dot(x, w) # Update the parameters\n",
    "        h = sigmoid(z) # Get Sigmoid value\n",
    "        \n",
    "        l = loss(h, y) # Get loss value\n",
    "        \n",
    "    return l, w # Return the gradient and parameters after iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58654a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8e0b9",
   "metadata": {},
   "source": [
    "Then we set the learning rate and the number of iterations to train the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set parameters and train\n",
    "\"\"\"\n",
    "\n",
    "x = df[['X0','X1']].values\n",
    "y = df['Y'].values\n",
    "lr = 0.001 # Learning rate\n",
    "num_iter = 10000 # Iterations\n",
    "\n",
    "#Train\n",
    "L = Logistic_Regression(x, y, lr, num_iter)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b83c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe1624",
   "metadata": {},
   "source": [
    "Based on the weight we calculated, the function of the classification boundary line is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aee1bf",
   "metadata": {},
   "source": [
    "$$y = L[1][0] + L[1][1]*x1 + L[1][2]*x2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8075cbe",
   "metadata": {},
   "source": [
    "<div style=\"color: #999;font-size: 12px;font-style: italic;\">* $L[*][*]$ denotes the corresponding values selected from the $L$ array.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e258508",
   "metadata": {},
   "source": [
    "With the classification boundary line function, we can draw it into the original graph to see how the classification works. The following code involves using Matplotlib to draw outlines, no need to cover:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aea0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the above results\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['X0'],df['X1'], c=df['Y'])\n",
    "\n",
    "x1_min, x1_max = df['X0'].min(), df['X0'].max(),\n",
    "x2_min, x2_max = df['X1'].min(), df['X1'].max(),\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "probs = (np.dot(grid, np.array([L[1][1:3]]).T) + L[1][0]).reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, probs, levels=[0], linewidths=1, colors='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc62f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f995cd6",
   "metadata": {},
   "source": [
    "It can be seen that the red line which represents the dividing line obtained here is a linear function. It is more consistent with the separation trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd1381",
   "metadata": {},
   "source": [
    "In addition to drawing the decision boundary, that is, the dividing line, we can also draw the process of updating the loss function to observe the execution of the gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Draw the changing process of loss function\n",
    "\"\"\"\n",
    "\n",
    "def Logistic_Regression(x, y, lr, num_iter):\n",
    "    intercept = np.ones((x.shape[0], 1)) # Initialize intercept as 1\n",
    "    x = np.concatenate((intercept, x), axis=1)\n",
    "    w = np.zeros(x.shape[1]) # Initialize parameters as 1\n",
    "    \n",
    "    l_list = [] # Save loss function value\n",
    "    for i in range(num_iter): # Gradient descent iterations\n",
    "        z = np.dot(x, w) # Linear function\n",
    "        h = sigmoid(z) # Sigmoid function\n",
    "        \n",
    "        g = gradient(x, h, y) # Calculate the gradient\n",
    "        w -= lr * g # Calculate the step size and perform the gradient descent with lr\n",
    "        \n",
    "        z = np.dot(x, w) # Update the parameters\n",
    "        h = sigmoid(z) # Get Sigmoid value\n",
    "        \n",
    "        l = loss(h, y) # Get loss value\n",
    "        l_list.append(l)\n",
    "        \n",
    "    return l_list\n",
    "\n",
    "lr = 0.01 # Learning rate\n",
    "num_iter = 30000 # Iterations\n",
    "l_y = Logistic_Regression(x, y, lr, num_iter) # Train\n",
    "\n",
    "#Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([i for i in range(len(l_y))], l_y)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b47b0f0",
   "metadata": {},
   "source": [
    "You will find that after the iteration has reached 20,000 the data tends to be stable, which is close to the minimum value of the loss function. You can change your learning rate and number of iterations yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ac9a3",
   "metadata": {},
   "source": [
    "### Logistic Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27215c97",
   "metadata": {},
   "source": [
    "Above we have understood the principle of logistic regression and its Python implementation. This process is cumbersome, but it still makes sense. We highly recommend that you at least figure out 80% of the content in the section of principle. Next we introduce the logistic regression method in scikit-learn, which is much simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03944ef",
   "metadata": {},
   "source": [
    "In scikit-learn, the class and its default parameters for implementing logistic regression are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b01b3",
   "metadata": {},
   "source": [
    "```python<br>\n",
    "LogisticRegression(penalty=â€™l2â€™, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=â€™liblinearâ€™, max_iter=100, multi_class=â€™ovrâ€™, verbose=0, warm_start=False, n_jobs=1)<br>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7531ae",
   "metadata": {},
   "source": [
    "We introduce some commonly used parameters, for the rest their default values may be used:<br>\n",
    "<br>\n",
    "- `penalty`: L2 by default. Used to specify the norm used in penalization.<br>\n",
    "- `dual`: False by default. Dual or primal formulation. <br>\n",
    "- `tol`: Tolerance for stopping criteria.<br>\n",
    "- `fit_intercept`: Specifies if a constant should be added to the decision function.<br>\n",
    "- `random_state`: The seed of the pseudo random number generator to use when shuffling the data.<br>\n",
    "- `max_iter`: 100 by default. Maximum number of iterations taken for the solvers to converge.<br>\n",
    "<br>\n",
    "Besidesï¼Œ`solver` is used to specify methods to solve loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4210f2",
   "metadata": {},
   "source": [
    "So, the code wherein we use scikit-learn to build a logistic regression classifier is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(tol=0.001, max_iter=10000) # Set the same learning rate and iterations\n",
    "model.fit(x, y)\n",
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2817d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db7a75",
   "metadata": {},
   "source": [
    "You may find that the parameters obtained are inconsistent with the parameters obtained with the Python implementation above, for our solvers are different. Similarly, we can draw the resulting classification boundary line into a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot a graph\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['X0'],df['X1'], c=df['Y'])\n",
    "\n",
    "x1_min, x1_max = df['X0'].min(), df['X0'].max(),\n",
    "x2_min, x2_max = df['X1'].min(), df['X1'].max(),\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "probs = (np.dot(grid, model.coef_.T) + model.intercept_).reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, probs, levels=[0], linewidths=1, colors='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e676daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fee26e",
   "metadata": {},
   "source": [
    "Finally, we can look at the classification accuracy of the model on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820118c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e0f1d",
   "metadata": {},
   "source": [
    "So, back to the beginning of the course: What's the relationship between logistic regression and linear regression? I believe you should already have your own answer.<br>\n",
    "<br>\n",
    "In general opinion, the word \"logistic\" is the abbreviation of **logistic distribution** and also represents the logic between _right_ and _wrong_, $0$ and $1$, symbolizing binary classification problem. The word \"regression\" is derived from **linear regression**. We construct linear classification boundaries through linear functions to achieve the classification results.<br>\n",
    "<br>\n",
    "What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54da752",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151212c9",
   "metadata": {},
   "source": [
    "In this chapter, we learned a classification method called logistic regression. Logistic regression is a very common and practical binary classification method that is often used in practical problems such as spam judgment. In addition, logistic regression can also work for multi-classification problems; but since we are going to learn other methods that are more dominant in those problems, we will not explain them here. The knowledge points you need to master first are:<br>\n",
    "<br>\n",
    "- The relationship between logistic regression and linear regression<br>\n",
    "- Logistic regression model<br>\n",
    "- Logarithmic loss function<br>\n",
    "- Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d3650",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308e5b36",
   "metadata": {},
   "source": [
    "<div style=\"color: #999;font-size: 12px;font-style: italic;\">* Congratulations! You've completed the Supervised Learning: Classification.</div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
