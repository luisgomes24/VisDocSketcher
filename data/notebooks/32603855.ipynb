{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81edb5ce",
   "metadata": {},
   "source": [
    "#####  This notebook is for beginners by beginner . In this session will predict survival using most basic problem of Titanic Survivals using different Classifiers finally we will use XGboost to classify  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88465b",
   "metadata": {},
   "source": [
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "from joblib import Parallel, delayed\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a59162",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c4bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d61077",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a12d2cc",
   "metadata": {},
   "source": [
    "## Data Analysing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92bbf51",
   "metadata": {},
   "source": [
    "Finding number of dulicated and null value in our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42deaae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of duplicate rows: {train.duplicated().sum()}\\nnumber of null values:\\n{train.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d7767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of duplicate rows: {test.duplicated().sum()}\\nnumber of null values:\\n{test.isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f452c",
   "metadata": {},
   "source": [
    "To plot multiple pairwise bivariate distributions in a dataset, we can use the .pairplot() function. \n",
    "\n",
    "The diagonal plots are the univariate plots, and this displays the relationship for the (n, 2) combination of variables in a DataFrame as a matrix of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3877f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train,palette = [\"#8000ff\",\"#da8829\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c8c876",
   "metadata": {},
   "source": [
    "#### Heatmaps\n",
    "The main intention of Seaborn heatmap is to visualize the correlation matrix of data for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e478a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.corr(), annot=True, fmt=\".1f\", cmap = sns.color_palette(\"coolwarm\", 12), mask = np.zeros_like(train.corr()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf672e95",
   "metadata": {},
   "source": [
    "#### Now we perform Label encoding that is converting categorical data in numberical since most of ML algorithms works with numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a5dd5",
   "metadata": {},
   "source": [
    "For binary label encoding we can use lambda function to apply values. \n",
    "Here we add two new columns to our data and remove other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['catSex'] = train['Sex'].apply(lambda x: 1 if x == 'male' else 0).astype('int')\n",
    "train['catEmbark'] = train['Embarked'].apply(lambda x: 0 if x =='S' else(1 if x == 'C' else '2')).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['catSex'] = test['Sex'].apply(lambda x: 1 if x == 'male' else 0).astype('int')\n",
    "test['catEmbark'] = test['Embarked'].apply(lambda x: 0 if x =='S' else(1 if x == 'C' else '2')).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in ['Name', 'Ticket', 'Cabin', 'Sex', 'Embarked']:\n",
    "    train.drop(n, axis=1, inplace = True)\n",
    "for n in ['Name', 'Ticket', 'Cabin', 'Sex', 'Embarked']:\n",
    "    test.drop(n, axis=1, inplace = True)\n",
    "train.dropna(inplace = True)\n",
    "\n",
    "test['Age'].fillna(test['Age'].mode()[0], inplace=True)\n",
    "test['Fare'].fillna(test['Fare'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c175c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7bbd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c033833",
   "metadata": {},
   "source": [
    "Now our both training and test sets are completely numerical we can move further for filling missing values if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of duplicate rows: {train.duplicated().sum()}\\nnumber of null values:\\n{train.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea30881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of duplicate rows: {test.duplicated().sum()}\\nnumber of null values:\\n{test.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif_(X, thresh=5.0):\n",
    "    variables = [X.columns[i] for i in range(X.shape[1])]\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        dropped=False\n",
    "        print(len(variables))\n",
    "        vif = Parallel(n_jobs=-1,verbose=5)(delayed(variance_inflation_factor)(X[variables].values, ix) for ix in range(len(variables)))\n",
    "\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            print(time.ctime() + ' dropping \\'' + X[variables].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "            variables.pop(maxloc)\n",
    "            dropped=True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print([variables])\n",
    "    return X[[i for i in variables]]\n",
    "\n",
    "\n",
    "vif = calculate_vif_(train.loc[:, train.columns != 'Survived']) \n",
    "vif.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39719793",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.loc[:, train.columns != 'Survived'].values, train['Survived'].values\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc695229",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lazypredict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130bcb8a",
   "metadata": {},
   "source": [
    "#### Lazy Predict\n",
    "Lazy Predict is one of the best python libraries that helps you to semi-automate your Machine Learning Task. It builds a lot of basic models without much code and helps understand which models work better without any parameter tuning.\n",
    "\n",
    "Suppose we have a problem statement and we really need to apply all the models on that particular dataset and we have to analyze that how our basic model is performing. Here basic model means “Model without parameters”. So we can do this task directly using Lazy Predict. After getting all accuracy we can choose the top 5 models and then apply hyperparameter tuning to them. It provides a Lazy Classifier to solve the classification problem and Lazy Regressor to solve the regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15dbfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=False, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_cv, y_train, y_cv)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a8bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = models.sort_values('Accuracy', ascending = False).head(10)\n",
    "sns.barplot(x = temp_df['Accuracy'], y = temp_df.index).set_title('Top 10 models based on Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146892e",
   "metadata": {},
   "source": [
    "#### AUC-ROC\n",
    "AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = models.sort_values('ROC AUC', ascending = False).head(10)\n",
    "sns.barplot(x = temp_df['ROC AUC'], y = temp_df.index).set_title('Top 10 models based on AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa41458",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "The F1-score combines the precision and recall of a classifier into a single metric by taking their harmonic mean. It is primarily used to compare the performance of two classifiers. Suppose that classifier A has a higher recall, and classifier B has higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = models.sort_values('F1 Score', ascending = False).head(10)\n",
    "sns.barplot(x = temp_df['F1 Score'], y = temp_df.index).set_title('Top 10 models based on F1 Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd0ac57",
   "metadata": {},
   "source": [
    "Now we use XGboost as our classifier for final predications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a66827",
   "metadata": {},
   "source": [
    "We can do further tuning by knowing less relevant features and removing from data functions used model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb58f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_ftr_imp = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_df = pd.DataFrame(xgb_ftr_imp, index = train.loc[:,train.columns != 'Survived'].columns) \\\n",
    ".reset_index() \\\n",
    ".rename(columns = {0: 'importance', 'index': 'feature'}) \\\n",
    ".sort_values('importance', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f301fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=feature_importances_df['feature'], x = feature_importances_df['importance']).set_title('Importance of features for predicting Survival using XGBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cea7ca",
   "metadata": {},
   "source": [
    "And hence we can see \"Parch\" and \"Fare\" are less relevent and doesnt contribute in Predicting hence we remove them. which will also help to increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34510526",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in ['Parch', 'Fare']:\n",
    "    train.drop(n, axis=1, inplace = True)\n",
    "for n in ['Parch', 'Fare']:\n",
    "    test.drop(n, axis=1, inplace = True)\n",
    "train.dropna(inplace = True)\n",
    "X, y = train.loc[:, train.columns != 'Survived'].values, train['Survived'].values\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LazyClassifier(verbose=0,ignore_warnings=False, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_cv, y_train, y_cv)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba74ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c1a486",
   "metadata": {},
   "source": [
    "##### Finally we made prediction and stored them submission.csv file. And yes ready to submit this file to know score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0fdf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Survived'] = model.predict(test.values)\n",
    "submission = test[['PassengerId','Survived']]\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a12bd4",
   "metadata": {},
   "source": [
    "### Conclusions \n",
    "1. We made successfull predictions using XGboost as classifier.\n",
    "2. We also understood how import it is to analysize and not directly fit data in model.\n",
    "3. I learned new technique of Lazy learner in this Model which I learned from amazing notebooks available on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83add368",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
