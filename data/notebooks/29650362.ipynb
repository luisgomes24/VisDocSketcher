{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [\"10\", \"6\"] # Chance default parameters when plotting axes\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # Suppressing warnings\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205747b",
   "metadata": {},
   "source": [
    "# Getting Data and First Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/insurance/insurance.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting information from data\n",
    "print(\"Samples in data: {}\\n\".format(data.index.stop)) # Show the number of samples in the data\n",
    "data.info() # There are 1338 samples and no null elements in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing the central tendency, dispersion and shape of the dataset\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dfd6ba",
   "metadata": {},
   "source": [
    "# Distribution and Relations of Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of bmi\n",
    "sns.set(style='whitegrid')\n",
    "sns.histplot(data=data, x=\"bmi\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50393c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between \"smoker\" and \"charges\"\n",
    "sns.catplot(x=\"smoker\", y=\"charges\", data=data, kind=\"box\", palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9faa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between \"region\" and \"smoker\"\n",
    "\n",
    "data.groupby([\"region\",\"smoker\"]).size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa479064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smokers based on region\n",
    "sns.countplot(x=\"region\", hue=\"smoker\", data=data, palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation between \"age\" and \"bmi\"\n",
    "sns.lineplot(x=\"age\", y=\"bmi\", hue=\"sex\", data=data, palette=\"Paired\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Children count based on region\n",
    "children_count = data.groupby(\"region\")[\"children\"].sum()\n",
    "children_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = children_count.index.tolist(), y = children_count, palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e757197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between \"age\" and \"bmi\"\n",
    "sns.lineplot(x=\"age\", y=\"bmi\", data=data, palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c22f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between \"children\" and \"bmi\"\n",
    "data.groupby([\"children\"])[\"bmi\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = \"children\", y = \"bmi\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbf666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relations between \"bmi\" and \"charges\"\n",
    "sns.scatterplot(x = \"bmi\", y=\"charges\", hue=\"sex\", data=data, palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relations between \"region\", \"smoker\" and \"bmi\"\n",
    "sns.barplot(x = \"region\", y = \"bmi\", hue = \"smoker\", data=data, palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3a1a9",
   "metadata": {},
   "source": [
    "# Encoding Categorical Variables\n",
    "\n",
    "We need to encode categorical variables before including them into the train data. \n",
    "Approaching that situation says:\n",
    "- If there is a hierarchy among data, it is ordinal. (Use LabelEncoder)\n",
    "- If there is no hierarchy among the data, it is nominal. (Use OneHotEncoder)\n",
    "\n",
    "There is no hierarchical order in categorical variables which are \"sex\", \"smoker\" and \"region\". \n",
    "\n",
    "Therefore I will use OneHotEncoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas's get_dummies func easier than using TensorFlow's OneHotEncoder to convert data\n",
    "encoded_data = pd.get_dummies(data = data, columns=[\"sex\", \"smoker\", \"region\"])\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59397884",
   "metadata": {},
   "source": [
    "# BMI Outlier Detection with IQR\n",
    "\n",
    "One last touch will be the outlier detection of BMI before training.\n",
    "\n",
    "The concept of the Interquartile Range (IQR) is used to build the boxplot graphs. IQR is a concept in statistics that is used to measure the statistical dispersion and data variability by dividing the dataset into quartiles.\n",
    "In simple words, any dataset or any set of observations is divided into four defined intervals based upon the values of the data and how they compare to the entire dataset. A quartile is what divides the data into three points and four intervals.\n",
    "\n",
    "It is the difference between the third quartile and the first quartile (IQR = Q3 -Q1). Outliers in this case are defined as the observations that are below (Q1 âˆ’ 1.5x IQR) or boxplot lower whisker or above (Q3 + 1.5x IQR) or boxplot upper whisker. It can be visually represented by the box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a382426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting BMI values\n",
    "sorted_bmi = encoded_data[\"bmi\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1, Q2, Q3 and IQR.\n",
    "Q1 = np.percentile(sorted_bmi, 25, interpolation = 'midpoint') \n",
    "Q2 = np.percentile(sorted_bmi, 50, interpolation = 'midpoint') \n",
    "Q3 = np.percentile(sorted_bmi, 75, interpolation = 'midpoint') \n",
    "  \n",
    "print('Q1 25 percentile of the BMI values is, ', Q1)\n",
    "print('Q1 50 percentile of the BMI values is, ', Q2)\n",
    "print('Q1 75 percentile of the BMI values is, ', Q3)\n",
    "  \n",
    "IQR = Q3 - Q1 \n",
    "print('Interquartile range is', IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1360ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lower and upper limits as Q1 â€“ 1.5 IQR and Q3 + 1.5 IQR, respectively\n",
    "low_lim = Q1 - 1.5 * IQR\n",
    "up_lim = Q3 + 1.5 * IQR\n",
    "print('low_limit is', low_lim)\n",
    "print('up_limit is', up_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ea653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find outliers in the dataset\n",
    "outliers =[]\n",
    "for x in sorted_bmi:\n",
    "    if ((x> up_lim) or (x<low_lim)):\n",
    "         outliers.append(x)\n",
    "print(' Outliers in the dataset is', outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf24045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paint red outlier areas on the boxplot\n",
    "sns.boxplot(sorted_bmi, palette=\"Paired\")\n",
    "plt.axvspan(xmin = low_lim, xmax = sorted_bmi.min(), alpha=0.3, color='red')\n",
    "plt.axvspan(xmin = up_lim, xmax = sorted_bmi.max(), alpha=0.3, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping outliers from data\n",
    "\n",
    "clean_data = encoded_data[encoded_data[\"bmi\"] < 47.41]\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the outliers with a graph\n",
    "sns.boxplot(clean_data[\"bmi\"].sort_values(), palette=\"Paired\")\n",
    "plt.axvspan(xmin = low_lim, xmax = clean_data[\"bmi\"].sort_values().min(), alpha=0.3, color='red')\n",
    "plt.axvspan(xmin = up_lim, xmax = clean_data[\"bmi\"].sort_values().max(), alpha=0.3, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef4a328",
   "metadata": {},
   "source": [
    "# Splitting data into train and test\n",
    "\n",
    "To do this, I'll use sklearn.model_selection.train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting dependent (y: \"charges\") and independent (X: other features) variables \n",
    "X = clean_data.drop([\"charges\"], axis=1)\n",
    "y = clean_data[\"charges\"]\n",
    "\n",
    "# Splitting X and y into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303d91ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking splitted data\n",
    "print(X_train.ndim, X_test.ndim, y_train.ndim, y_test.ndim)\n",
    "print(\"\")\n",
    "print(type(X_train),type(X_test),type(y_train),type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db666bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting pandas Series to DataFrames\n",
    "y_train = y_train.to_frame()\n",
    "y_test = y_test.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff6a3e3",
   "metadata": {},
   "source": [
    "# Make data points closer by scaling\n",
    "\n",
    "The most widely used types of normalization techniques in machine learning are:\n",
    "- Min-max (MinMaxScaler)\n",
    "- Z-score (StandardScaler)\n",
    "- Log scaling (FunctionTransformer)\n",
    "\n",
    "In order to implement the above techniques, the following functions are used to achieve functionality:\n",
    "\n",
    "The fit(data): This method helps compute the mean and std dev for a given feature, which assists in further scaling.\n",
    "\n",
    "The transform(data): This supports performing scaling using mean and std dev calculated using the .fit() method.\n",
    "\n",
    "The fit_transform(): This method performs both fit and transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc0302",
   "metadata": {},
   "source": [
    "# Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1708c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data for better modelling performance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for i in [X_train, X_test, y_train, y_test]:\n",
    "  scaler.fit_transform(i)\n",
    "  print(scaler.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ea95a",
   "metadata": {},
   "source": [
    "# Training and Model Comparison\n",
    "\n",
    "For the modeling part I will compare 7 known algorithms for Regression:\n",
    "\n",
    "1: Linear Regression\n",
    "\n",
    "2: k-Nearest Neighbors Regressor\n",
    "\n",
    "3: DecisionTreeRegressor\n",
    "\n",
    "4: RandomForestRegressor\n",
    "\n",
    "5: AdaBoostRegressor\n",
    "\n",
    "6: GradientBoostingRegressor\n",
    "\n",
    "7: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f880c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and comparing models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Report function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating models with relevant class names\n",
    "linear_regression_model = LinearRegression()\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=7)\n",
    "\n",
    "tree_model = DecisionTreeRegressor(max_depth = 3)\n",
    "\n",
    "random_forest_model = RandomForestRegressor(max_depth = 3, n_estimators=500)\n",
    "\n",
    "ada_model = AdaBoostRegressor( n_estimators=50, learning_rate =.01)\n",
    "\n",
    "gradient_boosting_model = GradientBoostingRegressor(max_depth=2, n_estimators=100, learning_rate =.2)\n",
    "\n",
    "xgb_model = XGBRegressor(max_depth = 3, n_estimators=50, learning_rate =.2, objective = \"reg:squarederror\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba8068",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [('Linear Regression', linear_regression_model), \n",
    "              ('k Nearest Neighbours', knn_model),\n",
    "              ('Decision Tree', tree_model),\n",
    "              ('Random Forest', random_forest_model),\n",
    "              ('AdaBoost', ada_model),\n",
    "              ('Gradient Boosting Regressor', gradient_boosting_model), \n",
    "              ('XGBoost', xgb_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the regression model\n",
    "r2_scores = []\n",
    "\n",
    "for regressor_name, model in regressors:\n",
    "    \n",
    "    # checking model accuracy \n",
    "    scores = cross_val_score(model, X_train, y_train.values.ravel(), cv=5, scoring=\"r2\")\n",
    "    r2_scores.append(scores.mean()*100)\n",
    "\n",
    "    print(\"{} r2 Score:\".format(regressor_name),str(round(sum(scores)/5*100,3))+\"%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5552d",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "parameters = {\n",
    "              'eta': [.03, .07, 0.1], #(eta or shrinkage)\n",
    "              \"min_child_weight\": [.1, .5, 1],\n",
    "              \"max_depth\" : [4, 5, 6, 7]}\n",
    "\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb,\n",
    "                        parameters,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1\n",
    "                        )\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f7092",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of RMSE, RMAE and r2_score\n",
    "    \n",
    "# Checking model accuracy \n",
    "rmse = cross_val_score( XGBRegressor(learning_rate=0.03, max_depth = 4, min_child_weight = 0.1), X_train, y_train ,cv=5, scoring = \"neg_mean_squared_error\")\n",
    "print(\"Mean Squared Error of XGBoost:\", (rmse, 3))\n",
    "\n",
    "rmae = cross_val_score( XGBRegressor(learning_rate=0.07, max_depth = 4, min_child_weight = 0.1), X_train, y_train ,cv=5, scoring = \"neg_mean_absolute_error\")\n",
    "print(\"Mean Absolute Error of XGBoost:\", (rmae, 3))\n",
    "\n",
    "r2 = cross_val_score( XGBRegressor(learning_rate=0.07, max_depth = 4, min_child_weight = 0.1), X_train, y_train ,cv=5, scoring = \"r2\").mean() # Generally used to determine how good is the model. \n",
    "print(\"r2 Score of XGBoost:\",str(round(r2*100,4))+\"%\\n\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
