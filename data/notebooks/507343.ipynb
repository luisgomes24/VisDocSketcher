{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb81d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_output([\"ls\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888df3bb",
   "metadata": {},
   "source": [
    "First thing first@\n",
    "# Credits to the following awesome authors and kernels\n",
    "\n",
    "Author: QuantScientist    \n",
    "File: sub_200_ens_densenet.csv     \n",
    "Link: https://www.kaggle.com/solomonk/pytorch-cnn-densenet-ensemble-lb-0-1538     \n",
    "\n",
    "\n",
    "Author: wvadim     \n",
    "File: sub_TF_keras.csv     \n",
    "Link: https://www.kaggle.com/wvadim/keras-tf-lb-0-18     \n",
    "\n",
    "\n",
    "Author: Ed Miller    \n",
    "File: sub_fcn.csv    \n",
    "Link: https://www.kaggle.com/bluevalhalla/fully-convolutional-network-lb-0-193     \n",
    "\n",
    "\n",
    "Author: Chia-Ta Tsai    \n",
    "File: sub_blend009.csv    \n",
    "Link: https://www.kaggle.com/cttsai/ensembling-gbms-lb-203    \n",
    "\n",
    "\n",
    "Author: DeveshMaheshwari    \n",
    "File: sub_keras_beginner.csv    \n",
    "Link: https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d       \n",
    "\n",
    "### Without their truly dedicated efforts, this notebook will not be possible.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198cc81",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd89fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_path = \"../input/statoil-iceberg-submissions\"\n",
    "all_files = os.listdir(sub_path)\n",
    "\n",
    "# Read and concatenate submissions\n",
    "outs = [pd.read_csv(os.path.join(sub_path, f), index_col=0) for f in all_files]\n",
    "concat_sub = pd.concat(outs, axis=1)\n",
    "cols = list(map(lambda x: \"is_iceberg_\" + str(x), range(len(concat_sub.columns))))\n",
    "concat_sub.columns = cols\n",
    "concat_sub.reset_index(inplace=True)\n",
    "concat_sub.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "concat_sub.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b04ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data fields ready for stacking\n",
    "concat_sub['is_iceberg_max'] = concat_sub.iloc[:, 1:6].max(axis=1)\n",
    "concat_sub['is_iceberg_min'] = concat_sub.iloc[:, 1:6].min(axis=1)\n",
    "concat_sub['is_iceberg_mean'] = concat_sub.iloc[:, 1:6].mean(axis=1)\n",
    "concat_sub['is_iceberg_median'] = concat_sub.iloc[:, 1:6].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up cutoff threshold for lower and upper bounds, easy to twist \n",
    "cutoff_lo = 0.8\n",
    "cutoff_hi = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b9143",
   "metadata": {},
   "source": [
    "# Mean Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub['is_iceberg'] = concat_sub['is_iceberg_mean']\n",
    "concat_sub[['id', 'is_iceberg']].to_csv('stack_mean.csv', \n",
    "                                        index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ce645",
   "metadata": {},
   "source": [
    "**LB 0.1698** , decent first try - still some gap comparing with our top-line model performance in stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4abc557",
   "metadata": {},
   "source": [
    "# Median Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub['is_iceberg'] = concat_sub['is_iceberg_median']\n",
    "concat_sub[['id', 'is_iceberg']].to_csv('stack_median.csv', \n",
    "                                        index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57dc577",
   "metadata": {},
   "source": [
    "**LB 0.1575**, very close with our top-line model performance, but we want to see some improvement at least."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325359b0",
   "metadata": {},
   "source": [
    "# PushOut + Median Stacking \n",
    "\n",
    "Pushout strategy is a bit agressive given what it does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub['is_iceberg'] = np.where(np.all(concat_sub.iloc[:,1:6] > cutoff_lo, axis=1), 1, \n",
    "                                    np.where(np.all(concat_sub.iloc[:,1:6] < cutoff_hi, axis=1),\n",
    "                                             0, concat_sub['is_iceberg_median']))\n",
    "concat_sub[['id', 'is_iceberg']].to_csv('stack_pushout_median.csv', \n",
    "                                        index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db844069",
   "metadata": {},
   "source": [
    "**LB 0.1940**, not very impressive results given the base models in the pipeline..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de3073",
   "metadata": {},
   "source": [
    "# MinMax + Mean Stacking\n",
    "\n",
    "MinMax seems more gentle and it outperforms the previous one given its peformance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94663588",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub['is_iceberg'] = np.where(np.all(concat_sub.iloc[:,1:6] > cutoff_lo, axis=1), \n",
    "                                    concat_sub['is_iceberg_max'], \n",
    "                                    np.where(np.all(concat_sub.iloc[:,1:6] < cutoff_hi, axis=1),\n",
    "                                             concat_sub['is_iceberg_min'], \n",
    "                                             concat_sub['is_iceberg_mean']))\n",
    "concat_sub[['id', 'is_iceberg']].to_csv('stack_minmax_mean.csv', \n",
    "                                        index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41788b8a",
   "metadata": {},
   "source": [
    "**LB 0.1622**, need to stack with Median to see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d245f",
   "metadata": {},
   "source": [
    "# MinMax + Median Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8fbff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub['is_iceberg'] = np.where(np.all(concat_sub.iloc[:,1:6] > cutoff_lo, axis=1), \n",
    "                                    concat_sub['is_iceberg_max'], \n",
    "                                    np.where(np.all(concat_sub.iloc[:,1:6] < cutoff_hi, axis=1),\n",
    "                                             concat_sub['is_iceberg_min'], \n",
    "                                             concat_sub['is_iceberg_median']))\n",
    "concat_sub[['id', 'is_iceberg']].to_csv('stack_minmax_median.csv', \n",
    "                                        index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabaffe5",
   "metadata": {},
   "source": [
    "**LB 0.1488** - **Great!** This is an improvement to our top-line model performance (LB 0.1538). But can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a66bd0",
   "metadata": {},
   "source": [
    "# MinMax + BestBase Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5495732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model with best base performance\n",
    "sub_base = pd.read_csv('../input/statoil-iceberg-submissions/sub_200_ens_densenet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52201c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub['is_iceberg_base'] = sub_base['is_iceberg']\n",
    "concat_sub['is_iceberg'] = np.where(np.all(concat_sub.iloc[:,1:6] > cutoff_lo, axis=1), \n",
    "                                    concat_sub['is_iceberg_max'], \n",
    "                                    np.where(np.all(concat_sub.iloc[:,1:6] < cutoff_hi, axis=1),\n",
    "                                             concat_sub['is_iceberg_min'], \n",
    "                                             concat_sub['is_iceberg_base']))\n",
    "concat_sub[['id', 'is_iceberg']].to_csv('stack_minmax_bestbase.csv', \n",
    "                                        index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a94765",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515c76e",
   "metadata": {},
   "source": [
    "**LB 0.1463** - **Yes!** This is a decent score given none of the models in our ensemble pipeline has achieved thus better. I am sure there are more twisted ways to boost the score further, so will keep updating or just leave to more Kagglers to discover!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528d5bf",
   "metadata": {},
   "source": [
    "\n",
    "### P.S. As I wrote along this work, deeply I think, building strong & roboust model is always the key component, stacking only comes last with the promise to surprise, sometimes, in an unpleasant direction@ \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
