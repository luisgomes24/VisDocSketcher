{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf775abb",
   "metadata": {},
   "source": [
    "# Estimating Fish Weight- Using Multiple Linear Regression\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#lg-skt\">Multiple Linear Regression Using Scikit Learn Library</a></li>\n",
    "<li><a href=\"#lg-grd\">Multiple Linear Regression Using Gradient Descent Implementation</a></li>\n",
    "<li><a href=\"#lg-ne\">Multiple Linear Regression Using Normal Equation</a></li>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8a1a5",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b98908",
   "metadata": {},
   "source": [
    "Using the fish market data set we will try to create a model to estimate fish weight, this model will be based on multiple linear regression.\n",
    "\n",
    "We will be trying out three approaches:\n",
    "    1. Using the Scikit library.\n",
    "    2. Using the gradient descent implementation which gives us a way to fine tune hyper parameters like learning rate.\n",
    "    3. Using Normal equation implementation, this equation is helpful in a way that is does not require us to go through iterations and chossing learning rate as is the case with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2127eb0",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/fish-market/Fish.csv')\n",
    "print(df.shape)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab7056",
   "metadata": {},
   "source": [
    "Species -->\tspecies name of fish\n",
    "\n",
    "Weight -->\tweight of fish in Gram g\n",
    "\n",
    "Length1 --> vertical length in cm\n",
    "\n",
    "Length2 -->\tdiagonal length in cm\n",
    "\n",
    "Length3 -->\tcross length in cm\n",
    "\n",
    "Height -->\theight in cm\n",
    "\n",
    "Width -->\tdiagonal width in cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Species.unique())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b52c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2554c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff43187",
   "metadata": {},
   "outputs": [],
   "source": [
    "for specie in df.Species.unique():\n",
    "    df_spe=df.query('Species==@specie')\n",
    "    print(specie)\n",
    "    for col in df_spe.columns:\n",
    "        if(col =='Species'):continue\n",
    "        df_spe.boxplot(col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57437a42",
   "metadata": {},
   "source": [
    "Looking at the Specie wise box plots, we observe that for \"Roach\" specie, there are two records which are having anamoly.\n",
    "    1. In one record the Weight is zero.\n",
    "    2. In second record, the Weight, Length1, Length2, Length3, Height and Width measurements are all outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('Species==\"Roach\" & (Weight ==0 | Weight>350)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd93f3",
   "metadata": {},
   "source": [
    "So, we will drop the record with Index 54, since in this record all the parameters are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop([54])\n",
    "df.query('Species ==\"Roach\"').describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd40bcd",
   "metadata": {},
   "source": [
    "For the record where the weight is zero, looking at this record we see that the Length1,Length2,Length3 and width for this record lies around the 1st Quartile, and since this is just one record, we chose to replace the weight 0 with the 1st quartile value of weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61550c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[40,1]=df.query('Species ==\"Roach\"').describe().T['25%'].Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614542d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('Species ==\"Roach\"').describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07af0e2",
   "metadata": {},
   "source": [
    "Now we will look at the attributes in a consolidated way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864256fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if(col =='Species'):continue\n",
    "    df.boxplot(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe7c06",
   "metadata": {},
   "source": [
    "Here we observe that there are three outliers in 'Weight' attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('Weight> 1500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba72027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.query('Weight<= 1500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('Weight> 1500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(df, kind='scatter', hue='Species');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2026abb",
   "metadata": {},
   "source": [
    "<a id='lg-skt'></a>\n",
    "### Multiple Linear Regression Using Scikit Learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "df['Species']= label_encoder.fit_transform(df['Species']) \n",
    "\n",
    "df['Species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(['Weight'] , axis=1, inplace=False)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23245e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df[df.columns[1:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LinearRegression()\n",
    "lstSeed=[]\n",
    "lstRMSQ=[]\n",
    "lstRSq=[]\n",
    "for seed in range(0,150,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    lg.fit(X_train, y_train) #training the algorithm\n",
    "    pred = lg.predict(X_test)\n",
    "    root_mean_sq = np.sqrt(metrics.mean_squared_error(y_test,pred))\n",
    "    r_sq = metrics.r2_score(y_test,pred)\n",
    "    lstRSq.append(r_sq)\n",
    "    lstSeed.append(seed)\n",
    "    lstRMSQ.append(root_mean_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d90f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric=pd.DataFrame({\n",
    "    'Seed': lstSeed, \n",
    "    'RMSQ': lstRMSQ,\n",
    "    'RSQ': lstRSq})\n",
    "df_metric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4191cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=df_metric.plot('Seed', 'RMSQ',legend=False)\n",
    "ax2 = ax.twinx()\n",
    "df_metric.plot('Seed', 'RSQ', ax=ax2,color=\"r\",legend=False)\n",
    "ax.figure.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194742b",
   "metadata": {},
   "source": [
    "Looking at the plot we can choose the seed value to be 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "lg.fit(X_train, y_train) #training the algorithm\n",
    "pred = lg.predict(X_test)\n",
    "print('root mean sq:',np.sqrt(metrics.mean_squared_error(y_test,pred)))\n",
    "print('r squared:',metrics.r2_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fbb229",
   "metadata": {},
   "source": [
    "<a id='lg-grd'></a>\n",
    "### Multiple Linear Regression Using Gradient Descent Implementation\n",
    "\n",
    "In this section we will explicitly implement gradient descent and cost function, we will tune various parameters like learning rate, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5adaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(['Weight'] , axis=1, inplace=False)\n",
    "y= df[df.columns[1:2]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "X_train_inter = np.ones((X_train.shape[0],1))\n",
    "X_train = np.concatenate((X_train_inter, X_train), axis = 1)\n",
    "\n",
    "X_test_inter = np.ones((X_test.shape[0],1))\n",
    "X_test = np.concatenate((X_test_inter, X_test), axis = 1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003034fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X,y,theta):\n",
    "    #number of training examples\n",
    "    m= len(y)\n",
    "    hypothesis= X.dot(theta)\n",
    "    #Take a summation of the squared values\n",
    "    delta=np.sum(np.square(hypothesis-y))\n",
    "    J=(1/(2*m))*delta\n",
    "    return J\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    #number of training examples\n",
    "    m, n = np.shape(X)\n",
    "    x_t = X.transpose()\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "    for i in range(num_iters):\n",
    "        hypothesis = np.dot(X,theta)-y\n",
    "        gradient = np.dot(x_t, hypothesis) / m\n",
    "        #update the theta\n",
    "        theta = theta- alpha*gradient\n",
    "        J_history[i]=np.sum(hypothesis**2) / (2*m)\n",
    "    return theta,J_history\n",
    "\n",
    "def predict(x_test,theta):\n",
    "    n = len(x_test)\n",
    "    predicted_vals=[]\n",
    "    for i in range(0,n):\n",
    "        predicted_vals.append(np.matmul(theta.T,x_test[i,:]))\n",
    "    return predicted_vals\n",
    "\n",
    "def runEpoch(X,y,theta,alpha,iterations,epochs):\n",
    "    dicGradient={}\n",
    "    dicRMSQ={}\n",
    "    dicRSQ={}\n",
    "    dicJ_Hist={}\n",
    "    J_hist=[]\n",
    "    X_t_act, X_valid, y_t_act, y_valid = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    for epoch in range(epochs):\n",
    "        print('Running Epoch {}'.format(epoch))\n",
    "        theta,J_History=gradientDescent(X_t_act,y_t_act,theta,alpha,iterations)\n",
    "        dicGradient[epoch]=(theta,J_History)\n",
    "        J_hist.extend(J_History)\n",
    "        pred_vals=predict(X_valid,theta)\n",
    "        root_mean_sq = np.sqrt(metrics.mean_squared_error(y_valid,pred_vals))\n",
    "        r_sq = metrics.r2_score(y_valid,pred_vals)\n",
    "        dicRMSQ[epoch]=root_mean_sq\n",
    "        print('Epoch {0}: RMSQ {1}'.format(epoch,root_mean_sq))\n",
    "        dicRSQ[epoch]=r_sq\n",
    "    key_min = min(dicRMSQ.keys(), key=(lambda k: dicRMSQ[k]))\n",
    "    return dicGradient[key_min][0],J_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=X_train.shape[1]\n",
    "theta=np.zeros((n, 1))\n",
    "theta,J_History=runEpoch(X_train,y_train,theta,0.00065,4000,25)\n",
    "print(theta)\n",
    "plt.plot(J_History);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede06763",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals=predict(X_test,theta)\n",
    "preds=[]\n",
    "for pred in pred_vals:\n",
    "    preds.append(abs(pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35824ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_sq = np.sqrt(metrics.mean_squared_error(y_test,preds))\n",
    "r_sq = metrics.r2_score(y_test,preds)\n",
    "print('root mean sq:',root_mean_sq)\n",
    "print('r squared:',r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02a368",
   "metadata": {},
   "source": [
    "<a id='lg-ne'></a>\n",
    "### Multiple Linear Regression Using Normal Equation\n",
    "\n",
    "Normal equation, will for some linear regression (usually when features are less than aprroximately 10000) problems gives us a much better way to solve the optimal value of $\\theta$\n",
    "\n",
    "**Normal Equation**\n",
    "\n",
    "$\\theta = (X^{T}X)^{-1}X^{T}y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e701bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEquation(X,y):\n",
    "    x_trans=X.T\n",
    "    inv=np.linalg.pinv(np.dot(x_trans,X))\n",
    "    theta=np.dot(np.dot(inv,x_trans),y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_ne= normalEquation(X_train,y_train)\n",
    "print(theta_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48aab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals=predict(X_test,theta_ne)\n",
    "preds=[]\n",
    "for pred in pred_vals:\n",
    "    preds.append(abs(pred[0]))\n",
    "root_mean_sq = np.sqrt(metrics.mean_squared_error(y_test,preds))\n",
    "r_sq = metrics.r2_score(y_test,preds)\n",
    "print('root mean sq:',root_mean_sq)\n",
    "print('r squared:',r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba0095",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3be8e4",
   "metadata": {},
   "source": [
    "In context of this data set we can see that the gradient descent implementation gives a bit better result than Scikit library or using Normal equation, this may be attributed to tuning parameters available with gradient descent.\n",
    "But gradient descent has drawback with respect to Normal Equation, that it has to go through lot more iterations (time consuming) and we need to choose learning rate."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
