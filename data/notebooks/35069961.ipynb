{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17356e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **DATA PROCESSING**\n",
    "\n",
    "import numpy as np # Array Porcessing \n",
    "import pandas as pd # Data Processing\n",
    "\n",
    "# **DATA ANALYSIS**\n",
    "\n",
    "import matplotlib.pyplot as plt # Plots\n",
    "import seaborn as sns # Graphs\n",
    "\n",
    "# ****\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # Scaling of Data\n",
    "\n",
    "# **MACHINE LEARNING MODELS**\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# **METRICS**\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "\n",
    "# **INPUT**\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49eaa2c",
   "metadata": {},
   "source": [
    "Lets get our data into working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac84a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/car-price-prediction-challenge/car_price_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be94dde",
   "metadata": {},
   "source": [
    "It is a good practice to take a look at our dataset before processing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e605684",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1083a2e",
   "metadata": {},
   "source": [
    "So we have $19237$ rows and $18$ columns accounting to $347,886$. `ID` column seems to be unique for all the values so just drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678455cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"ID\" , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d8e76",
   "metadata": {},
   "source": [
    "And now our dataset, looks like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56310843",
   "metadata": {},
   "source": [
    "Lets get a info of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f84775",
   "metadata": {},
   "source": [
    "Now we need to work on this dataset, lets work chronological here. `Price` is the target of our model and is in the integer type. Next comes the `Levy` column, which is kind of tax column in object datatype, so we need to convert this into integer type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(to_replace = \"-\" , value = 0 , inplace = True)\n",
    "tra_1 = data[\"Levy\"].astype(int)\n",
    "data.drop(\"Levy\" , axis = 1 , inplace = True)\n",
    "data = pd.concat([data , tra_1] , axis = 1 , join = \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d18bc",
   "metadata": {},
   "source": [
    "Now we have managed with the `Levy` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0761d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cef8c6",
   "metadata": {},
   "source": [
    "Next comes the `Leather interior` column. This is in `yes` and `no`. format. It would be great if we denoted `yes with 1` and `no with 0`. We are using `replace` keyword here. There are three more options like `get_dummies` from pandas `OneHotEncoder` and `Ordinal Encoder` from sklearn. They will also do the same thing, just doing it in a different way. In case you want to use those here is the code.\n",
    "```\n",
    "# *****************************REPLACE*****************************\n",
    "import pandas as pd \n",
    "\n",
    "data.replace(to_replace , value  , inplace = True)\n",
    "\n",
    "# *****************************PANDAS GET DUMMIES*****************************\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# **************METHOD 1**************\n",
    "\n",
    "data = pd.get_dummies(dataframe , columns , drop_first = True)\n",
    "\n",
    "# **************METHOD 2**************\n",
    "\n",
    "data = pd.get_dummies(dataframe , columns)\n",
    "\n",
    "# *****************************ORDINAL ENCODER*****************************\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# **************METHOD 1**************\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "data = oe.fit_transform(data)\n",
    "new_data = pd.DataFrame(new_data)\n",
    "\n",
    "sample_data = data.drop(cat , axis = 1)\n",
    "\n",
    "data_proc = pd.concat([sample_data , new_data] , axis = 1 , join = \"inner\")\n",
    "data_proc = pd.DataFrame(data_proc)\n",
    "\n",
    "# **************METHOD 2**************\n",
    "\n",
    "oe = OrdinalEncoder(columns = [\"No\" , \"Yes\"])\n",
    "data = oe.fit_transform(data)\n",
    "new_data = pd.DataFrame(new_data)\n",
    "\n",
    "sample_data = data.drop(cat , axis = 1)\n",
    "\n",
    "data_proc = pd.concat([sample_data , new_data] , axis = 1 , join = \"inner\")\n",
    "data_proc = pd.DataFrame(data_proc)\n",
    "\n",
    "# *****************************ONE HOT ENCODER*****************************\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.preprocssing import OneHotEncoder\n",
    "\n",
    "# **************METHOD 1**************\n",
    "\n",
    "ohe = OneHotEncoder(drop = \"first\" , sparse = False)\n",
    "\n",
    "new_data = ohe.fit_transform(data[cat])\n",
    "new_data = pd.DataFrame(new_data)\n",
    "\n",
    "sample_data = data.drop(cat , axis = 1)\n",
    "\n",
    "data_proc = pd.concat([sample_data , new_data] , axis = 1 , join = \"inner\")\n",
    "data_proc = pd.DataFrame(data_proc)\n",
    "\n",
    "# **************METHOD 2**************\n",
    "\n",
    "ohe = OneHotEncoder(drop = \"first\" , sparse = True)\n",
    "\n",
    "new_data = ohe.fit_transform(data[cat])\n",
    "new_data = new_data.to_aaray()\n",
    "new_data = pd.DataFrame(new_data)\n",
    "\n",
    "sample_data = data.drop(cat , axis = 1)\n",
    "\n",
    "data_proc = pd.concat([sample_data , new_data] , axis = 1 , join = \"inner\")\n",
    "data_proc = pd.DataFrame(data_proc)\n",
    "```\n",
    "\n",
    "I will be using `get_dummies` for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(to_replace = \"Yes\" , value = 1 , inplace = True)\n",
    "data.replace(to_replace = \"No\" , value = 0 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fdb91",
   "metadata": {},
   "source": [
    "Now lets take look a data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d661cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a92ae",
   "metadata": {},
   "source": [
    "Now lets work on the mileage column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_1 = data[\"Mileage\"].str.split(\" km\" , n = 0 , expand = True)\n",
    "data.drop(\"Mileage\" , axis = 1 , inplace = True)\n",
    "tra_2 = trans_1[0].astype(int)\n",
    "data[\"Mileage\"] = tra_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b50c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d9ec7",
   "metadata": {},
   "source": [
    "Now comes the `Engine volume` part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfdfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_2 = data[\"Engine volume\"].str.split(\" Turbo\" , n = 1 , expand = True)\n",
    "tra_3 = trans_2[0].astype(float)\n",
    "data[\"Engine volume1\"] = tra_3\n",
    "data[\"Turbo\"] = trans_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"Engine volume\" , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ea6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(to_replace = \"\" , value = 1 , inplace = True)\n",
    "data[\"Turbo\"].fillna(value = 0 , axis = 0 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5db40e",
   "metadata": {},
   "source": [
    "Now lets get a info of our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43d7b4",
   "metadata": {},
   "source": [
    "Lets now segregate the values and also plot them respectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    if data[i].dtypes == object:\n",
    "        data[i].value_counts().plot(kind = \"pie\" , autopct = \"%.2f\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        sns.distplot(data[i])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9dda1",
   "metadata": {},
   "source": [
    "As we can see applying one hot encoding on all of the columns will make our dataset very big to compute, so we will combine the smaller values in one category that is `other `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies(column , threshold , data):\n",
    "    y = column + \"_other\"\n",
    "    \n",
    "    repl = data[column].value_counts()[data[column].value_counts() <= threshold].index\n",
    "    repla = pd.get_dummies(data[column].replace(repl , y))\n",
    "    \n",
    "    data.drop(column , axis = 1 , inplace = True)\n",
    "    \n",
    "    data = pd.concat([data , repla] , axis = 1 , join = \"inner\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff7b494",
   "metadata": {},
   "source": [
    "For the manufacturere, the threshold will be 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b54c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dummies(\"Manufacturer\" , 800 , data)\n",
    "data = dummies(\"Model\" , 400 , data)\n",
    "data = dummies(\"Category\" ,500 , data)\n",
    "data = dummies(\"Fuel type\" , 800 , data)\n",
    "data = dummies(\"Color\" , 600 , data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dfcf61",
   "metadata": {},
   "source": [
    "Now comes the normal catgorical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data , columns = [\"Gear box type\" , \"Drive wheels\" , \"Doors\" , \"Wheel\"] , drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b75c8",
   "metadata": {},
   "source": [
    "Now fill the null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Turbo\"].fillna(value = 0 , axis = 0 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_3 = data[\"Turbo\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9127ce",
   "metadata": {},
   "source": [
    "Now we will divide our data into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "train , test = np.split(data.sample(frac = 1) , [int(0.8 * len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3adf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(dataframe):\n",
    "    x = dataframe.drop(\"Price\" , axis = 1)\n",
    "    y = dataframe[\"Price\"]\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    x = sc.fit_transform(x)\n",
    "    \n",
    "    return x , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , Y_train = pre(train)\n",
    "X_test , Y_test = pre(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4d4b8",
   "metadata": {},
   "source": [
    "Now lets train our model and test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = KNeighborsRegressor()\n",
    "model_0.fit(X_train , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbebfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LinearRegression()\n",
    "model_1.fit(X_train , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3729fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = RandomForestClassifier()\n",
    "model_2.fit(X_train , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = SVC()\n",
    "model_3.fit(X_train , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = DecisionTreeRegressor()\n",
    "model_4.fit(X_train , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22bcd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(Y_test , model_0.predict(X_test)))\n",
    "print(r2_score(Y_test , model_1.predict(X_test)))\n",
    "print(r2_score(Y_test , model_2.predict(X_test)))\n",
    "print(r2_score(Y_test , model_3.predict(X_test)))\n",
    "print(r2_score(Y_test , model_4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626ede7",
   "metadata": {},
   "source": [
    "Look we upgraded it to $17$ percent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd4c67",
   "metadata": {},
   "source": [
    "**THATS IT FOR TODAY GUYS**\n",
    "\n",
    "**WE WILL BE MAKING IMPROVMENTS IN FUTURE, STAY TUNED FOR THAT**\n",
    "\n",
    "**DONT FORGET TO MAKE AN UPVOTE IF YOU LIKED IT, IT HELPS :)**\n",
    "\n",
    "**PEACE OUT !!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136feb8",
   "metadata": {},
   "source": [
    "# Version Info\n",
    "\n",
    "* **Version 1 - Raw Code**\n",
    "* **Version 2 - Documnetation**\n",
    "* **Version 3 - SC Additon**\n",
    "* **Version 4 - Model Addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a3152",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
