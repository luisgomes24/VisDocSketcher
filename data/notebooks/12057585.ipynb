{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924c52db",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "This baseline model is adopted from [Lyft's example](https://github.com/lyft/l5kit/blob/master/examples/agent_motion_prediction/agent_motion_prediction.ipynb) on their l5kit repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc465a55",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "This baseline model is adopted from [Lyft's example](https://github.com/lyft/l5kit/blob/master/examples/agent_motion_prediction/agent_motion_prediction.ipynb) on their l5kit repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6225f",
   "metadata": {},
   "source": [
    "### Installing l5kit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c087e",
   "metadata": {},
   "source": [
    "### Installing l5kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pymap3d==2.1.0\n",
    "!pip install -U l5kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e175c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pymap3d==2.1.0\n",
    "!pip install -U l5kit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220f420",
   "metadata": {},
   "source": [
    "### Importing PyTorch and l5kit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b48f8",
   "metadata": {},
   "source": [
    "### Importing PyTorch and l5kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    " !pip3 install resnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    " !pip3 install resnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a85d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dbb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e4a00",
   "metadata": {},
   "source": [
    "### Prepare data path and config file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f754b772",
   "metadata": {},
   "source": [
    "### Prepare data path and config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d549028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"../input/lyft-motion-prediction-autonomous-vehicles\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "#cfg = load_config_data(\"../input/lyft-config-files/agent_motion_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ea3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"../input/lyft-motion-prediction-autonomous-vehicles\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "#cfg = load_config_data(\"../input/lyft-config-files/agent_motion_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b132fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"wide_resnet18\"\n",
    "IMG_SIZE = 224\n",
    "# --- Lyft configs ---\n",
    "cfg = {\n",
    "          'model_params': {'model_architecture': 'resnet18',\n",
    "          'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "        'model_name': \"model_resnet101_output\",\n",
    "        'lr': 1e-4,\n",
    "        'weight_path': \"/kaggle/input/lyftpretrained-resnet101/lyft_resnet101_model.pth\",\n",
    "        'train': True,\n",
    "        'predict': True},\n",
    "\n",
    "        'raster_params': {'raster_size': [IMG_SIZE, IMG_SIZE],\n",
    "          'pixel_size': [0.5, 0.5],\n",
    "          'ego_center': [0.25, 0.5],\n",
    "          'map_type': 'py_semantic',\n",
    "          'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "          'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "          'dataset_meta_key': 'meta.json',\n",
    "          'filter_agents_threshold': 0.5},\n",
    "\n",
    "        'train_data_loader': {'key': 'scenes/train.zarr',\n",
    "          'batch_size': 8,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0},\n",
    "\n",
    "        \"valid_data_loader\":{\"key\": \"scenes/validate.zarr\",\n",
    "                            \"batch_size\": 8,\n",
    "                            \"shuffle\": False,\n",
    "                            \"num_workers\": 0},\n",
    "    \n",
    "        \"sample_data_loader\": {\n",
    "        'key': 'scenes/sample.zarr',\n",
    "        'batch_size': 8,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0},\n",
    "         \n",
    "        \"test_data_loader\":{\n",
    "        'key': \"scenes/test.zarr\",\n",
    "        'batch_size': 8,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0},\n",
    "\n",
    "    \n",
    "        'train_params': {\"epochs\": 10, 'checkpoint_every_n_steps': 200,\n",
    "          'max_num_steps':1000,\n",
    "          'eval_every_n_steps': 100}\n",
    "        }\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb49bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"wide_resnet18\"\n",
    "IMG_SIZE = 224\n",
    "# --- Lyft configs ---\n",
    "cfg = {\n",
    "          'model_params': {'model_architecture': 'resnet18',\n",
    "          'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "        'model_name': \"model_resnet101_output\",\n",
    "        'lr': 1e-4,\n",
    "        'weight_path': \"/kaggle/input/lyftpretrained-resnet101/lyft_resnet101_model.pth\",\n",
    "        'train': True,\n",
    "        'predict': True},\n",
    "\n",
    "        'raster_params': {'raster_size': [IMG_SIZE, IMG_SIZE],\n",
    "          'pixel_size': [0.5, 0.5],\n",
    "          'ego_center': [0.25, 0.5],\n",
    "          'map_type': 'py_semantic',\n",
    "          'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "          'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "          'dataset_meta_key': 'meta.json',\n",
    "          'filter_agents_threshold': 0.5},\n",
    "\n",
    "        'train_data_loader': {'key': 'scenes/train.zarr',\n",
    "          'batch_size': 8,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0},\n",
    "\n",
    "        \"valid_data_loader\":{\"key\": \"scenes/validate.zarr\",\n",
    "                            \"batch_size\": 8,\n",
    "                            \"shuffle\": False,\n",
    "                            \"num_workers\": 0},\n",
    "    \n",
    "        \"sample_data_loader\": {\n",
    "        'key': 'scenes/sample.zarr',\n",
    "        'batch_size': 8,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0},\n",
    "         \n",
    "        \"test_data_loader\":{\n",
    "        'key': \"scenes/test.zarr\",\n",
    "        'batch_size': 8,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0},\n",
    "\n",
    "    \n",
    "        'train_params': {\"epochs\": 10, 'checkpoint_every_n_steps': 200,\n",
    "          'max_num_steps':1000,\n",
    "          'eval_every_n_steps': 100}\n",
    "        }\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058c98a",
   "metadata": {},
   "source": [
    "Config is where you can make your changes to have different `model_architecture`, `history_step_size`, `history_num_frames`, `batch_size`, etc. Inspect `cfg` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be9941",
   "metadata": {},
   "source": [
    "Config is where you can make your changes to have different `model_architecture`, `history_step_size`, `history_num_frames`, `batch_size`, etc. Inspect `cfg` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc04ee95",
   "metadata": {},
   "source": [
    "### Build a baseline CNN with Resnet50 backbone\n",
    "\n",
    "Size of `num_targets`: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9622f19",
   "metadata": {},
   "source": [
    "### Build a baseline CNN with Resnet50 backbone\n",
    "\n",
    "Size of `num_targets`: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg: Dict) -> torch.nn.Module:\n",
    "    # load pre-trained Conv2D model\n",
    "    model = resnet18(pretrained=True)\n",
    "\n",
    "    # change input channels number to match the rasterizer's output\n",
    "    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "    num_in_channels = 3 + num_history_channels\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        num_in_channels,\n",
    "        model.conv1.out_channels,\n",
    "        kernel_size=model.conv1.kernel_size,\n",
    "        stride=model.conv1.stride,\n",
    "        padding=model.conv1.padding,\n",
    "        bias=False,\n",
    "    )\n",
    "    # change output size to (X, Y) * number of future states\n",
    "    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "    model.fc = nn.Linear(in_features=512, out_features=num_targets)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254bb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg: Dict) -> torch.nn.Module:\n",
    "    # load pre-trained Conv2D model\n",
    "    model = resnet18(pretrained=True)\n",
    "\n",
    "    # change input channels number to match the rasterizer's output\n",
    "    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "    num_in_channels = 3 + num_history_channels\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        num_in_channels,\n",
    "        model.conv1.out_channels,\n",
    "        kernel_size=model.conv1.kernel_size,\n",
    "        stride=model.conv1.stride,\n",
    "        padding=model.conv1.padding,\n",
    "        bias=False,\n",
    "    )\n",
    "    # change output size to (X, Y) * number of future states\n",
    "    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "    model.fc = nn.Linear(in_features=512, out_features=num_targets)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data, model, device, criterion):\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "    # Forward pass\n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    loss = criterion(outputs, targets)\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data, model, device, criterion):\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "    # Forward pass\n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    loss = criterion(outputs, targets)\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef4293",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732956c",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd01f7d",
   "metadata": {},
   "source": [
    "In the config file the `train_data_loader`'s key is a sample zarr file. Change this to `train.zarr` file either by doing something like below or chaning in the config file itself. If you are using kaggle's GPU, you can increase the batch size too. The default batch is 16 and it only takes around 2GB of GPU memory while you train. The number of workers to load the data is set to 16. You can reduce this a bit to put less work on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72504d16",
   "metadata": {},
   "source": [
    "In the config file the `train_data_loader`'s key is a sample zarr file. Change this to `train.zarr` file either by doing something like below or chaning in the config file itself. If you are using kaggle's GPU, you can increase the batch size too. The default batch is 16 and it only takes around 2GB of GPU memory while you train. The number of workers to load the data is set to 16. You can reduce this a bit to put less work on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb68896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT TRAIN DATASET============================================================\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"], \n",
    "                             num_workers=train_cfg[\"num_workers\"])\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT TRAIN DATASET============================================================\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"], \n",
    "                             num_workers=train_cfg[\"num_workers\"])\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d36edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT VALIDATION DATASET============================================================\n",
    "valid_cfg = cfg[\"valid_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "validate_zarr = ChunkedDataset(dm.require(valid_cfg[\"key\"])).open()\n",
    "valid_dataset = AgentDataset(cfg, validate_zarr, rasterizer)\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=valid_cfg[\"shuffle\"], batch_size=valid_cfg[\"batch_size\"], \n",
    "                             num_workers=valid_cfg[\"num_workers\"])\n",
    "print(\"==================================VALIDATION DATA==================================\")\n",
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT VALIDATION DATASET============================================================\n",
    "valid_cfg = cfg[\"valid_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "validate_zarr = ChunkedDataset(dm.require(valid_cfg[\"key\"])).open()\n",
    "valid_dataset = AgentDataset(cfg, validate_zarr, rasterizer)\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=valid_cfg[\"shuffle\"], batch_size=valid_cfg[\"batch_size\"], \n",
    "                             num_workers=valid_cfg[\"num_workers\"])\n",
    "print(\"==================================VALIDATION DATA==================================\")\n",
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d097f36",
   "metadata": {},
   "source": [
    "### Build model, set optimizer and loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b620070f",
   "metadata": {},
   "source": [
    "### Build model, set optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8516b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== INIT MODEL\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(cfg).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== INIT MODEL\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(cfg).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b568a",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Train for 1000 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c672bdc0",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Train for 1000 steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe160d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, valid_dataloader, opt=None, criterion=None, lrate=1e-4):\n",
    "        \"\"\"Function for training the model\"\"\"\n",
    "        print(\"Building Model...\")\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = build_model(cfg).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        criterion = nn.MSELoss(reduction=\"none\")\n",
    "                             \n",
    "                \n",
    "        print(\"Training...\")\n",
    "        losses = []\n",
    "        losses_mean = []\n",
    "        \n",
    "        val_losses = []\n",
    "        val_losses_mean = []\n",
    "        \n",
    "        progress = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
    "        \n",
    "        train_iter = iter(train_dataloader)\n",
    "        val_iter = iter(valid_dataloader)\n",
    "        \n",
    "        for i in progress:\n",
    "            try:\n",
    "                data = next(train_iter)\n",
    "            except StopIteration:\n",
    "                train_iter = iter(train_dataloader)\n",
    "                data = next(train_iter)\n",
    "                    \n",
    "            model.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "                    \n",
    "            loss, _ = forward(data, model, device, criterion)\n",
    "                        \n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            if VALIDATION:\n",
    "                with torch.no_grad():\n",
    "                    try:\n",
    "                        val_data = next(val_iter)\n",
    "                    except StopIteration:\n",
    "                        val_iter = iter(val_dataloader)\n",
    "                        val_data = next(val_iter)\n",
    "\n",
    "                    val_loss, _  = forward(val_data, model, device, criterion)\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    val_losses_mean.append(np.mean(val_losses))\n",
    "                    \n",
    "                desc = f\"Loss: {round(loss.item(), 4)} Validation Loss: {round(val_loss.item(), 4)}\"\n",
    "            else:\n",
    "                desc = f\"Loss: {round(loss.item(), 4)}\"\n",
    "                \n",
    "            #if len(losses)>0 and loss < min(losses):\n",
    "            #    print(f\"Loss improved from {min(losses)} to {loss}\")\n",
    "                \n",
    "            \n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            losses_mean.append(np.mean(losses))\n",
    "            progress.set_description(desc)\n",
    "            \n",
    "        return losses_mean, val_losses_mean, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1da80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, valid_dataloader, opt=None, criterion=None, lrate=1e-4):\n",
    "        \"\"\"Function for training the model\"\"\"\n",
    "        print(\"Building Model...\")\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = build_model(cfg).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        criterion = nn.MSELoss(reduction=\"none\")\n",
    "                             \n",
    "                \n",
    "        print(\"Training...\")\n",
    "        losses = []\n",
    "        losses_mean = []\n",
    "        \n",
    "        val_losses = []\n",
    "        val_losses_mean = []\n",
    "        \n",
    "        progress = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
    "        \n",
    "        train_iter = iter(train_dataloader)\n",
    "        val_iter = iter(valid_dataloader)\n",
    "        \n",
    "        for i in progress:\n",
    "            try:\n",
    "                data = next(train_iter)\n",
    "            except StopIteration:\n",
    "                train_iter = iter(train_dataloader)\n",
    "                data = next(train_iter)\n",
    "                    \n",
    "            model.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "                    \n",
    "            loss, _ = forward(data, model, device, criterion)\n",
    "                        \n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            if VALIDATION:\n",
    "                with torch.no_grad():\n",
    "                    try:\n",
    "                        val_data = next(val_iter)\n",
    "                    except StopIteration:\n",
    "                        val_iter = iter(val_dataloader)\n",
    "                        val_data = next(val_iter)\n",
    "\n",
    "                    val_loss, _  = forward(val_data, model, device, criterion)\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    val_losses_mean.append(np.mean(val_losses))\n",
    "                    \n",
    "                desc = f\"Loss: {round(loss.item(), 4)} Validation Loss: {round(val_loss.item(), 4)}\"\n",
    "            else:\n",
    "                desc = f\"Loss: {round(loss.item(), 4)}\"\n",
    "                \n",
    "            #if len(losses)>0 and loss < min(losses):\n",
    "            #    print(f\"Loss improved from {min(losses)} to {loss}\")\n",
    "                \n",
    "            \n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            losses_mean.append(np.mean(losses))\n",
    "            progress.set_description(desc)\n",
    "            \n",
    "        return losses_mean, val_losses_mean, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, val_losses, model = train(train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, val_losses, model = train(train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Analysis\n",
    "plt.plot(losses, c=\"red\", label=\"Mean Training Loss\")\n",
    "plt.plot(val_losses, c=\"green\", label=\"Mean Validation Loss\")\n",
    "plt.xlabel('Training step', fontsize=12) \n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Analysis\n",
    "plt.plot(losses, c=\"red\", label=\"Mean Training Loss\")\n",
    "plt.plot(val_losses, c=\"green\", label=\"Mean Validation Loss\")\n",
    "plt.xlabel('Training step', fontsize=12) \n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3005b7",
   "metadata": {},
   "source": [
    "### Load evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba866825",
   "metadata": {},
   "source": [
    "### Load evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf45aa",
   "metadata": {},
   "source": [
    "Due to the fact that the following steps take way too long, they are commented out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15100f51",
   "metadata": {},
   "source": [
    "Due to the fact that the following steps take way too long, they are commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading eval dataset\n",
    "eval_cfg = cfg[\"sample_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "eval_dataloader = DataLoader(eval_dataset, \n",
    "                             shuffle=eval_cfg[\"shuffle\"], \n",
    "                             batch_size=eval_cfg[\"batch_size\"], \n",
    "                             num_workers=eval_cfg[\"num_workers\"])\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading eval dataset\n",
    "eval_cfg = cfg[\"sample_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "eval_dataloader = DataLoader(eval_dataset, \n",
    "                             shuffle=eval_cfg[\"shuffle\"], \n",
    "                             batch_size=eval_cfg[\"batch_size\"], \n",
    "                             num_workers=eval_cfg[\"num_workers\"])\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== EVAL LOOP\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "\n",
    "agent_ids = []\n",
    "progress_bar = tqdm(eval_dataloader)\n",
    "for data in progress_bar:\n",
    "    _, ouputs = forward(data, model, device, criterion)\n",
    "    future_coords_offsets_pd.append(ouputs.cpu().numpy().copy())\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== EVAL LOOP\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "\n",
    "agent_ids = []\n",
    "progress_bar = tqdm(eval_dataloader)\n",
    "for data in progress_bar:\n",
    "    _, ouputs = forward(data, model, device, criterion)\n",
    "    future_coords_offsets_pd.append(ouputs.cpu().numpy().copy())\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff970f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = compute_error_csv(eval_gt_path, pred_path)\n",
    "print(f\"NLL: {error:.5f}\\nL2: {np.sqrt(2*error/cfg['model_params']['future_num_frames']):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fea3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = compute_error_csv(eval_gt_path, pred_path)\n",
    "print(f\"NLL: {error:.5f}\\nL2: {np.sqrt(2*error/cfg['model_params']['future_num_frames']):.5f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
