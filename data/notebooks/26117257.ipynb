{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5df83",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*wli6Okh8p3vgaH69PdijkA.png)medium.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c281af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d18192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import kornia as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7c175",
   "metadata": {},
   "source": [
    "#Load an image with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bgr: np.array = cv2.imread('../input/image-matching-challenge-2022/train/taj_mahal/images/05106170_2123793679.jpg')  # HxWxC / np.uint8\n",
    "img_rgb: np.array = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img_rgb); plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565019be",
   "metadata": {},
   "source": [
    "#Load an image with Torchvision\n",
    "\n",
    "It returns the images in a torch.Tensor in the shape (C,H,W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05383ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rgb: torch.tensor = torchvision.io.read_image('../input/image-matching-challenge-2022/train/taj_mahal/images/05106170_2123793679.jpg')  # CxHxW / torch.uint8\n",
    "x_rgb = x_rgb.unsqueeze(0)  # BxCxHxW\n",
    "print(x_rgb.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be8beb",
   "metadata": {},
   "source": [
    "#Load an image with Kornia\n",
    "\n",
    "\"The utility is kornia.image_to_tensor which casts a numpy.ndarray to a torch.Tensor and permutes the channels to leave the image ready for being used with any other PyTorch or Kornia component. The image is casted into a 4D torch.Tensor with zero-copy.\"\n",
    "\n",
    "https://kornia-tutorials.readthedocs.io/en/latest/hello_world_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fdd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by https://kornia-tutorials.readthedocs.io/en/latest/hello_world_tutorial.html\n",
    "\n",
    "x_bgr: torch.tensor = K.image_to_tensor(img_bgr)  # CxHxW / torch.uint8\n",
    "x_bgr = x_bgr.unsqueeze(0)  # 1xCxHxW\n",
    "print(f\"convert from '{img_bgr.shape}' to '{x_bgr.shape}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05796e",
   "metadata": {},
   "source": [
    "#Convert from BGR to RGB with a kornia.color component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rgb: torch.tensor = K.color.bgr_to_rgb(x_bgr)  # 1xCxHxW / torch.uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34207fd6",
   "metadata": {},
   "source": [
    "#Visualize an image with Matplotib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d421cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bgr: np.array = K.tensor_to_image(x_bgr)\n",
    "img_rgb: np.array = K.tensor_to_image(x_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f47569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by https://kornia-tutorials.readthedocs.io/en/latest/hello_world_tutorial.html\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(32, 16))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].axis('off')\n",
    "axs[0].imshow(img_rgb)\n",
    "\n",
    "axs[1].axis('off')\n",
    "axs[1].imshow(img_bgr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7e625",
   "metadata": {},
   "source": [
    "#Let's take a shot with T≈çdai-ji (Temple in Nara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25016a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by https://kornia-tutorials.readthedocs.io/en/latest/hello_world_tutorial.html\n",
    "\n",
    "img_bgr1: np.array = cv2.imread('../input/image-matching-challenge-2022/train/temple_nara_japan/images/07628787_6482156727.jpg')  # HxWxC / np.uint8\n",
    "img_rgb1: np.array = cv2.cvtColor(img_bgr1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img_rgb1); plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by https://kornia-tutorials.readthedocs.io/en/latest/hello_world_tutorial.html\n",
    "\n",
    "x_bgr1: torch.tensor = K.image_to_tensor(img_bgr1)  # CxHxW / torch.uint8\n",
    "x_bgr1 = x_bgr1.unsqueeze(0)  # 1xCxHxW\n",
    "print(f\"convert from '{img_bgr1.shape}' to '{x_bgr1.shape}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert from BGR to RGB with a kornia.color component.\n",
    "\n",
    "x_rgb1: torch.tensor = K.color.bgr_to_rgb(x_bgr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize an image with Matplotib\n",
    "\n",
    "img_bgr1: np.array = K.tensor_to_image(x_bgr1)\n",
    "img_rgb1: np.array = K.tensor_to_image(x_rgb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by https://kornia-tutorials.readthedocs.io/en/latest/hello_world_tutorial.html\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(32, 16))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].axis('off')\n",
    "axs[0].imshow(img_rgb1)\n",
    "\n",
    "axs[1].axis('off')\n",
    "axs[1].imshow(img_bgr1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by https://kornia-tutorials.readthedocs.io/en/latest/data_augmentation_sequential.html\n",
    "\n",
    "from kornia import augmentation as K\n",
    "from kornia.augmentation import AugmentationSequential\n",
    "from kornia.geometry import bbox_to_mask\n",
    "from kornia.utils import image_to_tensor, tensor_to_image\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "def plot_resulting_image(img, bbox, keypoints, mask):\n",
    "    img = img * mask\n",
    "    img_draw = cv2.polylines(np.array(to_pil(img)), bbox.numpy(), isClosed=True, color=(255, 0, 0))\n",
    "    for k in keypoints[0]:\n",
    "        img_draw = cv2.circle(img_draw, tuple(k.numpy()[:2]), radius=6, color=(255, 0, 0), thickness=-1)\n",
    "    return img_draw\n",
    "\n",
    "img = cv2.imread(\"../input/image-matching-challenge-2022/train/piazza_san_marco/images/07961084_7175726083.jpg\", cv2.IMREAD_COLOR)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "img_tensor = image_to_tensor(img).float() / 255.\n",
    "plt.imshow(img); plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159b972",
   "metadata": {},
   "source": [
    "#Define Augmentation Sequential and Different Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by https://kornia-tutorials.readthedocs.io/en/latest/data_augmentation_sequential.html\n",
    "\n",
    "aug_list = AugmentationSequential(\n",
    "    K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=1.0),\n",
    "    K.RandomAffine(360, [0.1, 0.1], [0.7, 1.2], [30., 50.], p=1.0),\n",
    "    K.RandomPerspective(0.5, p=1.0),\n",
    "    data_keys=[\"input\", \"bbox\", \"keypoints\", \"mask\"],\n",
    "    return_transform=False,\n",
    "    same_on_batch=False,\n",
    ")\n",
    "\n",
    "bbox = torch.tensor([[[355,10],[660,10],[660,250],[355,250]]])\n",
    "keypoints = torch.tensor([[[465, 115], [545, 116]]])\n",
    "mask = bbox_to_mask(torch.tensor([[[155,0],[900,0],[900,400],[155,400]]]), w, h).float()\n",
    "\n",
    "img_out = plot_resulting_image(img_tensor, bbox, keypoints, mask)\n",
    "plt.imshow(img_out); plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ca733",
   "metadata": {},
   "source": [
    "#Forward Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by https://kornia-tutorials.readthedocs.io/en/latest/data_augmentation_sequential.html\n",
    "\n",
    "out_tensor = aug_list(img_tensor, bbox.float(), keypoints.float(), mask)\n",
    "img_out = plot_resulting_image(\n",
    "    out_tensor[0][0],\n",
    "    out_tensor[1].int(),\n",
    "    out_tensor[2].int(),\n",
    "    out_tensor[3][0],\n",
    ")\n",
    "plt.imshow(img_out); plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a46e7b",
   "metadata": {},
   "source": [
    "#Inverse Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tensor_inv = aug_list.inverse(*out_tensor)\n",
    "img_out = plot_resulting_image(\n",
    "    out_tensor_inv[0][0],\n",
    "    out_tensor_inv[1].int(),\n",
    "    out_tensor_inv[2].int(),\n",
    "    out_tensor_inv[3][0],\n",
    ")\n",
    "plt.imshow(img_out); plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c551b6",
   "metadata": {},
   "source": [
    "#Patch Augmentation Sequential with patchwise_apply=True\n",
    "\n",
    "patchwise_apply is a feature that used to define unique processing pipeline for each patch location. If patchwise_apply=True, the number of pipelines defined must be as same as the number of patches in an image.\n",
    "\n",
    "https://kornia-tutorials.readthedocs.io/en/latest/data_patch_sequential.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by https://kornia-tutorials.readthedocs.io/en/latest/data_patch_sequential.html\n",
    "\n",
    "from kornia.augmentation import PatchSequential, ImageSequential\n",
    "\n",
    "pseq = PatchSequential(\n",
    "    ImageSequential(\n",
    "        K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n",
    "        K.RandomPerspective(0.2, p=0.5),\n",
    "        K.RandomSolarize(0.1, 0.1, p=0.5),\n",
    "    ),\n",
    "    K.RandomAffine(15, [0.1, 0.1], [0.7, 1.2], [0., 20.], p=0.5),\n",
    "    K.RandomPerspective(0.2, p=0.5),\n",
    "    ImageSequential(\n",
    "        K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n",
    "        K.RandomPerspective(0.2, p=0.5),\n",
    "        K.RandomSolarize(0.1, 0.1, p=0.5),\n",
    "    ),\n",
    "    K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n",
    "    K.RandomAffine(15, [0.1, 0.1], [0.7, 1.2], [0., 20.], p=0.5),\n",
    "    K.RandomPerspective(0.2, p=0.5),\n",
    "    K.RandomSolarize(0.1, 0.1, p=0.5),\n",
    "    K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n",
    "    K.RandomAffine(15, [0.1, 0.1], [0.7, 1.2], [0., 20.], p=0.5),\n",
    "    ImageSequential(\n",
    "        K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n",
    "        K.RandomPerspective(0.2, p=0.5),\n",
    "        K.RandomSolarize(0.1, 0.1, p=0.5),\n",
    "    ),\n",
    "    K.RandomSolarize(0.1, 0.1, p=0.5),\n",
    "    K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5),\n",
    "    K.RandomAffine(15, [0.1, 0.1], [0.7, 1.2], [0., 20.], p=0.5),\n",
    "    K.RandomPerspective(0.2, p=0.5),\n",
    "    K.RandomSolarize(0.1, 0.1, p=0.5),\n",
    "    patchwise_apply=True,\n",
    "    same_on_batch=True,\n",
    ")\n",
    "out_tensor = pseq(img_tensor[None].repeat(2, 1, 1, 1))\n",
    "to_pil(torch.cat([out_tensor[0], out_tensor[1]], dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045db43c",
   "metadata": {},
   "source": [
    "#Patch Augmentation Sequential rocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b647668",
   "metadata": {},
   "source": [
    "#Patch Augmentation Sequential with patchwise_apply=False\n",
    "\n",
    "If patchwise_apply=False, all the args will be combined and applied as one pipeline for each patch.\n",
    "\n",
    "https://kornia-tutorials.readthedocs.io/en/latest/data_patch_sequential.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by https://kornia-tutorials.readthedocs.io/en/latest/data_patch_sequential.html\n",
    "\n",
    "pseq = PatchSequential(\n",
    "    K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.75),\n",
    "    K.RandomElasticTransform(alpha=(4., 4.)),\n",
    "    patchwise_apply=False,\n",
    "    same_on_batch=False\n",
    ")\n",
    "out_tensor = pseq(img_tensor[None].repeat(2, 1, 1, 1))\n",
    "to_pil(torch.cat([out_tensor[0], out_tensor[1]], dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57dd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kornia_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec31ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import kornia as K\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kornia_moons.feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code by Aziz Amindzhanov  https://www.kaggle.com/code/azizdzhon/kornia-moons-imc-2022\n",
    "\n",
    "img = cv2.cvtColor(cv2.imread('../input/image-matching-challenge-2022/train/taj_mahal/images/05106170_2123793679.jpg'), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "det = cv2.ORB_create(500)\n",
    "kps, descs = det.detectAndCompute(img, None)\n",
    "\n",
    "out_img = cv2.drawKeypoints(img, kps, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(out_img)\n",
    "\n",
    "\n",
    "lafs = laf_from_opencv_ORB_kpts(kps)\n",
    "visualize_LAF(K.image_to_tensor(img, False), lafs, 0)\n",
    "\n",
    "kps_back = opencv_ORB_kpts_from_laf(lafs)\n",
    "out_img2 = cv2.drawKeypoints(img, kps_back, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(out_img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca8b03",
   "metadata": {},
   "source": [
    "#Acknowledgements:\n",
    "\n",
    "Kornia AI is on the mission to leverage and democratize the next generation of Computer Vision tools and Deep Learning libraries within the context of an Open Source community.\n",
    "\n",
    "https://kornia.readthedocs.io/en/latest/\n",
    "\n",
    "Aziz Amindzhanov  https://www.kaggle.com/code/azizdzhon/kornia-moons-imc-2022"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
