{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected = True)\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "\n",
    "import optuna\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.layers import Concatenate, LSTM, GRU\n",
    "from tensorflow.keras.layers import Bidirectional, Multiply\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "np.random.seed(2022)\n",
    "tf.random.set_seed(2022)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#########################################################\n",
    "train = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\n",
    "t_lbls = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\n",
    "test = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\n",
    "ss = pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e9920",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DATA INFORMATION')\n",
    "print()\n",
    "print('Count of sequences:')\n",
    "print(f'train - {int(len(train)/60)} | test - {int(len(test)/60)}')\n",
    "print()\n",
    "print('Missing values:')\n",
    "print(f'train - {train.isna().sum().sum()} | test - {test.isna().sum().sum()}')\n",
    "print()\n",
    "print('Distribution of target:')\n",
    "print(f'\"1\" - {round(t_lbls[\"state\"].value_counts()[1]/len(t_lbls)*100,2)}% | \"0\" - {round(t_lbls[\"state\"].value_counts()[0]/len(t_lbls)*100,2)}%')\n",
    "print()\n",
    "print('-'*39)\n",
    "print()\n",
    "print('Train features')\n",
    "display(train[train.columns.tolist()[3:]].describe().transpose()[['mean', 'min', 'max']]\\\n",
    ".style.background_gradient(cmap='Blues'))\n",
    "print()\n",
    "print('-'*39)\n",
    "print()\n",
    "print('Test features')\n",
    "display(test[test.columns.tolist()[3:]].describe().transpose()[['mean', 'min', 'max']]\\\n",
    ".style.background_gradient(cmap='Blues'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6efe777",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, i = list(t_lbls[t_lbls['state']==0]['sequence'][:3]) + list(t_lbls[t_lbls['state']==1]['sequence'][:3]), 0\n",
    "colors = ['#c21b1b', '#c21b1b', '#c21b1b', '#21a5de', '#21a5de', '#21a5de']\n",
    "fig = plt.figure(figsize = (15, 20))\n",
    "for sensor in train.columns.tolist()[3:]:\n",
    "    for color, seq in zip(colors, seqs):\n",
    "        i += 1\n",
    "        plt.subplot(13,6,i)\n",
    "        sns.set_style(\"white\")\n",
    "        if i < 7: \n",
    "            plt.title(f\"Sequence {seq}\", size = 12, fontname = 'monospace')\n",
    "        a = sns.lineplot(data=train[train['sequence']==seq][sensor], color = color, linewidth = 1)\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('')\n",
    "        if (i-1) % 6 == 0: \n",
    "            plt.ylabel(sensor, size = 12, fontname = 'monospace')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "fig.tight_layout(h_pad = 3)\n",
    "\n",
    "plt.figtext(0.5, 1.05, 'Sequences examples', fontsize = 23, fontname = 'monospace', ha='center')\n",
    "plt.figtext(0.22, 1.03, 'Target 0', fontsize = 20, fontname = 'monospace', color = '#c21b1b')\n",
    "plt.figtext(0.71, 1.03, 'Target 1', fontsize = 20, fontname = 'monospace', color = '#21a5de')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc355d0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2c8de",
   "metadata": {},
   "source": [
    "**For DNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d936c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.columns.tolist()[3:]\n",
    "def prep(df):\n",
    "    for feature in features:\n",
    "        df[feature+'_lag1'] = df.groupby('sequence')[feature].shift(1)\n",
    "        df.fillna(0, inplace=True)\n",
    "        df[feature+'_diff1'] = df[feature] - df[feature+'_lag1']\n",
    "\n",
    "prep(train)\n",
    "prep(test)\n",
    "\n",
    "features = train.columns.tolist()[3:]\n",
    "sc = StandardScaler()\n",
    "train[features] = sc.fit_transform(train[features])\n",
    "test[features] = sc.transform(test[features])\n",
    "\n",
    "groups = train[\"sequence\"]\n",
    "labels = t_lbls[\"state\"]\n",
    "\n",
    "train = train.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\n",
    "train = train.reshape(-1, 60, train.shape[-1])\n",
    "\n",
    "test = test.drop([\"sequence\", \"subject\", \"step\"], axis=1).values\n",
    "test = test.reshape(-1, 60, test.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5823367",
   "metadata": {},
   "source": [
    "**For XGB model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ac081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(df):\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    result['sequence'] = df['sequence'].unique()\n",
    "    result = result.merge(df[['sequence', 'subject']], on='sequence', how='left')\n",
    "    \n",
    "    for sensor in test.columns.tolist()[3:]:\n",
    "        aggs = df.groupby('sequence').agg({sensor: ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew']})\n",
    "        aggs.columns = aggs.columns.map('_'.join)\n",
    "        result = result.merge(aggs.reset_index(), on='sequence', how='left')\n",
    "        \n",
    "        aggs = df.groupby('subject').agg({sensor: ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew']})\n",
    "        aggs.columns = aggs.columns.map('_subject_'.join)\n",
    "        result = result.merge(aggs.reset_index(), on='subject', how='left')\n",
    "    \n",
    "    return result\n",
    "\n",
    "train = prep(train)\n",
    "test = prep(test)\n",
    "\n",
    "train.drop_duplicates(inplace=True)\n",
    "test.drop_duplicates(inplace=True)\n",
    "\n",
    "features = train.columns.tolist()[2:]\n",
    "sc = StandardScaler()\n",
    "train[features] = sc.fit_transform(train[features])\n",
    "test[features] = sc.transform(test[features])\n",
    "\n",
    "X = train[features]\n",
    "test = test[features]\n",
    "y = t_lbls['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7ff7e",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, data = X, target = y):\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': 10000,\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 9),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 10, 300),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0001, 1.0, log = True),\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 10.0, log = True),\n",
    "        'lambda': trial.suggest_float('lambda', 0.0001, 10.0, log = True),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'booster': 'gbtree',\n",
    "        'random_state': 2022,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'auc'\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier(**params)\n",
    "    scores = []\n",
    "    k = StratifiedKFold(n_splits = 5, random_state = 228, shuffle = True)\n",
    "    for fold, (train_idx, val_idx) in enumerate(k.split(X, y)):\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 50, verbose = False)\n",
    "        \n",
    "        val_preds = model.predict_proba(X_val)[:,1]\n",
    "        val_score = roc_auc_score(y_val, val_preds)\n",
    "\n",
    "        scores.append(val_score)\n",
    "        \n",
    "        print(f\"Fold {fold+1} | AUC: {val_score}\")\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective, n_trials = 150)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "print('Best value:', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 10000, \n",
    "               'tree_method': 'gpu_hist',\n",
    "               'booster': 'gbtree',\n",
    "               'random_state': 2022,\n",
    "               'use_label_encoder': False,\n",
    "               'eval_metric': 'auc',\n",
    "               'max_depth': 5,\n",
    "               'learning_rate': 0.013430615331501902, \n",
    "               'min_child_weight': 11, \n",
    "               'gamma': 0.2507452781368943, \n",
    "               'alpha': 0.00010885805925508797,\n",
    "               'lambda': 0.017572701090619865,\n",
    "               'colsample_bytree': 0.5450566395055472,\n",
    "               'subsample': 0.6223621722492708}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee505188",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, scores = [], []\n",
    "k = StratifiedKFold(n_splits = 10, random_state = 2022, shuffle = True)\n",
    "for fold, (train_idx, val_idx) in enumerate(k.split(X, y)):\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = XGBClassifier(**best_params)\n",
    "   \n",
    "    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 50)\n",
    "    \n",
    "    val_preds = model.predict_proba(X_val)[:,1]\n",
    "    val_score = roc_auc_score(y_val, val_preds)\n",
    "    print(f'Fold {fold+1} AUC: {round(val_score, 4)}')\n",
    "    \n",
    "    scores.append(val_score)\n",
    "    predictions.append(model.predict_proba(test)[:,1])\n",
    "print('------------------')    \n",
    "print(f'Mean AUC - {round(np.mean(scores), 4)}')\n",
    "\n",
    "predictions = sum(predictions)/k.n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14379b7",
   "metadata": {},
   "source": [
    "**Mean AUC on 10 folds - 0.9391**\n",
    "\n",
    "**LB score - 0.917**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss[\"state\"] = predictions\n",
    "ss.to_csv('submission11_XGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e39686",
   "metadata": {},
   "source": [
    "# DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    \n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    BATCH_SIZE = 256\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272dcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model():\n",
    "\n",
    "    x_input = Input(shape=(train.shape[-2:]))\n",
    "    x1 = Bidirectional(LSTM(768, return_sequences=True))(x_input)\n",
    "        \n",
    "    x21 = Bidirectional(LSTM(512, return_sequences=True))(x1)\n",
    "    x22 = Bidirectional(LSTM(512, return_sequences=True))(x_input)\n",
    "    l2 = Concatenate(axis=2)([x21, x22])\n",
    "        \n",
    "    x31 = Bidirectional(LSTM(384, return_sequences=True))(l2)\n",
    "    x32 = Bidirectional(LSTM(384, return_sequences=True))(x21)\n",
    "    l3 = Concatenate(axis=2)([x31, x32])\n",
    "        \n",
    "    x41 = Bidirectional(LSTM(256, return_sequences=True))(l3)\n",
    "    x42 = Bidirectional(LSTM(128, return_sequences=True))(x32)\n",
    "    l4 = Concatenate(axis=2)([x41, x42])\n",
    "        \n",
    "    l5 = Concatenate(axis=2)([x1, l2, l3, l4])\n",
    "    g = GlobalMaxPooling1D()(l5)\n",
    "    x7 = Dense(128, activation='selu')(g)\n",
    "    x8 = Dropout(0.2)(x7)\n",
    "    x_output = Dense(units=1, activation=\"sigmoid\")(x8)\n",
    "    \n",
    "    model = Model(inputs=x_input, outputs=x_output, name='lstm_model')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = dnn_model()\n",
    "\n",
    "plot_model(\n",
    "    model, \n",
    "    to_file='Super_Model.png', \n",
    "    show_shapes=False,\n",
    "    show_layer_names=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "    VERBOSE = True\n",
    "    predictions, scores = [], []\n",
    "    k = GroupKFold(n_splits = 15)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(k.split(train, labels, groups.unique())):\n",
    "        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
    "    \n",
    "        X_train, X_val = train[train_idx], train[val_idx]\n",
    "        y_train, y_val = labels.iloc[train_idx].values, labels.iloc[val_idx].values\n",
    "        \n",
    "        model = dnn_model()\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics='AUC')\n",
    "\n",
    "        lr = ReduceLROnPlateau(monitor=\"val_auc\", factor=0.5, \n",
    "                               patience=2, verbose=VERBOSE, mode=\"max\")\n",
    "\n",
    "        es = EarlyStopping(monitor=\"val_auc\", patience=7, \n",
    "                           verbose=VERBOSE, mode=\"max\", \n",
    "                           restore_best_weights=True)\n",
    "        \n",
    "        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "        chk_point = ModelCheckpoint(f'./TPS_model_2022_{fold+1}C.h5', options=save_locally, \n",
    "                                    monitor='val_auc', verbose=VERBOSE, \n",
    "                                    save_best_only=True, mode='max')\n",
    "        \n",
    "        model.fit(X_train, y_train, \n",
    "                  validation_data=(X_val, y_val), \n",
    "                  epochs=20,\n",
    "                  verbose=VERBOSE,\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  callbacks=[lr, chk_point, es])\n",
    "        \n",
    "        load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "        model = load_model(f'./TPS_model_2022_{fold+1}C.h5', options=load_locally)\n",
    "        \n",
    "        y_pred = model.predict(X_val, batch_size=BATCH_SIZE).squeeze()\n",
    "        score = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "        predictions.append(model.predict(test, batch_size=BATCH_SIZE).squeeze())\n",
    "        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n",
    "    \n",
    "    print(f'Mean AUC on {k.n_splits} folds - {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562960a",
   "metadata": {},
   "source": [
    "**Mean AUC on 15 folds - 0.9718**\n",
    "\n",
    "**LB score - 0.968**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss[\"state\"] = sum(predictions)/k.n_splits \n",
    "ss.to_csv('submission12.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac3bfd",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f747df2",
   "metadata": {},
   "source": [
    "Results of XGB and 2 DNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_csv('../input/tps-apr/s_0.968.csv')\n",
    "s2 = pd.read_csv('../input/tps-apr/s_0.97.csv') # the result from https://www.kaggle.com/code/hamzaghanmi/tps-april-tensorflow-bi-lstm\n",
    "s3 = pd.read_csv('../input/tps-apr/submission11_XGB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ecd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss['state'] = s1['state']*0.3 + s2['state']*0.45 + s3['state']*0.25 \n",
    "ss.to_csv('blend_sub12.csv', index=False)\n",
    "ss"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
