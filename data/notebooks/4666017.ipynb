{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649bd5fb",
   "metadata": {},
   "source": [
    "### Author: Guilherme Resende\n",
    "This practical work was developed over the [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset and is meant to analyze the data and build a predictive model capable of distinguish whether a given transaction is or is not a fraud. [Anomaly Detection](https://pt.wikipedia.org/wiki/Detec%C3%A7%C3%A3o_de_anomalias) problems are interesting because they demand a different approach given the regular methods usually do not perform well in such scenarios.\n",
    "\n",
    "Here I decided to test a stacked approach based on Wolpert's work [[DH Wolpert - 1992](https://www.sciencedirect.com/science/article/pii/S0893608005800231)]. This approach presents the input data $X_1$ to a given layer($L_1$) of learning models, combine their outputs with the actual input $X_2 = [X_1, L_1(X_1)]$ and present the new data to the next level of learning models. This procedure is performed until the structure gets to the final stack layer, that is, the final output. The outputs from inner layers are called meta-features.\n",
    "\n",
    "![stacked_generalization.png](attachment:stacked_generalization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.offline as pyo, plotly.graph_objs as go\n",
    "import missingno as msno\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import *\n",
    "from scikitplot.plotters import plot_ks_statistic\n",
    "from scikitplot.metrics import plot_ks_statistic, plot_roc\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb971fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacking(object):\n",
    "    def __init__(self, data, important_cols=None, fl_models=None, fl_models_names=None):\n",
    "        self.data = data\n",
    "        self.important_cols = important_cols\n",
    "        self.fl_models = fl_models\n",
    "        self.fl_models_names = fl_models_names\n",
    "        for name in self.fl_models_names:\n",
    "            self.data[name] = 0\n",
    "        \n",
    "        self.fl_data, self.sl_data = self._stacking_split()\n",
    "        self.sl_model = XGBClassifier()\n",
    "    \n",
    "    \n",
    "    def _fl_fit(self, X_train, Y_train):        \n",
    "        for model in self.fl_models:\n",
    "            model.fit(X_train, Y_train)\n",
    "    \n",
    "    \n",
    "    def _fl_predict(self, X_test):\n",
    "        preds = []\n",
    "        for i, model in enumerate(self.fl_models):\n",
    "             preds.append(model.predict(X_test).tolist())\n",
    "        return np.array(preds).T.tolist()\n",
    "            \n",
    "    \n",
    "    def _build_fl_models(self):\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        all_indices = []\n",
    "        preds = []\n",
    "        for index, (train_indices, test_indices) in enumerate(kf.split(self.fl_data[important_cols], self.fl_data['Fraud'])):\n",
    "            train = self.fl_data.iloc[train_indices]\n",
    "            test = self.fl_data.iloc[test_indices]\n",
    "            all_indices += test_indices.tolist()\n",
    "            \n",
    "            self._fl_fit(train[self.important_cols].values, train['Fraud'].values)\n",
    "            preds += self._fl_predict(test[important_cols].values)\n",
    "            print(\"{}/5 of data ended training\".format(index+1))\n",
    "        preds = pd.DataFrame(preds, columns=self.fl_models_names)\n",
    "        self.fl_data = self.fl_data.iloc[all_indices]\n",
    "        for name in self.fl_models_names:\n",
    "            self.fl_data[name] = preds[name].values\n",
    "\n",
    "            \n",
    "    def _sl_fit(self, X_train, Y_train):\n",
    "        self.sl_model.fit(X_train, Y_train)\n",
    "        \n",
    "        \n",
    "    def _sl_predict(self, X_test):\n",
    "        return self.sl_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    def _sl_predict_probs(self, X_test):\n",
    "        return self.sl_model.predict_proba(X_test)\n",
    "    \n",
    "    \n",
    "    def run_stacked(self):\n",
    "        tik = time.time()\n",
    "        self._build_fl_models()\n",
    "        self._sl_fit(self.fl_data[important_cols+self.fl_models_names], self.fl_data['Fraud'])\n",
    "        tok = time.time()\n",
    "        \n",
    "        minutes = int((tok-tik) / 60)\n",
    "        hours = int(minutes / 60)\n",
    "        minutes = minutes % 60\n",
    "\n",
    "        print(\"The Training Process Took {}h {} min\".format(hours, minutes))\n",
    "        \n",
    "        preds = pd.DataFrame(self._fl_predict(self.sl_data[important_cols].values), columns=self.fl_models_names)\n",
    "        for name in self.fl_models_names:\n",
    "            self.sl_data[name] = preds[name].values\n",
    "            \n",
    "        self.preds = self._sl_predict(self.sl_data[important_cols+self.fl_models_names])\n",
    "        self.probs = self._sl_predict_probs(self.sl_data[important_cols+self.fl_models_names])\n",
    "\n",
    "        \n",
    "    def _stacking_split(self, split_at=0.7):\n",
    "        frauds = self.data[self.data['Fraud'] == 1]\n",
    "        non_frauds = self.data[self.data['Fraud'] == -1]\n",
    "        fraud_pivot, non_frauds_pivot = int(len(frauds)*split_at), int(len(non_frauds)*split_at)\n",
    "\n",
    "        fl_data = pd.concat([frauds.iloc[:fraud_pivot], non_frauds.iloc[:non_frauds_pivot]])\n",
    "        sl_data = pd.concat([frauds.iloc[fraud_pivot:], non_frauds.iloc[non_frauds_pivot:]])\n",
    "        \n",
    "        return fl_data, sl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada53825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the figure size to Seaborn and Matplotlib plots\n",
    "rcParams['figure.figsize'] = [15, 15]\n",
    "# Allows Plotly to plot offline\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['V'+str(index) for index in range(29)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4969287",
   "metadata": {},
   "source": [
    "First thing we need to do is to read the data from the repository and check if everything was read as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6310be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c718e6",
   "metadata": {},
   "source": [
    "Once the data is read correctly, it is needed to check if the dataset is not corrupted and if there isno null values. As the dataset is originated from a dimensionality reduction, it is expected to not have missing values, however, it is good to make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b12acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc27bf",
   "metadata": {},
   "source": [
    "As we can see above there was no missing values in the dataset. Next thing needed is to create a naming pattern to the columns and then make sure all data is normalized. In order to standardize all column names,  I've changed 'Time' and 'Amount' to $V_0$ and $V_{29}$, respectively. Posteriorly, I do a standard scaling setting unit variance to the two columns here mentioned. As the machine learning models used below outputs -1 to anomalies/outliers and 1 to normal records/inliers and the dataset uses 1 to anomalies/outliers and 0 to normal records/inlies, I here do a mapping from $1 \\rightarrow -1$ and $0 \\rightarrow 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(index=str, columns={\"Class\": \"Fraud\", \"Time\": \"V0\", \"Amount\": \"V29\"})\n",
    "for col in [\"V0\", \"V29\"]:\n",
    "    data[col] = scale(data[col].values)\n",
    "    \n",
    "data[\"Fraud\"] = data[\"Fraud\"].apply(lambda x: -1 if x else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611a92f",
   "metadata": {},
   "source": [
    "The ideal case to treat a problem as anomaly detection problem, is when one of the classes is pretty narrow and the other one is abundant. Thus, to make sure I am, indeed, dealing with an anomaly detection problem, I present the percentage of the data that corresponds to frauds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdd0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = round(len(data[data.Fraud == -1]) / len(data) * 100, 4)\n",
    "print(\"Percentage of Frauds is {}%\".format(percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3066cbe",
   "metadata": {},
   "source": [
    "As we can see above, the frauds corresponds to only a small amount of the whole data, that is, 0.1727%, way less than 1 percent. Hence, I am indeed dealing with an anomaly detection problem!\n",
    "<p> To start going down the road to build the machine learning model to detect anomalies, I first need to check if the 'col_names' variable is as I expect it to be, that is, do not contains any undesirable column such as the fraud labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953dccde",
   "metadata": {},
   "source": [
    "Since everything is OK with 'col_names', I will now remove some of the features that do not aggregates lots of information to the analysis - present redundant information. To do that, I am going to fit a XGBoost model and then get the features importance learned by XGBoost. In order to filter the \"unecessary\" features, I will stablish a minimum importance level (previously tested, but here omitted for simplicity's sake) of 0.015, that is, every feature that has importance of at least 0.015 will remain for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data[col_names].values, data[\"Fraud\"].values\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X, Y)\n",
    "feat_importances = dict(zip(col_names, model.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee28d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_cols = []\n",
    "importance = 0\n",
    "for key, value in feat_importances.items():\n",
    "    if(value > 0.015):\n",
    "        important_cols.append(key)\n",
    "        importance += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num. Features Initially: {}\".format(len(feat_importances)))\n",
    "print(\"Num. Features After: {}\".format(len(important_cols)))\n",
    "print(\"Preserved Importance: {}\".format(round(importance, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f2d60",
   "metadata": {},
   "source": [
    "As we can see above, the feature importance analysis was able to reduce the number of features from 29 to 17 and yet, preserve $\\approx 89%$ of the total importance described by the XGBoost model. However, it is just an estimate, since I do not expect the model to correctly learn all data characteristics. \n",
    "\n",
    "<p> Next step I will take in this analysis is to plot all features histograms in order to check their display. I do not expect them all to look Gaussian like, however, it would be good if they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eedac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(nrows=4, ncols=4, sharey=True)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if(i == 3 and j > 0):\n",
    "            break\n",
    "            \n",
    "        axes[i, j].hist(data[important_cols[i*4 + j]].values, bins=75)\n",
    "        axes[i, j].set_title(important_cols[i*4 + j])\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847c486",
   "metadata": {},
   "source": [
    "As it was evidenced above, almost all features already follows a Gaussian distribution, except for $V_{6}$ and $V_4$. However, even though these columns have some deviational behaviour, they do resambles a bell shaped curve and hence will be used without problem on the following approachs.\n",
    "\n",
    "<p> For the following step, I am going to plot the features correlation in order to avoid inputting redundant information on the models I am about to build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fcb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data[important_cols].corr().values, \n",
    "            linewidth=0.5, \n",
    "            xticklabels=important_cols, \n",
    "            yticklabels=important_cols)\n",
    "\n",
    "plt.title(\"Important Features Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261ada5",
   "metadata": {},
   "source": [
    "As the plot above shows, the features are almost nothing correlated, so, there will be no problem inputting all of them together. Once this things are known, the models are ready to be defined and created. For this dataset I will develop a two level stacking approach, in which the first layer is composed by two supervised learning models (Elliptic Envelope and Isolation Forest) and one unsupervised (Local Outlier Factor), and the second layer is composed of a XGBoost classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca87966",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_names = [\"EllipticEnvelope\", \n",
    "                    \"IsolationForest\",\n",
    "                    \"LocalOutlierFactor\"]\n",
    "\n",
    "models = [EllipticEnvelope(support_fraction=0.7), \n",
    "              IsolationForest(behaviour=\"new\", contamination=\"auto\"), \n",
    "              LocalOutlierFactor(novelty=True, contamination=\"auto\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b3314",
   "metadata": {},
   "source": [
    "When dealing with supervised learning models, we need to try to reduce the chance of overfitting, that is, do not let it spend too much time trying to learn the data specificities/noise and lose the ability to generalize the results. When a model overfits we can expect it to have a great performance on the training data and have a poor performance on test data. One way to avoid overfitting is to perform K-Fold-1-Out Cross-Validation on the data. That means divide the whole dataset into K folds, train with K-1 folds and then test it on the one left out. To this work I decided to set $K=5$. Hence, as I go through the 5 left out folds, all data will have been validated with a test set.\n",
    "\n",
    "As I am dealing with a very narrow class, it would be good to make sure the number of anomalies is approximately the same over all splits. To do so, I will use a Stratified K-Fold-1-Out Cross-Validation process.For each model defined above, there will be created a new column representing the model's outputs to be used as meta-features on the second learning level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "stack = Stacking(data=data[important_cols + ['Fraud']], \n",
    "                 important_cols=important_cols, \n",
    "                 fl_models=models, \n",
    "                 fl_models_names=models_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21def90",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.run_stacked()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab23bee",
   "metadata": {},
   "source": [
    "As aforementioned, once the K-Fold-1-Out Cross-Validation process ends, every new column will have been built using only the test folds. So, for each new column created, I calculate Precision, Recall and F1-score all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = []\n",
    "precs = []\n",
    "f1s = []\n",
    "\n",
    "for name in models_names:\n",
    "    prediction = stack.fl_data[name].values\n",
    "    Y_fl = stack.fl_data['Fraud'].values\n",
    "    precs.append(precision_score(Y_fl, prediction))\n",
    "    recs.append(recall_score(Y_fl, prediction))\n",
    "    f1s.append(f1_score(Y_fl, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e407ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(x=models_names, \n",
    "                y=precs, \n",
    "                name=\"Precision\")\n",
    "trace2 = go.Bar(x=models_names, \n",
    "                y=recs, \n",
    "                name=\"Recall\")\n",
    "trace3 = go.Bar(x=models_names, \n",
    "                y=f1s, \n",
    "                name=\"F1\")\n",
    "\n",
    "traces_list = [trace1, trace2, trace3]\n",
    "layout = go.Layout(title=\"First Layer Models Performance\")\n",
    "figure = go.Figure(traces_list, layout)\n",
    "\n",
    "pyo.iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d905a5",
   "metadata": {},
   "source": [
    "As a way to check if a stacking approach is suitable for this dataset, we need to know if the first level models outputs are usually different from each other. If so, they probably will aggregate value to the learning process as a whole and improve the performance, that is, the final model only need to know who to trust in each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52853352",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Test the discordance rate over first level outputs\n",
    "df_tmp = stack.fl_data[[\"Fraud\"]]\n",
    "df_tmp[\"outputs_sum\"] = stack.fl_data[models_names].sum(axis=1).values\n",
    "\n",
    "all_anomalies = len(df_tmp[df_tmp[\"Fraud\"] == -1])\n",
    "outputs_sum = df_tmp[df_tmp[\"Fraud\"] == -1][\"outputs_sum\"].values\n",
    "\n",
    "# If it is equals 3, all first level models agreed that it was \n",
    "# not an anomaly and consequently, if it is equals -3, all first \n",
    "# level models considered a record as an anomaly\n",
    "outputs_sum = [0 if value == 3 or value == -3 else 1 for value in outputs_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs_sum[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The discordance among first level models is: {} (regarding anomalous records)\"\\\n",
    "                                                  .format(sum(outputs_sum) / all_anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12370e",
   "metadata": {},
   "source": [
    "The whole process performance can be measured contrasting the last layer outputs with the actual labels. In order to measure how well the developed approach performed I first plot the values regarding Precision, Recall and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944576e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['Precision','Recall','F1-Score']\n",
    "\n",
    "Y_sl = stack.sl_data['Fraud']\n",
    "Y = [np.round(precision_score(Y_sl, stack.preds)*100, 2), \n",
    "     np.round(recall_score(Y_sl, stack.preds)*100, 2),\n",
    "     np.round(f1_score(Y_sl, stack.preds)*100, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876107ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Bar(x=X, y=Y)\n",
    "traces = [trace]\n",
    "\n",
    "layout = go.Layout(title='Stack Performance Over Metrics')\n",
    "figure = go.Figure(traces, layout)\n",
    "\n",
    "pyo.iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec341ef7",
   "metadata": {},
   "source": [
    "As we can see, the stacking process seems to be pretty performatic, reaching almost 100% score in Precision, Recall and F1. Hence, in order to make sure the model is capable of well separate the classes, I present the ROC curve visualization together with the respective AUC metric regarding the anomalous records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c722e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the figure size to Seaborn and Matplotlib plots\n",
    "rcParams['figure.figsize'] = [9, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(Y_sl, stack.probs, plot_micro=False, plot_macro=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41341736",
   "metadata": {},
   "source": [
    "As presented above, the model seems to be, indeed, pretty good in classifying whether a record is an anomalous transation or not. Then, as a way to certify the model is not only getting the answers right by guess, I present the visualization and values of [Kolmogorov-Smirnov Test (KS Test)](https://pt.wikipedia.org/wiki/Teste_Kolmogorov-Smirnov). The KS Test points out how much a sampled distribution is close to the original distribution, in other words, considering the true labels as the original distribution and the model's output as the sampled distribution, how close is the sample probability distribution to the actual outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ks_statistic(Y_sl, stack.probs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387e19e",
   "metadata": {},
   "source": [
    "Considering that, all values are being predicted with a high confidence level, we are able to say that the approach here developed is pretty satisfactory. The approach performed way better than expected, almost mimicking all actual outputs. However, considering that, we do not have the actual data distribution the model probably got biased by the sampled distribution presented to it. In order to further validate the model, I intend to run the current approach with a new database taken from a different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1212ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
