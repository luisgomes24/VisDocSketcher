{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3bf45f",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification Challenge\n",
    "## BERT - TensorFlow 2 & Hugging Face Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87765030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DON'T TOUCH!!\n",
    "!pip install transformers\n",
    "!pip install tensorflow==2.1.0\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc181c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = '../input/aldon-preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19978868",
   "metadata": {},
   "source": [
    "## 1. Data Pipeline\n",
    "- Loading the datasets from CSVs\n",
    "- Preprocessing (Tokenization, Truncation & Padding)\n",
    "- Creating efficient data pipelines using tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc760645",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../input/aldon-preprocessed-2/preprocessed_train_data.csv'\n",
    "test_path = '../input/aldon-preprocessed-2/preprocessed_test_data.csv'\n",
    "test_labels_path = '../input/aldon-preprocessed-2/test_label.csv'\n",
    "subm_path = '../input/aldon-preprocessed/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8835d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'insult', 'threat', 'identity_hate']\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_test_labels = pd.read_csv(test_labels_path)\n",
    "df_test_labels = df_test_labels.set_index('id')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking types of each columns from df_train\n",
    "empDfObj = pd.DataFrame(df_train, columns=['id', 'comment_text', 'toxic', 'severe_toxic','obscene','threat','insult','indentity_hate'])\n",
    "print('Data type of each column of Dataframe :')\n",
    "dataTypeSeries = empDfObj.dtypes\n",
    "\n",
    "print(dataTypeSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b690a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking types of each columns from df_test\n",
    "empDfObj = pd.DataFrame(df_test, columns=['id', 'comment_text'])\n",
    "print('Data type of each column of Dataframe :')\n",
    "dataTypeSeries = empDfObj.dtypes\n",
    "\n",
    "print(dataTypeSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04484175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Bert tokenizer and masks\n",
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "bert_model_name = 'bert-base-multilingual-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
    "MAX_LEN = 128\n",
    "\n",
    "def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokenized_sentence = tokenizer.encode(\n",
    "                            sentence,                  # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_seq_len,  # Truncate all sentences.\n",
    "                    )\n",
    "        \n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "def create_attention_masks(tokenized_and_padded_sentences):\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in tokenized_and_padded_sentences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return np.asarray(attention_masks)\n",
    "\n",
    "input_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "attention_masks = create_attention_masks(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels =  df_train[label_cols].values\n",
    "test_labels = df_test[label_cols].values\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n",
    "\n",
    "train_size = len(train_inputs)\n",
    "validation_size = len(validation_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe99ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if TPU is available\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54344489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and validation datasets\n",
    "BATCH_SIZE=32 \n",
    "NR_EPOCHS=2\n",
    "def create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n",
    "    if train:\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "    dataset = dataset.repeat(epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if train:\n",
    "        dataset = dataset.prefetch(1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n",
    "validation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0d18e",
   "metadata": {},
   "source": [
    "## 2. BERT Model\n",
    "- Load the pretrained BERT base-model from Transformers library\n",
    "- Take the first hidden-state from BERT output (corresponding to CLS token) and feed it into a Dense layer with 6 neurons and sigmoid activation (Classifier). The outputs of this layer can be interpreted as probabilities for each of the 6 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "class BertClassifier(tf.keras.Model):    \n",
    "        def __init__(self, bert: TFBertModel, num_classes: int):\n",
    "            super().__init__()\n",
    "            self.bert = bert\n",
    "            self.classifier = Dense(num_classes, activation='sigmoid')\n",
    "\n",
    "        @tf.function\n",
    "        def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "            outputs = self.bert(input_ids,\n",
    "                                   attention_mask=attention_mask,\n",
    "                                   token_type_ids=token_type_ids,\n",
    "                                   position_ids=position_ids,\n",
    "                                   head_mask=head_mask)\n",
    "            cls_output = outputs[1]\n",
    "            cls_output = self.classifier(cls_output)\n",
    "\n",
    "            return cls_output\n",
    "        \n",
    "model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb12d44e",
   "metadata": {},
   "source": [
    "## 3. Training Loop\n",
    "- Use BinaryCrossentropy as loss function (is calculated for each of the output 6 output neurons ...that's like training 6 binary classification tasks at the same time) \n",
    "- Use the Adam optimizer \n",
    "- AUC evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2273d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import create_optimizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "lw = 2\n",
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "validation_steps = validation_size // BATCH_SIZE\n",
    "\n",
    "# | Loss Function\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "validation_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "# | Optimizer \n",
    "warmup_steps = steps_per_epoch // 3\n",
    "total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\n",
    "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
    "\n",
    "# | Metrics\n",
    "train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
    "validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, token_ids, masks, labels):\n",
    "    labels = tf.dtypes.cast(labels, tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(token_ids, attention_mask=masks)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    #Loss Function into gradient\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #Apply gradient to optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "\n",
    "    for i, auc in enumerate(train_auc_metrics):\n",
    "        auc.update_state(labels[:,i], predictions[:,i])\n",
    "        \n",
    "@tf.function\n",
    "def validation_step(model, token_ids, masks, labels):\n",
    "    labels = tf.dtypes.cast(labels, tf.float32)\n",
    "\n",
    "    predictions = model(token_ids, attention_mask=masks, training=False)\n",
    "    v_loss = loss_object(labels, predictions)\n",
    "\n",
    "    validation_loss(v_loss)\n",
    "    for i, auc in enumerate(validation_auc_metrics):\n",
    "        auc.update_state(labels[:,i], predictions[:,i])\n",
    "        \n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_class = 6\n",
    "\n",
    "        \n",
    "def train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print('=' * 50, f\"EPOCH {epoch+1}\", '=' * 50)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n",
    "            train_step(model, token_ids, masks, labels)\n",
    "            if i % 500 == 0:\n",
    "                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n",
    "                for i, label_name in enumerate(label_cols):\n",
    "                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n",
    "                    train_auc_metrics[i].reset_states()\n",
    "\n",
    "        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n",
    "            validation_step(model, token_ids, masks, labels)\n",
    "\n",
    "        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n",
    "\n",
    "        for i, label_name in enumerate(label_cols):\n",
    "            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n",
    "            validation_auc_metrics[i].reset_states()\n",
    "\n",
    "        print('\\n')\n",
    "    \n",
    "\n",
    "\n",
    "train(model, train_dataset, validation_dataset, steps_per_epoch, validation_steps, NR_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22620278",
   "metadata": {},
   "source": [
    "## 4. Run predictions on test-set & save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964bab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = tokenize_sentences(df_test['comment_text'], tokenizer, MAX_LEN)\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "test_attention_masks = create_attention_masks(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "labelstest = np.array(labels)\n",
    "test_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
    "\n",
    "TEST_BATCH_SIZE = 32\n",
    "test_steps = len(df_test) // TEST_BATCH_SIZE\n",
    "\n",
    "test_dataset = create_dataset((test_input_ids, test_attention_masks, test_labels), batch_size=TEST_BATCH_SIZE, train=False, epochs=1)\n",
    "\n",
    "df_submission = pd.read_csv(subm_path, index_col='id')\n",
    "\n",
    "for i, (token_ids, masks, labels) in enumerate(tqdm(test_dataset, total=test_steps)):\n",
    "    sample_ids = df_test.iloc[i*TEST_BATCH_SIZE:(i+1)*TEST_BATCH_SIZE]['id'] \n",
    "    predictions = model(token_ids, attention_mask=masks).numpy()\n",
    "    for i, auc in enumerate(test_auc_metrics):\n",
    "        auc.update_state(labels[:,i], predictions[:,i])\n",
    "    \n",
    "    df_submission.loc[sample_ids, label_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008cd3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_acc=[0,0,0,0,0,0] \n",
    "print(\"Testing Accuracy : \")\n",
    "for i, label_name in enumerate(label_cols):\n",
    "    print(f\"{label_name} roc_auc {test_auc_metrics[i].result()}\")\n",
    "    label_acc[i] = test_auc_metrics[i].result()\n",
    "    test_auc_metrics[i].reset_states()\n",
    "    \n",
    "    \n",
    "labelnames = np.array(label_acc)\n",
    "df_submission2 = df_submission[:3001]\n",
    "labelspred = df_submission2[label_cols].values\n",
    "labelstrue = df_test[label_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fcee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(labels, predictions, label):\n",
    "    #For graphs\n",
    "   \n",
    "    for i in range(6):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels[:,i], predictions[:,i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(1)\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'slategrey', 'firebrick', 'springgreen'])\n",
    "    for i, color in zip(range(n_class), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "        label='ROC curve of class '+ label_cols[i] + ' : ' + str(label[i]) )\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves of all 6 classes')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b42db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(labelstrue, labelspred, labelnames)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
