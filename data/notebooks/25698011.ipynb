{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b352299f",
   "metadata": {},
   "source": [
    "# Wide Data For Geographic Information\n",
    "\n",
    "The data in this competition is connected through both location and time.  Many notebooks make good use of the time series aspect of this competition, but I have not see n any that try to make geographic connections between the 65 x-y/direction.  This notebook is my attempt to try this. \n",
    "\n",
    "Instead of the rows being a timestamp-direction pair with the target being the congestion, my data are transformed to be only a timestamp with the targets being the congestion of all 65 x-y/direction combinations.  The features I use are the 1 day lag, row-wise statistics (mean, var, etc) of the 1 day lag features, and the daily moving median.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445da6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2022/train.csv')\n",
    "test_y =  pd.read_csv('/kaggle/input/tabular-playground-series-mar-2022/test.csv')\n",
    "ss = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2022/sample_submission.csv')\n",
    "\n",
    "\n",
    "#Scaling the TARGET values back by 100 for NN learning \n",
    "TARGET = 'congestion'\n",
    "train_y[TARGET] = train_y[TARGET] / 100\n",
    "test_y[TARGET] = 0\n",
    "\n",
    "#Turning time to DateTime\n",
    "train_y['time'] = pd.to_datetime(train_y['time'])\n",
    "test_y['time'] = pd.to_datetime(test_y['time'])\n",
    "ss['time'] = test_y['time']\n",
    "\n",
    "#Combing all location features \n",
    "train_y['xydir'] = train_y['x'].astype(str) + '_' + train_y['y'].astype(str) + train_y['direction']\n",
    "test_y['xydir'] = test_y['x'].astype(str) + '_' + test_y['y'].astype(str) + test_y['direction']\n",
    "ss['xydir'] = test_y['xydir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning the data from `long` to `wide`\n",
    "train_y = train_y[['time','xydir','congestion']].set_index(['time','xydir'])\n",
    "train_y = train_y.unstack()\n",
    "train_y.columns = train_y.columns.map(lambda x: x[1])\n",
    "train_y = train_y.reset_index()\n",
    "\n",
    "test_y = test_y[['time','xydir','congestion']].set_index(['time','xydir'])\n",
    "test_y = test_y.unstack()\n",
    "test_y.columns = test_y.columns.map(lambda x: x[1])\n",
    "test_y = test_y.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91288b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature Engineering\n",
    "'''\n",
    "#Feature lists\n",
    "RAW_FEATURES = [feat for feat in train_y.columns if feat != 'time']\n",
    "HM_MOVING_MEDIANS = [f'HMMM_{feat}' for feat in RAW_FEATURES] #For daily moving medians\n",
    "DHM_MOVING_MEDIANS = [f'DHMMM_{feat}' for feat in RAW_FEATURES] #For weekly moving medians\n",
    "ONE_HOUR_MEAN = ['1hm'+feat for feat in RAW_FEATURES] # for mean over the hour\n",
    "\n",
    "#Creating day, hour, and minute features\n",
    "train_y['day'] = train_y.time.dt.day % 7\n",
    "train_y['hour'] =train_y.time.dt.hour\n",
    "train_y['minute'] = train_y.time.dt.minute\n",
    "train_y['dhm'] = train_y['day'].astype(str) + '_' + train_y['hour'].astype(str) + '_' + train_y['minute'].astype(str)\n",
    "train_y['hm'] = train_y['hour'].astype(str) + '_' + train_y['minute'].astype(str)\n",
    "train_y['day'] = train_y['day'] / 7\n",
    "train_y['hour'] =train_y['hour'] / 24\n",
    "train_y['minute'] = train_y['minute']/40\n",
    "\n",
    "#1) Row wise mean, median, var, kurtosis\n",
    "train_y['row_kurtosis'] = train_y[RAW_FEATURES].kurtosis(axis=1)\n",
    "train_y['row_mean'] = train_y[RAW_FEATURES].mean(axis=1)\n",
    "train_y['row_median'] = train_y[RAW_FEATURES].median(axis=1)\n",
    "train_y['row_var'] = train_y[RAW_FEATURES].var(axis=1)\n",
    "\n",
    "#2) moving median\n",
    "#help from https://stackoverflow.com/questions/36969174/pandas-average-value-for-the-past-n-days\n",
    "#train_y[HM_MOVING_MEDIANS] = train_y.groupby('hm')[RAW_FEATURES].apply(lambda x: x.shift().expanding(min_periods=1).median())\n",
    "train_y[HM_MOVING_MEDIANS] = train_y.groupby('hm')[RAW_FEATURES].apply(lambda x: x.expanding(min_periods=1).median())\n",
    "\n",
    "#3) weekly moving median\n",
    "train_y[DHM_MOVING_MEDIANS] = train_y.groupby('dhm')[RAW_FEATURES].apply(lambda x: x.expanding(min_periods=1).median())\n",
    "\n",
    "#4) 1 hour mean\n",
    "train_y[ONE_HOUR_MEAN] = train_y[RAW_FEATURES].rolling(3).mean().fillna(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating train_X and test_X\n",
    "----------------------------\n",
    "\n",
    "train_X is the lag -1 day features and train_y is just the present day\n",
    "test_X, also, is one day behind test_y\n",
    "'''\n",
    "train_X = train_y[['time']].copy()\n",
    "train_X['time'] = train_X.time - pd.Timedelta(days=1)\n",
    "train_X = pd.merge(train_X, train_y, on='time', how='left')\n",
    "\n",
    "test_X = test_y[['time']].copy()\n",
    "test_X['time'] = test_X.time - pd.Timedelta(days=1)\n",
    "test_X = pd.merge(test_X, train_y, on='time', how='left')\n",
    "\n",
    "#Discarding all Nan rows from train_X, train_y\n",
    "missing = train_X['0_0EB'].isnull()\n",
    "train_X = train_X[~missing].reset_index(drop=True)\n",
    "train_y = train_y[~missing].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8480b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use train time stamps 1 week before test as validation data\n",
    "val_times = test_y.time - pd.Timedelta(days=7) \n",
    "val_msk = train_X.time.isin(val_times)\n",
    "\n",
    "val_X = train_X[val_msk].copy()\n",
    "val_y = train_y[val_msk].copy()\n",
    "\n",
    "train_X = train_X[~val_msk].copy()\n",
    "train_y = train_y[~val_msk].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vanilla_nn(FEATURES, RAW_FEATURES, DROPOUT_LEVEL, FIRST_DROPOUT, NUM_NEURONS, NUM_HIDDEN_LAYERS, lr):\n",
    "    INPUT_SHAPE, OUTPUT_SHAPE = len(FEATURES), len(RAW_FEATURES) \n",
    "    inp = tf.keras.layers.Input(shape = (INPUT_SHAPE,) )\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(FIRST_DROPOUT)(x)\n",
    "\n",
    "    for i in range(NUM_HIDDEN_LAYERS):\n",
    "        x = tf.keras.layers.Dense(NUM_NEURONS[i], activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(DROPOUT_LEVEL)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(OUTPUT_SHAPE, activation= 'sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inp, outputs= x)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss=tf.keras.losses.MeanSquaredError())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fdf7a",
   "metadata": {},
   "source": [
    "Below is my optuna trial to find both optimal features and optimal hyperparameters for the neural network.  Uncomment if you want to perform this yourself.  It takes around 1 hour for 100 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def objective(trial):\n",
    "    ###################################\n",
    "    # Generate our trial model.\n",
    "    ###################################\n",
    "    #Model Architecture specifications\n",
    "    NN_params = {}\n",
    "    FEATURES = RAW_FEATURES.copy()\n",
    "    if trial.suggest_categorical(\"hm_mm\", [True, False]):\n",
    "        FEATURES += HM_MOVING_MEDIANS\n",
    "    if trial.suggest_categorical(\"dhm_mm\", [True, False]):\n",
    "        FEATURES += DHM_MOVING_MEDIANS\n",
    "    if trial.suggest_categorical(\"one_hour_mean\", [True, False]):\n",
    "        FEATURES += ONE_HOUR_MEAN\n",
    "    if trial.suggest_categorical(\"row_stats\", [True, False]):\n",
    "        FEATURES += ['row_kurtosis', 'row_mean', 'row_median', 'row_var']\n",
    "    if trial.suggest_categorical(\"time_base\", [True, False]):\n",
    "        FEATURES += ['day', 'hour', 'minute']\n",
    "    NN_params['FEATURES'] = FEATURES\n",
    "    NN_params['RAW_FEATURES'] = RAW_FEATURES\n",
    "    NN_params[\"DROPOUT_LEVEL\"] = trial.suggest_float(\"dropout\", 0.00,0.6)\n",
    "    NN_params[\"FIRST_DROPOUT\"] = trial.suggest_float(\"first_dropout\", 0.00,0.9)\n",
    "    NN_params[\"NUM_HIDDEN_LAYERS\"] = trial.suggest_int(\"depth\", 1,8)\n",
    "    NN_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    num_neurons = []\n",
    "    for i in range(NN_params[\"NUM_HIDDEN_LAYERS\"]):\n",
    "        if i==0:\n",
    "            num_neurons.append(trial.suggest_int(f\"num_neurons_l{i}\", len(RAW_FEATURES),5000))\n",
    "        else:\n",
    "            num_neurons.append(trial.suggest_int(f\"num_neurons_l{i}\", len(RAW_FEATURES), num_neurons[-1]))\n",
    "    NN_params[\"NUM_NEURONS\"] = num_neurons\n",
    "    \n",
    "    model = create_vanilla_nn(**NN_params)\n",
    "    BATCH_SIZE = 256\n",
    "    ES = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', min_delta=0, patience=20, restore_best_weights=True)\n",
    "\n",
    "    H = model.fit(train_X[NN_params[\"FEATURES\"]], train_y[RAW_FEATURES], batch_size= BATCH_SIZE, epochs=200, \n",
    "              validation_split=.05, callbacks = [ES], verbose = 0)\n",
    "    \n",
    "    \n",
    "        #Val Score\n",
    "    val_preds = model.predict(val_X[NN_params[\"FEATURES\"]], batch_size=1000)\n",
    "    score = 100 * np.mean(np.abs(val_y[RAW_FEATURES].values - val_preds))\n",
    "    \n",
    "    \n",
    "    #Stopping Memory Leaks\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    return score\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53046adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = RAW_FEATURES.copy() + HM_MOVING_MEDIANS + ONE_HOUR_MEAN + ['row_kurtosis', 'row_mean', 'row_median', 'row_var'] + ['day', 'hour', 'minute']\n",
    "params = {'FEATURES': FEATURES, \n",
    "          'RAW_FEATURES': RAW_FEATURES, \n",
    "          'DROPOUT_LEVEL': 0.12292871613987645, \n",
    "          'FIRST_DROPOUT': 0.4302064801787497, \n",
    "          'NUM_NEURONS': [4526], \n",
    "          'NUM_HIDDEN_LAYERS': 1,\n",
    "          'lr': 0.0030322842074454745, }\n",
    "\n",
    "#Run the model 10 times and average preds\n",
    "n_folds = 10\n",
    "val_cum = 0\n",
    "for i in range(n_folds):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = create_vanilla_nn(**params)\n",
    "    BATCH_SIZE = 256\n",
    "    ES = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', min_delta=0, patience=20, restore_best_weights=True)\n",
    "\n",
    "    H = model.fit(train_X[FEATURES], train_y[RAW_FEATURES], batch_size= BATCH_SIZE, epochs=200, \n",
    "              validation_data=(val_X[FEATURES], val_y[RAW_FEATURES]), callbacks = [ES], verbose=0)\n",
    "    val_preds = model.predict(val_X[FEATURES], batch_size=1000)\n",
    "    test_y[RAW_FEATURES] += model.predict(test_X[FEATURES]) / n_folds      \n",
    "\n",
    "    #Val Score\n",
    "    val_cum += val_preds / n_folds\n",
    "    val_score = 100 * np.mean(np.abs(val_y[RAW_FEATURES].values - val_preds))\n",
    "    print(f'FOLD {i+1} of {n_folds}: {val_score}')\n",
    "\n",
    "cumulative_score = 100 * np.mean(np.abs(val_y[RAW_FEATURES].values - val_cum))\n",
    "print(f'Cumulative validation: {cumulative_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Predictions back to regular shape from `wide`\n",
    "test_y = pd.melt(test_y, id_vars = 'time', value_vars=RAW_FEATURES, var_name = 'xydir',value_name = 'congestion')\n",
    "ss = pd.merge(ss[['row_id', 'time','xydir']], test_y, on = ['time','xydir'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02682c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#Post Processing\n",
    "#####################\n",
    "#Idea from https://www.kaggle.com/code/ambrosm/tpsmar22-generalizing-the-special-values\n",
    "train_melt = pd.melt(train_y, id_vars = 'time', value_vars=RAW_FEATURES, var_name = 'xydir',value_name = 'congestion')\n",
    "train_melt\n",
    "\n",
    "last_month = ss.time[0] - pd.Timedelta(days=60)\n",
    "lm = train_melt[train_melt.time >last_month].copy()\n",
    "lm['day_of_week'] = lm.time.dt.day % 7\n",
    "lm['hour'] = lm.time.dt.hour\n",
    "lm['minute'] = lm.time.dt.minute\n",
    "ss['day_of_week'] = ss.time.dt.day % 7\n",
    "ss['hour'] = ss.time.dt.hour\n",
    "ss['minute'] = ss.time.dt.minute\n",
    "\n",
    "#Getting quantiles\n",
    "lower = lm.groupby(['day_of_week','hour','minute', 'xydir']).congestion.quantile(0.15).reset_index()\n",
    "upper = lm.groupby(['day_of_week','hour','minute', 'xydir']).congestion.quantile(0.7).reset_index()\n",
    "\n",
    "#place quantiles in ss\n",
    "on = ['day_of_week','hour','minute', 'xydir']\n",
    "ss['lower'] = ss[on].merge(lower, on = on, how = 'left')['congestion']\n",
    "ss['upper'] = ss[on].merge(upper, on = on, how = 'left')['congestion']\n",
    "\n",
    "#Clip by quantiles\n",
    "ss['congestion'] = np.clip(ss['congestion'], ss['lower'], ss['upper'])\n",
    "\n",
    "#Submission\n",
    "ss['congestion'] = ss['congestion'] *100 #I scored 50+ when I forgot to do this.  Watch out!\n",
    "ss[['row_id','congestion']].to_csv('ss.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ac125",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
