{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5047d9",
   "metadata": {},
   "source": [
    "# The Best Model for TPSAPR22 Without Neural Networks\n",
    "\n",
    "This notebook shows how to solve TPSAPR22 with good feature engineering and a `HistGradientBoostingClassifier`. It furthermore shows how to cross-validate correctly without creating a data leak.\n",
    "\n",
    "Some features have been inspired by C4rl05/V's [XGBoost notebook](https://www.kaggle.com/code/cv13j0/tps-apr-2022-xgboost-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from cycler import cycler\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import scipy.stats\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pd.set_option(\"precision\", 3)\n",
    "plt.rcParams['axes.facecolor'] = '#0057b8' # blue\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n",
    "                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff73a5",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "\n",
    "We read the data and pivot the training data so that we have a dataframe with one row per sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "train = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\n",
    "train_labels = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\n",
    "test = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\n",
    "\n",
    "sensors = [col for col in train.columns if 'sensor_' in col]\n",
    "\n",
    "train_pivoted0 = train.pivot(index=['sequence', 'subject'], columns='step', values=sensors)\n",
    "display(train_pivoted0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d52aae",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Let's keep it simple and calculate only the following features:\n",
    "- For every sensor, we calculate mean, standard deviation, interquartile range, standard deviation divided by mean, and kurtosis. This gives the first 5\\*13=65 features.\n",
    "- For the special sensor_02, we count how many times it goes up or down.\n",
    "- For sensor_02, we calculate the sum of all upward / downward steps, the maximum of all upward / downward steps, and the mean of all upward / downward steps. \n",
    "- For every subject, we count how many sequences belong to it, and we add this count as a feature to all sequences of the subject (the [EDA](https://www.kaggle.com/code/ambrosm/tpsapr22-eda-which-makes-sense) gives the motivation for this feature). \n",
    "\n",
    "Now we have 74 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def engineer(df):\n",
    "    new_df = pd.DataFrame([], index=df.index)\n",
    "    for sensor in sensors:\n",
    "        new_df[sensor + '_mean'] = df[sensor].mean(axis=1)\n",
    "        new_df[sensor + '_std'] = df[sensor].std(axis=1)\n",
    "        new_df[sensor + '_iqr'] = scipy.stats.iqr(df[sensor], axis=1)\n",
    "        new_df[sensor + '_sm'] = np.nan_to_num(new_df[sensor + '_std'] / \n",
    "                                               new_df[sensor + '_mean'].abs()).clip(-1e30, 1e30)\n",
    "        new_df[sensor + '_kurtosis'] = scipy.stats.kurtosis(df[sensor], axis=1)\n",
    "    new_df['sensor_02_up'] = (df.sensor_02.diff(axis=1) > 0).sum(axis=1)\n",
    "    new_df['sensor_02_down'] = (df.sensor_02.diff(axis=1) < 0).sum(axis=1)\n",
    "    new_df['sensor_02_upsum'] = df.sensor_02.diff(axis=1).clip(0, None).sum(axis=1)\n",
    "    new_df['sensor_02_downsum'] = df.sensor_02.diff(axis=1) .clip(None, 0).sum(axis=1)\n",
    "    new_df['sensor_02_upmax'] = df.sensor_02.diff(axis=1).max(axis=1)\n",
    "    new_df['sensor_02_downmax'] = df.sensor_02.diff(axis=1).min(axis=1)\n",
    "    new_df['sensor_02_upmean'] = np.nan_to_num(new_df['sensor_02_upsum'] / new_df['sensor_02_up'], posinf=40)\n",
    "    new_df['sensor_02_downmean'] = np.nan_to_num(new_df['sensor_02_downsum'] / new_df['sensor_02_down'], neginf=-40)\n",
    "    return new_df\n",
    "\n",
    "train_pivoted = engineer(train_pivoted0)\n",
    "\n",
    "train_shuffled = train_pivoted.sample(frac=1.0, random_state=1)\n",
    "labels_shuffled = train_labels.reindex(train_shuffled.index.get_level_values('sequence'))\n",
    "labels_shuffled = labels_shuffled[['state']].merge(train[['sequence', 'subject']].groupby('sequence').min(),\n",
    "                                                   how='left', on='sequence')\n",
    "labels_shuffled = labels_shuffled.merge(labels_shuffled.groupby('subject').size().rename('sequence_count'),\n",
    "                                        how='left', on='subject')\n",
    "train_shuffled['sequence_count_of_subject'] = labels_shuffled['sequence_count'].values\n",
    "\n",
    "selected_columns = train_shuffled.columns\n",
    "print(len(selected_columns))\n",
    "#train_shuffled.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b5736",
   "metadata": {},
   "source": [
    "To get a first impression of the usefulness of the 74 features, we plot how the target depends on every feature, i.e., a diagram of $P(y=1|x)$.  To get a meaningful plot, we apply two transformations:\n",
    "- The x axis is not the value of the feature, but its index (when sorted by feature value).\n",
    "- The y axis is not the target value (which can be only 0 or 1), but a rolling mean over 1000 targets.\n",
    "\n",
    "The diagram shows bad features with an almost horizontal line (the probability of the positive target is 0.5 independently of the feature value) (e.g. sensor_05_std). Good features have a curve with high y_max - y_min (e.g. sensor_02_std). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afddf2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dependence between every feature and the target\n",
    "ncols = len(train_shuffled.columns) // 13\n",
    "plt.subplots(15, ncols, sharey=True, sharex=True, figsize=(15, 40))\n",
    "for i, col in enumerate(train_shuffled.columns):\n",
    "    temp = pd.DataFrame({col: train_shuffled[col].values,\n",
    "                         'state': labels_shuffled.state.values})\n",
    "    temp = temp.sort_values(col)\n",
    "    temp.reset_index(inplace=True)\n",
    "    plt.subplot(15, ncols, i+1)\n",
    "    plt.scatter(temp.index, temp.state.rolling(1000).mean(), s=2)\n",
    "    plt.xlabel(col)\n",
    "    plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e880e86",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "We don't need all 74 features. In a first step we drop 26 features which proved to be useless in a previous run of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some useless features\n",
    "dropped_features = ['sensor_05_kurtosis', 'sensor_08_mean',\n",
    "                    'sensor_05_std', 'sensor_06_kurtosis',\n",
    "                    'sensor_06_std', 'sensor_03_std',\n",
    "                    'sensor_02_kurtosis', 'sensor_03_kurtosis',\n",
    "                    'sensor_09_kurtosis', 'sensor_03_mean',\n",
    "                    'sensor_00_mean', 'sensor_02_iqr',\n",
    "                    'sensor_05_mean', 'sensor_06_mean',\n",
    "                    'sensor_07_std', 'sensor_10_iqr',\n",
    "                    'sensor_11_iqr', 'sensor_12_iqr',\n",
    "                    'sensor_09_mean', 'sensor_02_sm',\n",
    "                    'sensor_03_sm', 'sensor_05_iqr', \n",
    "                    'sensor_06_sm', 'sensor_09_iqr', \n",
    "                    'sensor_07_iqr', 'sensor_10_mean']\n",
    "selected_columns = [f for f in selected_columns if f not in dropped_features]\n",
    "len(selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff444c",
   "metadata": {},
   "source": [
    "Now we select features sequentially. We start with zero features and add one feature after the other. In every step we select the feature which increases the model's validation score the most. In this example, we select all features, and the output tells us which features are useful and which aren't.\n",
    "\n",
    "The same algorithm can be run backward by setting `backward` to `True`. It then starts with all features and repeatedly deletes the feature which adds the least value to the model's validation score.\n",
    "\n",
    "The model is a `HistGradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4609f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential feature selection\n",
    "# This code is a more verbose form of scikit-learn's SequentialFeatureSelector\n",
    "estimator = HistGradientBoostingClassifier(learning_rate=0.05, max_leaf_nodes=25,\n",
    "                                       max_iter=1000, min_samples_leaf=500,\n",
    "                                       l2_regularization=1,\n",
    "                                       max_bins=255,\n",
    "                                       random_state=4, verbose=0)\n",
    "\n",
    "X, y = train_shuffled[selected_columns], labels_shuffled.state\n",
    "n_iterations, backward = 48, False\n",
    "\n",
    "if n_iterations != 0:\n",
    "    n_features = X.shape[1]\n",
    "    current_mask = np.zeros(shape=n_features, dtype=bool)\n",
    "    history = []\n",
    "    for _ in range(n_iterations):\n",
    "        candidate_feature_indices = np.flatnonzero(~current_mask)\n",
    "        scores = {}\n",
    "        for feature_idx in candidate_feature_indices:\n",
    "            candidate_mask = current_mask.copy()\n",
    "            candidate_mask[feature_idx] = True\n",
    "            X_new = X.values[:, ~candidate_mask if backward else candidate_mask]\n",
    "            scores[feature_idx] = cross_val_score(\n",
    "                estimator,\n",
    "                X_new,\n",
    "                y,\n",
    "                cv=GroupKFold(n_splits=5),\n",
    "                groups=train_shuffled.index.get_level_values('subject'),\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=-1,\n",
    "            ).mean()\n",
    "            #print(f\"{str(X.columns[feature_idx]):30} {scores[feature_idx]:.3f}\")\n",
    "        new_feature_idx = max(scores, key=lambda feature_idx: scores[feature_idx])\n",
    "        current_mask[new_feature_idx] = True\n",
    "        history.append(scores[new_feature_idx])\n",
    "        new = 'Deleted' if backward else 'Added'\n",
    "        print(f'{new} feature: {str(X.columns[new_feature_idx]):30}'\n",
    "              f' {scores[new_feature_idx]:.3f}')\n",
    "\n",
    "    print()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(np.arange(len(history)) + (0 if backward else 1), history)\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('Features removed' if backward else 'Features added')\n",
    "    plt.title('Sequential Feature Selection')\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.show()\n",
    "\n",
    "    if backward:\n",
    "        current_mask = ~current_mask\n",
    "    selected_columns = np.array(selected_columns)[current_mask]\n",
    "    print(selected_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63070545",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "For cross-validation, we use a GroupKFold grouped on subjects. If we didn't group on subjects, we'd have a data leak (see the [EDA](https://www.kaggle.com/code/ambrosm/tpsapr22-eda-which-makes-sense) for an explanation).\n",
    "\n",
    "The model is a `HistGradientBoostingClassifier`; I got the same cv score using an `XGBClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cross-validation of the classifier\n",
    "\n",
    "print(f\"{len(selected_columns)} features\")\n",
    "score_list = []\n",
    "kf = GroupKFold(n_splits=5)\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train_shuffled, groups=train_shuffled.index.get_level_values('subject'))):\n",
    "    X_tr = train_shuffled.iloc[idx_tr][selected_columns]\n",
    "    X_va = train_shuffled.iloc[idx_va][selected_columns]\n",
    "    y_tr = labels_shuffled.iloc[idx_tr].state\n",
    "    y_va = labels_shuffled.iloc[idx_va].state\n",
    "\n",
    "    model = HistGradientBoostingClassifier(learning_rate=0.05, max_leaf_nodes=25,\n",
    "                                           max_iter=1000, min_samples_leaf=500,\n",
    "                                           l2_regularization=1,\n",
    "                                           validation_fraction=0.05,\n",
    "                                           max_bins=63,\n",
    "                                           random_state=3, verbose=0)\n",
    "#     model = XGBClassifier(n_estimators=500, n_jobs=-1,\n",
    "#                           eval_metric=['logloss'],\n",
    "#                           #max_depth=10,\n",
    "#                           colsample_bytree=0.8,\n",
    "#                           #gamma=1.4,\n",
    "#                           reg_alpha=6, reg_lambda=1.5,\n",
    "#                           tree_method='hist',\n",
    "#                           learning_rate=0.03,\n",
    "#                           verbosity=1,\n",
    "#                           use_label_encoder=False, random_state=3)\n",
    "\n",
    "    if True or type(model) != XGBClassifier:\n",
    "        model.fit(X_tr.values, y_tr)\n",
    "    else:\n",
    "        model.fit(X_tr.values, y_tr, eval_set = [(X_va.values, y_va)], \n",
    "                  eval_metric = ['auc'], early_stopping_rounds=30, verbose=10)\n",
    "    try:\n",
    "        y_va_pred = model.decision_function(X_va.values) # HistGradientBoostingClassifier\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            y_va_pred = model.predict_proba(X_va.values)[:,1] # XGBClassifier\n",
    "        except AttributeError:\n",
    "            y_va_pred = model.predict(X_va.values) # XGBRegressor\n",
    "    score = roc_auc_score(y_va, y_va_pred)\n",
    "    try:\n",
    "        print(f\"Fold {fold}: n_iter ={model.n_iter_:5d}    AUC = {score:.3f}\")\n",
    "    except AttributeError:\n",
    "        print(f\"Fold {fold}:                  AUC = {score:.3f}\")\n",
    "    score_list.append(score)\n",
    "    \n",
    "print(f\"OOF AUC:                       {np.mean(score_list):.3f}\") # 0.944\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0da2ae",
   "metadata": {},
   "source": [
    "# ROC curve\n",
    "\n",
    "We plot the ROC curve just because it looks nice. The area under the red curve is the score of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c366d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the roc curve for the last fold\n",
    "def plot_roc_curve(y_va, y_va_pred):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    fpr, tpr, _ = roc_curve(y_va, y_va_pred)\n",
    "    plt.plot(fpr, tpr, color='r', lw=2)\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=1, linestyle=\"--\")\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(y_va, y_va_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba333d",
   "metadata": {},
   "source": [
    "# Test predictions and submission\n",
    "\n",
    "We create a submission file as follows:\n",
    "1. We apply the same feature engineering to the test data as we did for the training data. Here it is important not to shuffle the test data so that the submission file is ordered correctly.\n",
    "2. We retrain the `HistGradientBoostingClassifier` 100 times with different seeds on 95 % of the training data.\n",
    "3. The decision functions of the 100 models can have different scales. To counter this, we convert the predictions to ranks using `scipy.stats.rankdata` and then submit the mean of the 100 ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for test\n",
    "test_pivoted0 = test.pivot(index=['sequence', 'subject'], columns='step', values=sensors)\n",
    "test_pivoted = engineer(test_pivoted0)\n",
    "sequence_count = test_pivoted.index.to_frame(index=False).groupby('subject').size().rename('sequence_count_of_subject')\n",
    "#display(test_pivoted.head(2))\n",
    "submission = pd.DataFrame({'sequence': test_pivoted.index.get_level_values('sequence')})\n",
    "test_pivoted = test_pivoted.merge(sequence_count, how='left', on='subject')\n",
    "test_pivoted.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain, predict and write submission\n",
    "print(f\"{len(selected_columns)} features\")\n",
    "\n",
    "pred_list = []\n",
    "for seed in range(100):\n",
    "    X_tr = train_shuffled[selected_columns]\n",
    "    y_tr = labels_shuffled.state\n",
    "\n",
    "    model = HistGradientBoostingClassifier(learning_rate=0.05, max_leaf_nodes=25,\n",
    "                                           max_iter=1000, min_samples_leaf=500,\n",
    "                                           validation_fraction=0.05,\n",
    "                                           l2_regularization=1,\n",
    "                                           max_bins=63,\n",
    "                                           random_state=seed, verbose=0)\n",
    "    model.fit(X_tr.values, y_tr)\n",
    "    pred_list.append(scipy.stats.rankdata(model.decision_function(test_pivoted[selected_columns].values)))\n",
    "    print(f\"{seed:2}\", pred_list[-1])\n",
    "print()\n",
    "submission['state'] = sum(pred_list) / len(pred_list)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
