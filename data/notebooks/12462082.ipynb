{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ecfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "device = 'cpu'\n",
    "hidden_dim=64\n",
    "im_chan=3\n",
    "batch_size=128\n",
    "max_epochs=100000\n",
    "display_step=5\n",
    "n_epochs_stop=5\n",
    "image_size=64\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "lr=0.01 #learning rate\n",
    "df = pd.read_csv('/kaggle/input/celeba-dataset/list_attr_celeba.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "n_rows = df.shape[0]\n",
    "n_classes=df.shape[1]-1\n",
    "#dividir em treinamento validacao e teste\n",
    "train = df.iloc[:n_rows//2,:]\n",
    "val = df.iloc[n_rows//2:3*n_rows//4,:]\n",
    "test = df.iloc[3*n_rows//4:,:]\n",
    "train_ids = [train.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),0] for i in range(int(math.ceil(train.shape[0]/batch_size)))]\n",
    "batches = [torch.Tensor(train.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),1:].clip(0,1).values).long().to(device) for i in range(int(math.ceil(train.shape[0]/batch_size)))]\n",
    "#Estou calculando a validacao por batch para economizar memoria\n",
    "#como o conjunto de validacao tem metade do tamanho do de treinamento o tamanho do batch ou o numero de batches\n",
    "#tem que ser dividido por 2 escolhi reduzir o tamanho do batch em vez da quantidade porque assim eu economizo tempo\n",
    "#(uso o mesmo for)\n",
    "val_ids = [val.iloc[i*batch_size//2:min(i*batch_size//2+batch_size//2,val.shape[0]),0] for i in range(int(math.ceil(train.shape[0]/batch_size)))]\n",
    "val_batches = [torch.Tensor(val.iloc[i*batch_size//2:min(i*batch_size//2+batch_size//2,val.shape[0]),1:].clip(0,1).values).long().to(device) for i in range(int(math.ceil(train.shape[0]/batch_size)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bbb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "device = 'cuda'\n",
    "hidden_dim=64\n",
    "im_chan=3\n",
    "batch_size=128\n",
    "max_epochs=100000\n",
    "display_step=5\n",
    "n_epochs_stop=5\n",
    "image_size=64\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "lr=0.01 #learning rate\n",
    "df = pd.read_csv('/kaggle/input/celeba-dataset/list_attr_celeba.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "n_rows = df.shape[0]\n",
    "n_classes=df.shape[1]-1\n",
    "#dividir em treinamento validacao e teste\n",
    "train = df.iloc[:n_rows//2,:]\n",
    "val = df.iloc[n_rows//2:3*n_rows//4,:]\n",
    "test = df.iloc[3*n_rows//4:,:]\n",
    "train_ids = [train.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),0] for i in range(int(math.ceil(train.shape[0]/batch_size)))]\n",
    "batches = [torch.Tensor(train.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),1:].values).long().to(device) for i in range(int(math.ceil(train.shape[0]/batch_size)))]\n",
    "#Estou calculando a validacao por batch para economizar memoria\n",
    "#como o conjunto de validacao tem metade do tamanho do de treinamento o tamanho do batch ou o numero de batches\n",
    "#tem que ser dividido por 2 escolhi reduzir o tamanho do batch em vez da quantidade porque assim eu economizo tempo\n",
    "#(uso o mesmo for)\n",
    "val_ids = [val.iloc[i*batch_size//2:min(i*batch_size//2+batch_size//2,val.shape[0]),0] for i in range(int(math.ceil(train.shape[0]/batch_size)))]\n",
    "val_batches = [torch.Tensor(val.iloc[i*batch_size//2:min(i*batch_size//2+batch_size//2,val.shape[0]),1:].values).long().to(device) for i in range(int(math.ceil(train.shape[0]/batch_size)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b669c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definicao do Classificador\n",
    "class Classif(nn.Module):\n",
    "    def __init__(self, im_chan=3, hidden_dim=64,n_classes=40):\n",
    "        super(Classif, self).__init__()\n",
    "        self.classif = nn.Sequential(\n",
    "            self.make_classif_block(im_chan, hidden_dim),\n",
    "            self.make_classif_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_classif_block(hidden_dim * 2, hidden_dim * 4, stride=3),\n",
    "            self.make_classif_block(hidden_dim * 4, n_classes, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_classif_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a block of the classifier\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the classifier\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_chan)\n",
    "        '''\n",
    "        classif_pred = self.classif(image)\n",
    "        return classif_pred.view(len(classif_pred), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definicao do Classificador\n",
    "class Classif(nn.Module):\n",
    "    def __init__(self, im_chan=3, hidden_dim=64,n_classes=40):\n",
    "        super(Classif, self).__init__()\n",
    "        self.classif = nn.Sequential(\n",
    "            self.make_classif_block(im_chan, hidden_dim),\n",
    "            self.make_classif_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_classif_block(hidden_dim * 2, hidden_dim * 4, stride=3),\n",
    "            self.make_classif_block(hidden_dim * 4, n_classes, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_classif_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a block of the classifier\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the classifier\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_chan)\n",
    "        '''\n",
    "        classif_pred = self.classif(image)\n",
    "        return classif_pred.view(len(classif_pred), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eacdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d48f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_loss = nn.MultiLabelMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inicializar e treinar\n",
    "classif = Classif(im_chan,hidden_dim,n_classes).to(device)\n",
    "classif_opt = torch.optim.Adam(classif.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "classif = classif.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ade5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inicializar e treinar\n",
    "classif = Classif(im_chan,hidden_dim,n_classes).to(device)\n",
    "classif_opt = torch.optim.Adam(classif.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "classif = classif.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c49fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treinamento\n",
    "min_val_loss=float('inf')\n",
    "cur_step = 0\n",
    "epochs_no_improve=0\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(max_epochs):\n",
    "    mean_train_loss=0\n",
    "    mean_val_loss=0\n",
    "    for batch_index in tqdm(range(len(batches))):\n",
    "        batch = batches[batch_index]\n",
    "        val_batch = val_batches[batch_index]\n",
    "        train_image_list = [transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)) for im_id in train_ids[batch_index]]\n",
    "        images = torch.stack(train_image_list).to(device)\n",
    "        classif_opt.zero_grad()\n",
    "        training_pred = classif(images).float()\n",
    "        training_loss = classif_loss(training_pred,batch.float())\n",
    "        training_loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        classif_opt.step()\n",
    "        \n",
    "        #Estou calculando a validacao por batch para economizar memoria\n",
    "        val_image_list = [transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)) for im_id in val_ids[batch_index]]\n",
    "        val_images = torch.stack(val_image_list).to(device)\n",
    "        val_pred = classif(val_images).float()\n",
    "        val_loss = classif_loss(val_pred,val_batch)\n",
    "        \n",
    "        # Keep track of the average losses\n",
    "        mean_train_loss += training_loss.item()/len(batches)\n",
    "        mean_val_loss += val_loss.item()/len(batches)\n",
    "    training_losses += [mean_train_loss]\n",
    "    validation_losses += [mean_val_loss]\n",
    "    ### Visualization code ###\n",
    "    if cur_step % display_step == 0 and cur_step > 0:\n",
    "        training_mean = sum(training_losses[-display_step:]) / display_step\n",
    "        #step_bins = 20\n",
    "        num_examples = (len(training_losses) )#// step_bins) * step_bins\n",
    "        plt.plot(\n",
    "            range(num_examples),# // step_bins), \n",
    "            torch.Tensor(training_losses[:num_examples]),\n",
    "            label=\"Training Loss\"\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(num_examples),#// step_bins), \n",
    "            torch.Tensor(validation_losses[:num_examples]),\n",
    "            label=\"Validation Loss\"\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if(mean_val_loss < min_val_loss):\n",
    "        epochs_no_improve=0\n",
    "        min_val_loss = mean_val_loss\n",
    "    else:\n",
    "        epochs_no_improve+=1\n",
    "        if(epochs_no_improve>n_epochs_stop):\n",
    "            print('Early stopping!' )\n",
    "            break;\n",
    "    cur_step += 1\n",
    "classif.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b33fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treinamento\n",
    "min_val_loss=float('inf')\n",
    "cur_step = 0\n",
    "epochs_no_improve=0\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(max_epochs):\n",
    "    mean_train_loss=0\n",
    "    mean_val_loss=0\n",
    "    for batch_index in tqdm(range(len(batches))):\n",
    "        batch = batches[batch_index]\n",
    "        val_batch = val_batches[batch_index]\n",
    "        train_image_list = [transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)) for im_id in train_ids[batch_index]]\n",
    "        images = torch.stack(train_image_list).to(device)\n",
    "        classif_opt.zero_grad()\n",
    "        training_pred = classif(images).float()\n",
    "        training_loss = classif_loss(training_pred,batch)\n",
    "        training_loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        classif_opt.step()\n",
    "        \n",
    "        #Estou calculando a validacao por batch para economizar memoria\n",
    "        val_image_list = [transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)) for im_id in val_ids[batch_index]]\n",
    "        val_images = torch.stack(val_image_list).to(device)\n",
    "        val_pred = classif(val_images).float()\n",
    "        val_loss = classif_loss(val_pred,val_batch)\n",
    "        # Keep track of the average losses\n",
    "        mean_train_loss += training_loss.item()/len(batches)\n",
    "        mean_val_loss += val_loss.item()/len(batches)\n",
    "    training_losses += [mean_train_loss]\n",
    "    validation_losses += [mean_val_loss]\n",
    "    ### Visualization code ###\n",
    "    if cur_step % display_step == 0 and cur_step > 0:\n",
    "        training_mean = sum(training_losses[-display_step:]) / display_step\n",
    "        step_bins = 20\n",
    "        num_examples = (len(training_losses) // step_bins) * step_bins\n",
    "        plt.plot(\n",
    "            range(num_examples // step_bins), \n",
    "            torch.Tensor(training_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "            label=\"Training Loss\"\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(num_examples // step_bins), \n",
    "            torch.Tensor(validation_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "            label=\"Validation Loss\"\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if(mean_val_loss.item() < min_val_loss):\n",
    "        print('got here!')\n",
    "        epochs_no_improve=0\n",
    "        min_val_loss = mean_val_loss.item()\n",
    "    else:\n",
    "        print('validation didn\\'t improve')\n",
    "        print(mean_train_loss.item(),mean_val_loss.item())\n",
    "        epochs_no_improve+=1\n",
    "        if(epochs_no_improve>n_epochs_stop):\n",
    "            print('Early stopping!' )\n",
    "            break;\n",
    "    cur_step += 1\n",
    "classif.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6864b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_loss = 0\n",
    "for i in range(int(math.ceil(test.shape[0]/batch_size))):\n",
    "    test_images = torch.stack([transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)).to(device) for im_id in test.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),0]])\n",
    "    test_pred = classif(test_images).float()\n",
    "    test_loss = classif_loss(test_pred,torch.Tensor(test.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),1:].values).long().to(device))\n",
    "    mean_test_loss += test_loss.item()/int(math.ceil(test.shape[0]/batch_size))\n",
    "print(mean_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_images = torch.stack([transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)) for im_id in test.iloc[:,0]])\n",
    "#test_pred = classif(test_images).float()\n",
    "#test_loss = classif_loss(test_pred,torch.Tensor(test.iloc[:,1:].values).long())\n",
    "#print(test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classif,'classif.pt')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
