{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778c79ca",
   "metadata": {},
   "source": [
    "# Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6499cb",
   "metadata": {},
   "source": [
    "[1. Introduction](#1)\n",
    "\n",
    "[2. Tensor](#2)\n",
    "\n",
    "[3. RGB Format](#3)\n",
    "\n",
    "[4. Convolutional Neural Network (CNN)](#4)\n",
    "\n",
    "* [4.1. Abstract Structure of CNN](#4.1)\n",
    "\n",
    "* [4.2 The LeNet-5 Architecture](#4.2)\n",
    "\n",
    "* [4.3 Convolution](#4.3)\n",
    "\n",
    "* [4.4 Non - Linear Activation Function](#4.4)\n",
    "\n",
    "* [4.5 Pooling](#4.5)\n",
    "\n",
    "* [4.6 Fully Connected layer](#4.6)\n",
    "\n",
    "[5. Importing Libraries and Data File](#5)\n",
    "\n",
    "[6. MNIST Data Analysis](#6)\n",
    "\n",
    "[7. Model Creation](#7)\n",
    "\n",
    "[8. Model Plot](#8)\n",
    "\n",
    "[9. Model Summary Interpretation](#9)\n",
    "\n",
    "[10. Evaluation of Validation Model](#10)\n",
    "\n",
    "[11. Image output in Different Layers](#11)\n",
    "\n",
    "[12. Augmentation](#12)\n",
    "\n",
    "[13. Reference](#13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4855b1b",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f881f",
   "metadata": {},
   "source": [
    "MNIST (\"Modified National Institute of Standards and Technology\") classic dataset of handwritten images has served as the basis for benchmarking classification algorithms.\n",
    "\n",
    "In this notebook CNN is used to identify digits from a dataset of tens of thousands of handwritten images. The efficency of CNN is increased by using augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6ca0b",
   "metadata": {},
   "source": [
    "# 2. Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9aab81",
   "metadata": {},
   "source": [
    "A tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space and  CNN takes a order 3 tensor as an input.\n",
    "\n",
    "* A scalar value ---> order 0 tensor - eg. A single number\n",
    "* A Vector ---> order 1 tensor - eg. A array\n",
    "* A matrix ---> order 2 tensor -eg.  Gray scale Image\n",
    "* A color Image --->order 3 tensor - eg. Color Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907c70c",
   "metadata": {},
   "source": [
    "# 3. RGB Format "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1bf037",
   "metadata": {},
   "source": [
    "An RGB image has three channels: \n",
    "* Red\n",
    "* Green\n",
    "* Blue\n",
    "\n",
    "If the RGB image is 24-bit (the industry standard as of 2005), each channel has 8 bits(total 8 * 3 =24), for red, green, and blue, where each image can store discrete pixels with conventional brightness intensities between 0 and 255.\n",
    "\n",
    "In each channel, the corresponding color of that channel will be brighter, that red will be brighter in red channel, green will brighter in green channel and blue will be brighter in blue channel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345650e",
   "metadata": {},
   "source": [
    "![image.png](attachment:389c810c-31da-4a90-89af-6e6a1145c528.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a758e9f",
   "metadata": {},
   "source": [
    "In the above picture, the grey trees have similar brightness in all channels, whereas the red dress is much brighter in the red channel than in the other two, and the green grass is much brighter in the green channel.\n",
    "\n",
    "A color triangle can be made with the three primary colors based on the additive combination like below where white will the center color. The color outside the color triangle will be grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f0939",
   "metadata": {},
   "source": [
    "![image.png](attachment:640ab796-e868-4f24-ab8b-f65deab9c889.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ddce6f",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "# 4. Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9eb3f",
   "metadata": {},
   "source": [
    "The Convolutional Neural Network (CNN) are used to recognize visual patterns from pixel images with variability.\n",
    "\n",
    "Some areas where CNN are used: \n",
    "* Image Classification\n",
    "* Image Sematic Segmentation\n",
    "* Objection Detection in Image\n",
    "\n",
    "\n",
    "A color Image with H rows and W columns is a tensor with H  *   W   *   3 [3- represents 3 channels] and gray scale can be represend in 3D as H  *  W  *  1. Each color image will have a brightness and its value will be ranging from 0 to 255. Color image with value 255 will be brightest and with 0 wil be dark.\n",
    "\n",
    "However, tensor greater than 3 order are also widely used in CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4cbbd3",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a> <br>\n",
    "##  4.1 Abstract Structure of CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52e530",
   "metadata": {},
   "source": [
    "![image.png](attachment:b4b64c6c-05a1-4e32-ab76-966d1820a339.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211cd83c",
   "metadata": {},
   "source": [
    "The input x1 an image goes through processing in the first layer, which is the first box. The parameters involved in the first box and collectively called w1 tensor and it will give output x2 which will be input to second layer processing, second box. It will continue till it gets the output xl. There will be one additional layer for back error propagation. If the CNN is used to handle an image classification problem with C classes, then xl will have C dimensional vector and a softmax transformation can be used in (L-1)th layer to make its output as a probability mass function.\n",
    "\n",
    "The last layer, wL is the loss layer. If t is the target for input x1, then t is converted to C dimensional vector and it will be a probability mass function, and distance between xL and t can be measured using cross-entropy loss.\n",
    "\n",
    "There the loss function will be\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d26657f",
   "metadata": {},
   "source": [
    "![image.png](attachment:b6bb745d-586d-4791-8ea6-99a63c89c2b4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f50946",
   "metadata": {},
   "source": [
    "In CNN, the prediction is done by using only forward propagation and loss layer is not needed for prediction, it is useful when we learn CNN parameters using a set of training examples. The output prediction will be.\n",
    "\n",
    "![image.png](attachment:549589ce-dd81-4a55-8e99-ca29bc63e5b4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253af134",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a> <br>\n",
    "## 4.2 The LeNet-5 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f7d77",
   "metadata": {},
   "source": [
    "The LeNet-5 Architecture designed by LeCun et al, for handwritten and machine-printed character recognition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0592840e",
   "metadata": {},
   "source": [
    "![image.png](attachment:bd356768-06ca-4332-a316-217d332d744f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d626622",
   "metadata": {},
   "source": [
    "* The input image is an 8-bit image of size 32 * 32\n",
    "* 2 convolution/pooling layers \n",
    "    * C1/S2 - C1 has 6 Filters of size 5 by 5\n",
    "    * C3/S4 - C3 has 16 Filters of size 5 by 5\n",
    "* Fully connected layers denoted by C5 and F6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af758109",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a> <br>\n",
    "## 4.3 Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6ca2f",
   "metadata": {},
   "source": [
    "The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels or feature detecors), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c214e2",
   "metadata": {},
   "source": [
    "![image.png](attachment:8e2083f8-4ff0-4e69-a963-337db5a6ab62.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9848d4f",
   "metadata": {},
   "source": [
    "The convolution input is the image broken down into pixel.\n",
    "\n",
    "Let us take a input image which is broken down to 3 x 4 matrix as above and if a 2 x 2  filter(kernel or feature detector.) with all values 1. \n",
    "\n",
    "Kernel detects features like edges or convex shapes and best weights for the kernel is learnt by network using back-propagation like any other neurons.. The Kernel will over lap the convolution input and will produce an output of matrix 2 * 3, which is called a feature map. This type of multiplication is called dot product. \n",
    "\n",
    "When kernel overlap convolution of top left, then the convolution result will be 1 x 1 + 1 x  4+ 1 x 2 + 1 x 5 =12 an it will continue till it reaches right bottom border.\n",
    "\n",
    "The convolution will reduce the size of input image and in case if input image and feature map is required to be of same size, padding is to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc761f9",
   "metadata": {},
   "source": [
    "<a id=\"4.4\"></a> <br>\n",
    "## 4.4 Non - Linear Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95bc89",
   "metadata": {},
   "source": [
    "CNN uses non-linear activation function, if the nonlinear operation between every two convolutional layers is removed, the cascade of two linear systems is equivalent to a single linear system. Then, we can simply go with one linear system and the necessity of a multi-layer network architecture is not obvious. \n",
    "\n",
    "The 3 activation function adopted by CNN are :\n",
    "* Sigmoid : Clips input to interval -1 and 1.\n",
    "* ReLU    : Clips negative value to zero and keeps positive unchanged.\n",
    "* PReLU   : Similar operation as ReLU, apart from mapping larger negative values to smaller one by reducing slope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fd9cc",
   "metadata": {},
   "source": [
    "![image.png](attachment:af214a7e-224b-453a-b0dd-a64c3d5946ba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571306b0",
   "metadata": {},
   "source": [
    "However, ReLU is most commonly used in CNN. Sigmoid function does not perform well as ReLU in CNN, because after several sigmoid layers, the gradient will vanish which will lead to difficulty in gradient based learning. Whereas ReLU, significantly reduce diffculty in learning CNN parameters and improves accuracy by backpropagating gradient of activated features without any change which is beneficial for gradient based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93358a71",
   "metadata": {},
   "source": [
    "<a id=\"4.5\"></a> <br>\n",
    "## 4.5 Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a73b84",
   "metadata": {},
   "source": [
    "The purpose of the pooling layers is to achieve spatial invariance by reducing the resolution of the feature maps. Each pooled feature map corresponds to one feature map of the previous layer. Also pooling removes noise in the data, helps to reduce overfitting and speed up the computation.\n",
    "\n",
    "Max pooling and subsampling are the commonly used pooling operations. The subsampling function takes the average over the inputs, multiplies it with a trainable scalar, adds a trainable bias, and passes the result through the non-linearity. Whereas the max pooling function applies a window function to the input patch, and computes the maximum in the neighborhood. \n",
    "\n",
    "![image.png](attachment:4783e495-23b0-4eb6-a7d6-6eca32af4572.png)\n",
    "\n",
    "\n",
    "Max pooling operation is vastly superior for capturing invariances in image-like data, compared to a subsampling operation, so it is commonly used in CNN as pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49183485",
   "metadata": {},
   "source": [
    "<a id=\"4.6\"></a> <br>\n",
    "## 4.6 Fully Connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadb7fa",
   "metadata": {},
   "source": [
    "In fully Connected layers, every neurons have connections to all activations in the previous layer. Before the data is fed to fully connected layer, flattening of data is to be done. Flattening will convert the pooled data into 1D linear array.\n",
    "\n",
    "Fully connected layer can be considered as the hidden layer in ANN, but here neuron will be fully connected to previous layer. Fully connected layer will be followed by a output layer which uses softmax activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e32cf",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "# 5. Importing Libraries and Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac51a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten,MaxPool2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint , EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import visualkeras \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060aa581",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n",
    "x_test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea4a87",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "# 6. MNIST Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5cdc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head().style.background_gradient(cmap='Dark2_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899380ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.describe().style.background_gradient(cmap='Dark2_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff3d0c",
   "metadata": {},
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:110%;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px\">\n",
    "\n",
    "<p style=\"padding: 10px;\n",
    "              color:white;\n",
    "          text-align:center;\">Missing Data Check</p>    \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc117ba",
   "metadata": {},
   "source": [
    "**There is no missing Data in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4bc67",
   "metadata": {},
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:110%;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px\">\n",
    "\n",
    "<p style=\"padding: 10px;\n",
    "              color:white;\n",
    "          text-align:center;\">Analysis and Visualization Of Target</p>         \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = x_train['label']\n",
    "x_train = x_train.drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64190c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts().rename_axis('unique_values').to_frame('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe16d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(set(y_train))\n",
    "print(\"number of classes:\", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.countplot(x = y_train, palette = 'Dark2')\n",
    "t= len(y_train)\n",
    "for p in ax.patches:\n",
    "    percentage = f'{100 * p.get_height() / t:.1f}%\\n'\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y), ha='center', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a89d10",
   "metadata": {},
   "source": [
    "**There are 10 classes in the output from 0 top 9.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970fbc9",
   "metadata": {},
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:110%;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px\">\n",
    "\n",
    "<p style=\"padding: 10px;\n",
    "              color:white;\n",
    "          text-align:center;\">Reshaping and Normalizing</p>         \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b759c",
   "metadata": {},
   "source": [
    "To input images to keras, the pixel format should be (H x W x D )[ (image_height, image_width, color_channels)] so we will reshape the data into ( 28 x 28 x 1) The maximum brightness of the pixel is 255 and to normalize we have to divide by 255, then all values of training and test set will lie between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47df310",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training data\", x_train.shape)\n",
    "print(\"\\nShape of test data\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values.reshape(-1,28,28,1)\n",
    "x_test = x_test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bcec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "x, y = 15, 5\n",
    "for i in range(75):  \n",
    "    plt.subplot(y, x, i+1)\n",
    "    plt.imshow(x_train[i],interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71594d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc30e72",
   "metadata": {},
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:110%;\n",
    "           font-family:Verdana;\n",
    "           letter-spacing:0.5px\">\n",
    "\n",
    "<p style=\"padding: 10px;\n",
    "              color:white;\n",
    "          text-align:center;\">One Hot Encoding</p>         \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6096f5",
   "metadata": {},
   "source": [
    "The ouput class is from 0 to 9, we have one hot encode it,else NN will consider it as normal classification problem. Either we can use to_categorical() from keras's utils.np_utils or get_dummies() from pandas(when using get_dummies() y_train is to be converted to dataframe).\n",
    "\n",
    "to_categorical() will convert class vector into a matrix from integers from 0 to number of classes. If we does not pass number of classes as a parameter, it will convert the matrix from 0 to largest number in y + 1. Here I have taken the number of classes in a variable K, so it can passed as num_class parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_categ_train = to_categorical(y_train,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05bcd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa044ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Encoded y_train first row\",y_categ_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9588ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train1, x_val, y_train1, y_val = train_test_split(x_train, y_categ_train, test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607eecfc",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "# 7. Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e44a8",
   "metadata": {},
   "source": [
    "* Sequential() ----> A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "* Conv2D() ----> Conv2D layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\n",
    "    * input_shape : Here only a single Conv2D layer is used, so we have to provid input shape in parameter input_shape without the sample axis here it will be (28, 28,1). In case the dimension has variable size then None is to provided.\n",
    "    * filters : It is an integer value and the dimensionality of the output space.\n",
    "    * kernel_size : It specifies the height and width of the 2D convolution window.\n",
    "    * activation : ReLU, as it is most efficient activation function.\n",
    "* MaxPool2D ---> Downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window (of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension.\n",
    "    * pool_size ---> In this parameter window size is to be mentioned over which to take the maximum. (2, 2) will take the max value over a 2x2 pooling window.\n",
    "* Flatten() --- > It will flattens the input to  that is equal to the number of elements contained in tensor without affecting the batch size.\n",
    "* Dense() ----> For fully connected layer, 128 neurons are using here and for output layer 10 neurons as the number of classes in output is 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c23aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(28, 28, 1), activation='relu',)) \n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c574c",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "# 8. Model Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3568fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c295c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualkeras.layered_view(model, scale_xy=10, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fdfdad",
   "metadata": {},
   "source": [
    "> <a id=\"9\"></a> <br>\n",
    "# 9. Model Summary Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ff820",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ea99b",
   "metadata": {},
   "source": [
    "Output Shape\n",
    "\n",
    "* Input Image : The size of each input image is (28,28,1), but as batch size varies, Keras appends None and the size of input image will be (None,28,28,1).\n",
    "* Conv2D Layer : Conv2D operates on a 2D window on the height, width dimensions. So here the convolution will be done for a (28, 28) image with kernel_size (4, 4) and stride =1, so the output size will be (28 - 4 + 1, 28 - 4 + 1) = (25, 25). By 32 filters and with varing batch size the output shape of conv2D layer will be (None, 25,25,32).\n",
    "* MaxPool2D Layer : Here the pool_size is (2, 2) and stride is not mentioned. When stride is not mentioned it will take the value of pool_size, so hee stride will be (2,2). So the output shape will be (None, (25 - 2)//2 + 1, (25 - 2)//2) + 1),32) = (None, 12, 12, 32)\n",
    "* Flatten : It will create a 1D array vector with shape (None, 12 x 12 x 32)) = (None, 4608)\n",
    "* Dense : Dense output is specified in the model.\n",
    "\n",
    "Param \n",
    "\n",
    "*  Conv2D layer =  (4 x 4 x 1 x 32) + 32  = 544\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75d0db",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a> <br>\n",
    "# 10. Evaluation of Validation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebdebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train1,y_train1,validation_data = (x_val,y_val),epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_val, y_val, verbose = 10 )\n",
    "print ( 'Loss is :', score[0], '\\nTraining Accuracy is :', score[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y=model.predict(x_val)\n",
    "y_val_hat=np.argmax(predict_y,axis=1)\n",
    "y_val_org = np.argmax(y_val, axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a4f81",
   "metadata": {},
   "source": [
    "## Filters used inModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f399d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "filters, biases = model.layers[0].get_weights()\n",
    "layer_outputs = model.layers[0].output\n",
    "model_Input = model.input\n",
    "print('Layer Name is :', layers[0].name, '\\nFitler Shaper:',filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (10,12))\n",
    "columns = 4\n",
    "rows = 8\n",
    "n_filters = columns * rows\n",
    "for i in range(1, n_filters + 1):\n",
    "    fil = filters[:,:,:,i-1]\n",
    "    f = plt.subplot(rows, columns, i)\n",
    "    f.set_xticks([])\n",
    "    f.set_yticks([])\n",
    "    plt.imshow(fil[: ,: ,0], cmap = 'gray')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd038ac",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f9925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Dark2):\n",
    "  \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "          plt.text(j, i, cm[i, j],\n",
    "                  horizontalalignment=\"center\",\n",
    "                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = confusion_matrix(y_val_org, y_val_hat) \n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07da2c",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val_org,y_val_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc280b2",
   "metadata": {},
   "source": [
    "**Classification Report looks good :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378342f6",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a> <br>\n",
    "# 11. Image Output In Different Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im = x_train[0]\n",
    "plt.imshow(test_im.reshape(28,28), cmap='viridis', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc89aec",
   "metadata": {},
   "source": [
    "## First Activation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4091bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[:4]]\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(test_im.reshape(1,28,28,1))\n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4294ca5",
   "metadata": {},
   "source": [
    "## Conv2D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa89b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = []\n",
    "for layer in model.layers[:-1]:\n",
    "    layer_names.append(layer.name) \n",
    "images_per_row = 16\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    if layer_name.startswith('conv'):\n",
    "        n_features = layer_activation.shape[-1]\n",
    "        size = layer_activation.shape[1]\n",
    "        n_cols = n_features // images_per_row\n",
    "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "        for col in range(n_cols):\n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0,:, :, col * images_per_row + row]\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size : (col + 1) * size,\n",
    "                             row * size : (row + 1) * size] = channel_image\n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                            scale * display_grid.shape[0]))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00f33f",
   "metadata": {},
   "source": [
    "## MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = []\n",
    "for layer in model.layers[:-1]:\n",
    "    layer_names.append(layer.name) \n",
    "images_per_row = 16\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    if layer_name.startswith('max'):\n",
    "        n_features = layer_activation.shape[-1]\n",
    "        size = layer_activation.shape[1]\n",
    "        n_cols = n_features // images_per_row\n",
    "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "        for col in range(n_cols):\n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0,:, :, col * images_per_row + row]\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size : (col + 1) * size,\n",
    "                             row * size : (row + 1) * size] = channel_image\n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                            scale * display_grid.shape[0]))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5eb286",
   "metadata": {},
   "source": [
    "<a id=\"12\"></a> <br>\n",
    "# 12. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68caa9d6",
   "metadata": {},
   "source": [
    "Augmentation is a technique to increase the diversity of training set by applying random (but realistic) transformations, such as image rotation, zoom in zoom out, horizontal or vertical flip etc to overcome limited quantity and limited diversity of images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen  =  ImageDataGenerator(   \n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.0,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb1b5a",
   "metadata": {},
   "source": [
    "Here augmention techinque used are :\n",
    "* rotation_range = 15 --> The image will be rotationed randomly with 15 degree.\n",
    "* width_shift_range=0.15 --> The possible values are floats in the interval [-1.5, +1.5)] . Here the image will be shiftede to the left or right(horizontal shifts) randomly between the range. Positive value will shift image towards right side and vice versa.\n",
    "* shear_range=0.2 --> Here the image will be distorted along x - axis with counter-clockwise direction in degrees. \n",
    "* zoom_range=0.2 --> It will zoom the image and random zoom will be in range of Range  [lower, upper] = [1-zoom_range, 1+zoom_range].\n",
    "* validation_split=0.0 --> Fraction of images reserved for validation and its value should be strictly between 0 and 1. \n",
    "* horizontal_flip=True --> Its value is boolean and when true will randomly flip inputs horizontally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(filepath='CNN-logo.h5',\n",
    "                      monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "earlystopping_callback = EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6744f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "steps = x_train.shape[0]//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = datagen.flow(x_train1, y_train1, batch_size)\n",
    "Val_generator = datagen.flow(x_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit_generator(train_generator, validation_data = Val_generator,epochs=1, steps_per_epoch = steps,callbacks=[checkpoint_callback, earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(x_test)\n",
    "res = np.argmax(res,axis = 1)\n",
    "res = pd.Series(res, name=\"Label\")\n",
    "submission = pd.concat([pd.Series(range(1 ,28001) ,name = \"ImageId\"),   res],axis = 1)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f573cb",
   "metadata": {},
   "source": [
    "<a id=\"13\"></a> <br> \n",
    "# 13. Reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ebf8c1",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Channel_(digital_image)\n",
    "\n",
    "https://en.wikipedia.org/wiki/SRGB\n",
    "\n",
    "https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
    "\n",
    "https://cs.nju.edu.cn/wujx/paper/CNN.pdf\n",
    "\n",
    "https://www.semanticscholar.org/paper/Understanding-convolutional-neural-networks-with-a-Kuo/52d7ae292f285ab24b050b8d229ac98cd674523c#citing-papers\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
