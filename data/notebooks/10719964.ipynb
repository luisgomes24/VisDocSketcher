{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ddd594",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5bd668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import contractions\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download(\"all\")\n",
    "from nltk.corpus import stopwords as sw\n",
    "from wordcloud import WordCloud, STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e2bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_details = pd.read_csv('../input/ctdsmedium-blogs/sb_blog.csv', encoding='ISO-8859-1', parse_dates=['release_date'])\n",
    "blog_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Category vs Claps: \", blog_details.groupby('category')['claps'].sum())\n",
    "print(\"\\nRatio of Women-to-Men heroes is \",blog_details.groupby('heroes_gender')['heroes'].count()[0]/(blog_details.groupby('heroes_gender')['heroes'].count()[0] + blog_details.groupby('heroes_gender')['heroes'].count()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea98a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by claps in descending order \n",
    "sorted_blog_details = blog_details.sort_values('claps', ascending=False)\n",
    "plt.bar(sorted_blog_details['heroes'], sorted_blog_details['claps'])\n",
    "plt.xlabel('Heroes', fontsize=15)\n",
    "plt.ylabel('Claps', fontsize=15)\n",
    "plt.xticks(sorted_blog_details['heroes'], fontsize=9, rotation=90)\n",
    "plt.title('Heroes Vs Claps')\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre processing methods\n",
    "def clean_the_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    # Expanding Contractions eg. i've to I have\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r\".*native.*\\n?\",\"\",text)    \n",
    "    text = re.sub('Hey.+science.', '', text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "          \n",
    "    text = re.sub(r\".*Audio.*\\n?\",\"\",text)    \n",
    "    text = re.sub(r\".*sanyambhutani.com.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Newsletter.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*so much.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Sanyam Bhutani:.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Subscribe.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*twitter.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Flow.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Follow.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Playlist.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*support.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Linkedin.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Kaggle:.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*hosted.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\"interview[s|ed|ing]\",\"\",text,re.IGNORECASE)\n",
    "    text = re.sub(r\".*track.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*available.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Blog.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Correction.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*Note.*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*About.*\\n?\",\"\",text)\n",
    "    #text = re.sub(r\".*YouTube.*.?\",\"\",text)\n",
    "    text = re.sub(r\".*lazy.*\\n?\",\"\",text)\n",
    "   \n",
    "    #text = re.sub(r\".*find.*.?\",\"\",text)\n",
    "    text = re.sub(r\".*Link[s].*\\n?\",\"\",text)\n",
    "    text = re.sub(r\".*KaggleDaysMeetup.*\\n?\",\"\",text)\n",
    "    \n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"enjoy\",\"\",text)\n",
    "    text = re.sub(r\"show\",\"\",text)\n",
    "    text = re.sub(r\"even\",\"\",text)\n",
    "    text = re.sub(r\"another\",\"\",text)\n",
    "    text = re.sub(r\"time\",\"\",text)\n",
    "    text = re.sub(r\"part\",\"\",text)\n",
    "    text = re.sub(r\"sanyam\",\"\",text)\n",
    "    text = re.sub(r\"episode\",\"\",text)\n",
    "    text = re.sub(r\"interview\",\"\",text)\n",
    "    text = re.sub(r\"bhutani\",\"\",text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    nopunc = clean_the_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    common_words = set(['sure','everyone','chai','us','also','hello','find','check','currently','two','description','welcome','podcast','many','like','get','lot','please','hi','heres','really','one'])\n",
    "    stop_wds = common_words.union(sw.words('english'))\n",
    "    remove_stopwords = [w for w in tokenized_text if w not in stop_wds]\n",
    "    combined_text = ' '.join(remove_stopwords)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing and counting word length of script and introduction\n",
    "blog_details['script'] = blog_details['transcript'].apply(str).apply(lambda x: text_preprocessing(x))\n",
    "blog_details['intro'] = blog_details['introduction'].apply(str).apply(lambda x: text_preprocessing(x))\n",
    "blog_details['script_len'] = blog_details['script'].apply(str).apply(lambda x: len(x.split()))\n",
    "blog_details['intro_len'] = blog_details['intro'].apply(str).apply(lambda x: len(x.split()))\n",
    "blog_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the average word length of blog interviews and the range that they usually are in.\n",
    "print(\"Average Word Length of blog interviews is {} words.\".format(int(round(np.percentile(blog_details['script_len'],50)))))\n",
    "lower_range = int(round(np.percentile(blog_details['script_len'],.1)))\n",
    "upper_range = int(round(np.percentile(blog_details['script_len'],95)))\n",
    "print(\"Word Length of blog interviews are between {} and {} words.\".format(lower_range, upper_range))\n",
    "print(\"Dr.Vladimir's interview is a long one (2564 words), which is standing below as an outlier.\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x=blog_details[\"script_len\"], linewidth=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70831f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_blog_details[['heroes','claps','script_len','intro_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa93bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# We will look at Frequency Distribution of words used in introduction\n",
    "blog_guest_intro = []\n",
    "for s in blog_details['intro']:\n",
    "    blog_guest_intro.append(sent_tokenize(s))   \n",
    "\n",
    "gi_string = ''.join(str(gi) for gi in blog_guest_intro)\n",
    "gi_string = re.sub('[^a-zA-Z]', ' ', gi_string )\n",
    "gi_tokenized_word = nltk.word_tokenize(gi_string)\n",
    "gi_fdist = FreqDist(gi_tokenized_word)\n",
    "print(gi_fdist.most_common(80))\n",
    "\n",
    "%matplotlib inline\n",
    "gi_fdist.plot(30, cumulative=False,title = \"Common words used in Guest Introduction\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Bigrams :\")\n",
    "bigrams = nltk.bigrams(gi_tokenized_word)\n",
    "print(list(bigrams)[:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202d976",
   "metadata": {},
   "source": [
    "# Please carry on your investigation"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
