{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fed0420",
   "metadata": {},
   "source": [
    "![jane-street.png](attachment:jane-street.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d7e01",
   "metadata": {},
   "source": [
    "### Dueling Double Deep Q-Learning\n",
    "by Alin Cijov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d24d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667118d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "if device == 'cuda': import cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c61477",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/kaggle/input/jane-street-market-prediction/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path, device):\n",
    "    if device == 'cuda':\n",
    "        df = cudf.read_csv(path)\n",
    "    else:\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "    features = [column for column in df.columns if 'feature' in column]\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "\n",
    "# load data and features\n",
    "df, features = load_df(path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad44846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_actions(df, features, device):\n",
    "    f_mean = df[features[1:]].mean()\n",
    "    f_std = df[features[1:]].std()\n",
    "    \n",
    "    df = df.query('weight > 0').reset_index(drop = True)\n",
    "    df[features[1:]] = df[features[1:]].fillna(f_mean)\n",
    "    df[features[1:]] = (df[features[1:]] - f_mean) / f_std\n",
    "    df['action'] = (df['resp'] > 0).astype('int')\n",
    "    \n",
    "    if device == 'cuda': df = df.to_pandas()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# add the action column\n",
    "df = add_actions(df, features, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec90d87",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68aa4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, df, features):\n",
    "        self.n_samples = df.shape[0]\n",
    "        self.weight = torch.FloatTensor(df['weight'].values)\n",
    "        self.resp = torch.FloatTensor(df['resp'].values)\n",
    "        self.states = torch.FloatTensor(df[features].values)\n",
    "        self.observation_space = df[features].shape[1]\n",
    "        self.action_space = 2\n",
    "        self.idx = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.idx = 0\n",
    "        return self.states[self.idx].view(1, -1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = self.weight[self.idx] * self.resp[self.idx] * action\n",
    "        self.idx += 1\n",
    "        if self.idx >= self.n_samples:\n",
    "            done = True\n",
    "            self.idx = 0\n",
    "        else:\n",
    "            done = False\n",
    "        info = 0\n",
    "        return self.states[self.idx].view(1, -1), reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8dab7f",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5532e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, \n",
    "                 epsilon_start = 1.,\n",
    "                 epsilon_final = 0.01,\n",
    "                 epsilon_decay = 8000,\n",
    "                 gamma = 0.99, \n",
    "                 lr = 1e-4, \n",
    "                 target_net_update_freq = 1000, \n",
    "                 memory_size = 100000, \n",
    "                 batch_size = 128, \n",
    "                 learning_starts = 5000,\n",
    "                 max_frames = 10000000): \n",
    "\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_final = epsilon_final\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_by_frame = lambda i: self.epsilon_final + (self.epsilon_start - self.epsilon_final) * np.exp(-1. * i / self.epsilon_decay)\n",
    "\n",
    "        self.gamma =gamma\n",
    "        self.lr =lr\n",
    "\n",
    "        self.target_net_update_freq =target_net_update_freq\n",
    "        self.memory_size =memory_size\n",
    "        self.batch_size =batch_size\n",
    "\n",
    "        self.learning_starts = learning_starts\n",
    "        self.max_frames = max_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e8094",
   "metadata": {},
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = [] \n",
    "        dones = []\n",
    "\n",
    "        for b in batch: \n",
    "            states.append(b[0])\n",
    "            actions.append(b[1])\n",
    "            rewards.append(b[2])\n",
    "            next_states.append(b[3])\n",
    "            dones.append(b[4])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a76b15c",
   "metadata": {},
   "source": [
    "# Dueling Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97093f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingNetwork(nn.Module): \n",
    "    def __init__(self, obs, ac): \n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.utils.weight_norm(nn.Linear(obs, 512)),\n",
    "                                   nn.ReLU(), \n",
    "                                   nn.utils.weight_norm(nn.Linear(512, 256)),\n",
    "                                   nn.ReLU())\n",
    "\n",
    "        self.value_head = nn.utils.weight_norm(nn.Linear(256, 1))\n",
    "        self.adv_head = nn.utils.weight_norm(nn.Linear(256, ac))\n",
    "\n",
    "    def forward(self, x): \n",
    "        out = self.model(x)\n",
    "\n",
    "        value = self.value_head(out)\n",
    "        adv = self.adv_head(out)\n",
    "\n",
    "        q_val = value + adv - adv.mean(1).reshape(-1,1)\n",
    "        return q_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb8958",
   "metadata": {},
   "source": [
    "# Dueling DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDDQN(nn.Module): \n",
    "    def __init__(self, obs, ac, config): \n",
    "        super().__init__()\n",
    "\n",
    "        self.q = DuelingNetwork(obs, ac).to(device)\n",
    "        self.target = DuelingNetwork(obs, ac).to(device)\n",
    "\n",
    "        self.target.load_state_dict(self.q.state_dict())\n",
    "\n",
    "        self.target_net_update_freq = config.target_net_update_freq\n",
    "        self.update_counter = 0\n",
    "\n",
    "    def get_action(self, x):\n",
    "        x = torch.FloatTensor(x).to(device)\n",
    "        with torch.no_grad(): \n",
    "            a = self.q(x).max(1)[1]\n",
    "\n",
    "        return a.item()\n",
    "\n",
    "    def update_policy(self, adam, memory, params): \n",
    "        b_states, b_actions, b_rewards, b_next_states, b_masks = memory.sample(params.batch_size)\n",
    "\n",
    "        states = torch.FloatTensor(b_states).to(device)\n",
    "        actions = torch.LongTensor(b_actions).reshape(-1,1).to(device)\n",
    "        rewards = torch.FloatTensor(b_rewards).reshape(-1,1).to(device)\n",
    "        next_states = torch.FloatTensor(b_next_states).to(device)\n",
    "        masks = torch.FloatTensor(b_masks).reshape(-1,1).to(device)\n",
    "\n",
    "        current_q_values = self.q(states).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            max_next_q_vals = self.target(next_states).max(1)[0].reshape(-1,1)\n",
    "\n",
    "        expected_q_vals = rewards + max_next_q_vals * 0.99 * masks\n",
    "\n",
    "        loss = F.mse_loss(expected_q_vals, current_q_values)\n",
    "        \n",
    "        adam.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for p in self.q.parameters(): \n",
    "            p.grad.data.clamp_(-1.,1.)\n",
    "        adam.step()\n",
    "\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_net_update_freq == 0: \n",
    "            self.update_counter = 0 \n",
    "            self.target.load_state_dict(self.q.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0682aaf",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(df, features)       \n",
    "config = Config(epsilon_start = 1.,\n",
    "                epsilon_final = 0.01,\n",
    "                epsilon_decay = 8000,\n",
    "                gamma = 0.99, \n",
    "                lr = 1e-4, \n",
    "                target_net_update_freq = 1000, \n",
    "                memory_size = env.n_samples // 100, \n",
    "                batch_size = 128, \n",
    "                learning_starts = 5000,\n",
    "                max_frames = env.n_samples)\n",
    "memory = ExperienceReplay(config.memory_size)\n",
    "agent = DuelingDDQN(env.observation_space, env.action_space, config)\n",
    "adam = optim.Adam(agent.q.parameters(), lr = config.lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3acc061",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "ep_reward = 0. \n",
    "recap = []\n",
    "rewards = []\n",
    "\n",
    "p_bar = tqdm(total = config.max_frames)\n",
    "for frame in range(config.max_frames):\n",
    "\n",
    "    epsilon = config.epsilon_by_frame(frame)\n",
    "\n",
    "    if np.random.random() > epsilon: \n",
    "        action = agent.get_action(s)\n",
    "    else: \n",
    "        action = np.random.randint(0, env.action_space)\n",
    "\n",
    "    ns, r, done, infos = env.step(action)\n",
    "    ep_reward += r \n",
    "    if done:\n",
    "        ns = env.reset()\n",
    "        recap.append(ep_reward)\n",
    "        p_bar.set_description('Rew: {:.3f}'.format(ep_reward))\n",
    "        ep_reward = 0.\n",
    "\n",
    "    memory.push((s.reshape(-1).numpy().tolist(), action, r, ns.reshape(-1).numpy().tolist(), 0. if done else 1.))\n",
    "    s = ns  \n",
    "\n",
    "    p_bar.update(1)\n",
    "\n",
    "    if frame > config.learning_starts:\n",
    "        agent.update_policy(adam, memory, config)\n",
    "\n",
    "    if frame % 1000 == 0:\n",
    "        print(f'{frame + 1}/{config.max_frames}:', ep_reward, end = '\\r')\n",
    "        rewards.append(ep_reward.item())\n",
    "\n",
    "p_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c430a85",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ca550",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Rewards per Episode\")\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a52daf3",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "[Dueling DDQN Github Implementation](https://github.com/MoMe36/DuelingDDQN)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
