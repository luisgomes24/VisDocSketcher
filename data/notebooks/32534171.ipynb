{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69096059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/criticalitypredictioninsurance/train.csv')\n",
    "test_df = pd.read_csv('../input/criticalitypredictioninsurance/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d36ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657a1cd",
   "metadata": {},
   "source": [
    "### Separate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb1aea",
   "metadata": {},
   "source": [
    "As we see there is 120 features, now we try to dig into the dataframe by separating the columns based on their number of unique values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bcca94",
   "metadata": {},
   "source": [
    "First I begin by separating all columns as *\"int64\"* type. we remove also the **Label** columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drop = ['Label']\n",
    "tmp_df = train_df.drop(col_drop, axis=1)\n",
    "\n",
    "numerics = ['int64']\n",
    "\n",
    "train_df_1 = tmp_df.select_dtypes(include = numerics) # int64 columns\n",
    "train_df_2 = tmp_df.select_dtypes(exclude = numerics) # remaining columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c33d2",
   "metadata": {},
   "source": [
    "Now let's keep only the features in *train_df_1* where the unique number of values is less than 10, the remaining columns will be appended to *train_df_2* dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_11 = pd.DataFrame(train_df_1.nunique(), columns=['unique_val'])\n",
    "\n",
    "train_df_12 = train_df_11[train_df_11['unique_val']<=10]\n",
    "train_df_13 = train_df_11[train_df_11['unique_val']>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e310c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = train_df[train_df_12.index]\n",
    "train_df_2 = train_df_2.join(train_df[train_df_13.index])\n",
    "\n",
    "print('train_df_1 contains', len(train_df_1.columns),'columns')\n",
    "print('train_df_2 contains', len(train_df_2.columns),'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40a875",
   "metadata": {},
   "source": [
    "In below we see bar plot for train_df_1 where the unique values of 97 feautres is either =2 or =3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "sns.barplot(x=train_df_12.index, y=train_df_12['unique_val'], palette=\"rocket\", ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 90, size = 8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84529d6f",
   "metadata": {},
   "source": [
    "where this dataframe contains zero NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1 = train_df_1.isna().sum().sum()\n",
    "print('total number of NaN in train_df_1 dataframe is:', n_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00a447",
   "metadata": {},
   "source": [
    "Now let's focus on train_df_2 dataframe containing 23 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25db1d",
   "metadata": {},
   "source": [
    "However, our second dataframe contains many missing values which is shown in below as percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94062aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = train_df_2.shape[0]\n",
    "n_2 = train_df_2.isna().sum()\n",
    "n_2_p = (n_2)*100/n_rows\n",
    "round(n_2_p,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa651619",
   "metadata": {},
   "source": [
    "For simplicity, we exclude the features where there is missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drop_index = np.where(n_2_p)\n",
    "col_drop_names = train_df_2.columns[col_drop_index]\n",
    "\n",
    "train_df_2 = train_df_2.drop(col_drop_names,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d474bb",
   "metadata": {},
   "source": [
    "In below we split the **train_df_2** into two separate dataframes containing *object* and *numeric* formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b1e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_21 = train_df_2['Info_prod_2']\n",
    "train_df_22 = train_df_2.drop(['Info_prod_2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zscore = train_df_22.apply(zscore)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.violinplot(data=df_zscore, palette=\"Set3\", bw=.4, cut=0, linewidth=.4)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6045bfe",
   "metadata": {},
   "source": [
    "**To conclude:** train_df has \n",
    "<br> 120 features in total where\n",
    "<br> 97  features their unique values is either 3 or 2\n",
    "<br> from 23 remaining features\n",
    "<br> 13 of them were removed due to many number of missing values and\n",
    "<br> 10 features has numberic format, their distribution shown as violinplots and\n",
    "<br> 1 feature is categorical. \n",
    "<br> finally, column *label* contains 6 unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3eac43",
   "metadata": {},
   "source": [
    "**Next steps:**\n",
    "<br><br> 1.1) remove the 13 features with NaN from *train_df* and *test_df*\n",
    "<br> 1.2) convert *Info_prod_2* column which is the only categorical feature, to numeric format.\n",
    "<br><br> 2) split **train_df** to new *train and test* datasets\n",
    "<br> 3) run the multi-class classification\n",
    "<br> 3.1) create the model\n",
    "<br> 3.2) train the model\n",
    "<br> 3.3) test the model\n",
    "<br> 3.4) compute the performance of algorithm using a suitable metric\n",
    "<br> 4.1) test the trained model on **test_df**\n",
    "<br> 4.2) compute the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7850c21",
   "metadata": {},
   "source": [
    "#### 1.1) drop NaN features from both train_df and test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(col_drop_names, axis=1)\n",
    "test_df = test_df.drop(col_drop_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1 = train_df.isna().sum().sum()\n",
    "n_2 = test_df.isna().sum().sum()\n",
    "\n",
    "print('train_df contains:', n_1,'NaN')\n",
    "print('test_df contains: ', n_2,' NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a20fc73",
   "metadata": {},
   "source": [
    "#### 1.2) convert Info_prod_2 column from object to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e7b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Creating a instance of label Encoder.\n",
    "le = LabelEncoder()\n",
    " \n",
    "# Using .fit and .transform function to fit label\n",
    "# encoder and return encoded label\n",
    "le.fit(train_df['Info_prod_2'])\n",
    "train_df['Info_prod_2'] = le.transform(train_df['Info_prod_2'])\n",
    "test_df['Info_prod_2'] = le.transform(test_df['Info_prod_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06989a",
   "metadata": {},
   "source": [
    "### 2) split train_df into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(train_df) # get number of rows in the training set\n",
    "training_size = 0.75 # fraction of training data to split off for internal testing\n",
    "\n",
    "# set up separate training and testing sets\n",
    "# in this case using shuffled array indices\n",
    "# there are many more ways to do this too\n",
    "indices = np.array(range(n)) # makes an array of row indices in order\n",
    "shuffle(indices)\n",
    "split_point = int(n*training_size)\n",
    "mytrain_i = indices[0:split_point]\n",
    "mytest_i = indices[split_point:]\n",
    "\n",
    "# now use those shuffled indices to separating training from test dataframes\n",
    "new_train_df = train_df.iloc[mytrain_i]\n",
    "new_test_df = train_df.iloc[mytest_i]\n",
    "\n",
    "print(\"samples in the new training subset:\",len(new_train_df))\n",
    "print(\"samples in the new test subset:\",len(new_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Features and Output\n",
    "\n",
    "drop_cols = ['Label']\n",
    "\n",
    "X_train = new_train_df.drop(drop_cols, axis=1)\n",
    "y_train = new_train_df['Label']\n",
    "\n",
    "X_test = new_test_df.drop(drop_cols, axis=1)\n",
    "y_test = new_test_df['Label']\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f7a2c",
   "metadata": {},
   "source": [
    "### 3) run the multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e8e5a",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning using grid search and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50639f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = range(20,25)\n",
    "max_depth = range(20,25)\n",
    "min_samples_leaf = [2]\n",
    "bootstrap = [True]\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": n_estimators,\n",
    "    \"max_depth\": max_depth,\n",
    "    \"min_samples_leaf\": min_samples_leaf,\n",
    "    \"bootstrap\": bootstrap,\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clf_model = GridSearchCV(estimator=clf, param_grid=param_grid, cv=2, verbose=10, n_jobs=-1)\n",
    "clf_model.fit(X_train, y_train)\n",
    "y_pred = clf_model.predict(X_test)\n",
    "print(\"Using hyperparameters --> \\n\", clf_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12563219",
   "metadata": {},
   "source": [
    "#### Set chosen hyperparameters to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Random Forest Classification to the training subset of original training data\n",
    "clf = RandomForestClassifier(n_estimators = clf_model.best_params_['n_estimators'], \n",
    "                            max_depth = clf_model.best_params_['max_depth'],\n",
    "                            min_samples_leaf = 2,\n",
    "                            random_state = 42, \n",
    "                            bootstrap = True)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2999dd8",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy =        ', round(accuracy_score(y_test, y_pred),2))\n",
    "\n",
    "print('micro precision = ', round(precision_score(y_test, y_pred, average = 'micro'),2))\n",
    "print('macro precision = ', round(precision_score(y_test, y_pred, average = 'macro'),2))\n",
    "\n",
    "print('micro recall =    ', round(recall_score(y_test, y_pred, average = 'micro'),2))\n",
    "print('macro recall =    ', round(recall_score(y_test, y_pred, average = 'macro'),2))\n",
    "\n",
    "print('micro f1_score =  ', round(f1_score(y_test, y_pred, average = 'micro'),2))\n",
    "print('macro f1_score =  ', round(f1_score(y_test, y_pred, average = 'macro'),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380eb6f",
   "metadata": {},
   "source": [
    "### 4) test the trained model on original test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a705fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(drop_cols, axis=1)\n",
    "y_train = train_df['Label']\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "clf.predict(test_df)\n",
    "\n",
    "test_df['Label'] = clf.predict(test_df)\n",
    "\n",
    "test_df.index.name = 'ID'\n",
    "test_df.index = test_df.index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['Label']].to_csv('my_prediction.csv', \n",
    "    index=True, header=True)\n",
    "\n",
    "print(\"Prediction complete. Saved as my_prediction.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
