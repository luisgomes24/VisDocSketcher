{\n  "data_sources": [\n    {\n      "file_name": "fashion-mnist_train.csv",\n      "description": "Training data containing 60k grayscale images of fashion articles and their labels."\n    },\n    {\n      "file_name": "fashion-mnist_test.csv",\n      "description": "Test data containing 10k grayscale images of fashion articles and their labels."\n    }\n  ],\n  "data_variables": [\n    {\n      "variable_name": "train_data",\n      "description": "DataFrame containing the training dataset for Fashion-MNIST."\n    },\n    {\n      "variable_name": "test_data",\n      "description": "DataFrame containing the testing dataset for Fashion-MNIST."\n    },\n    {\n      "variable_name": "train_data_x",\n      "description": "Features (pixel values) of training data extracted from train_data."\n    },\n    {\n      "variable_name": "train_data_y",\n      "description": "Labels of training data extracted from train_data."\n    },\n    {\n      "variable_name": "test_data_x",\n      "description": "Features (pixel values) of testing data extracted from test_data."\n    },\n    {\n      "variable_name": "test_data_y",\n      "description": "Labels of testing data extracted from test_data."\n    },\n    {\n      "variable_name": "train_x",\n      "description": "NumPy array of scaled training features ready for model training."\n    },\n    {\n      "variable_name": "train_y",\n      "description": "One-hot encoded NumPy array of training labels."\n    },\n    {\n      "variable_name": "test_x",\n      "description": "NumPy array of scaled testing features ready for model evaluation."\n    },\n    {\n      "variable_name": "test_y",\n      "description": "One-hot encoded NumPy array of testing labels."\n    },\n    {\n      "variable_name": "cv_x",\n      "description": "Cross-validation features obtained after splitting training features."\n    },\n    {\n      "variable_name": "cv_y",\n      "description": "Cross-validation labels obtained after splitting training labels."\n    }\n  ],\n  "data_flow": [\n    {\n      "variable_name": "train_data",\n      "created_by": "pd.read_csv",\n      "flows_to": ["train_data_x", "train_data_y"]\n    },\n    {\n      "variable_name": "test_data",\n      "created_by": "pd.read_csv",\n      "flows_to": ["test_data_x", "test_data_y"]\n    },\n    {\n      "variable_name": "train_data_x",\n      "created_by": "train_data.iloc[:, 1:785]",\n      "flows_to": ["train_x"]\n    },\n    {\n      "variable_name": "train_data_y",\n      "created_by": "train_data.iloc[:, 0:1]",\n      "flows_to": ["train_y"]\n    },\n    {\n      "variable_name": "test_data_x",\n      "created_by": "test_data.iloc[:, 1:785]",\n      "flows_to": ["test_x"]\n    },\n    {\n      "variable_name": "test_data_y",\n      "created_by": "test_data.iloc[:, 0:1]",\n      "flows_to": ["test_y"]\n    },\n    {\n      "variable_name": "train_x",\n      "created_by": "train_data_x.as_matrix()",\n      "flows_to": ["cv_x"]\n    },\n    {\n      "variable_name": "train_y",\n      "created_by": "to_categorical(train_data_y, 10)",\n      "flows_to": []\n    },\n    {\n      "variable_name": "test_x",\n      "created_by": "test_data_x.as_matrix()",\n      "flows_to": []\n    },\n    {\n      "variable_name": "test_y",\n      "created_by": "to_categorical(test_data_y, 10)",\n      "flows_to": []\n    },\n    {\n      "variable_name": "cv_x",\n      "created_by": "train_test_split(train_x, train_y, test_size = 5000)",\n      "flows_to": []\n    },\n    {\n      "variable_name": "cv_y",\n      "created_by": "train_test_split(train_x, train_y, test_size = 5000)",\n      "flows_to": []\n    }\n  ],\n  "models": [\n    {\n      "model_name": "Convolutional Neural Network",\n      "input_features": "train_x and test_x (image pixel values)",\n      "target_variable": "train_y and test_y (one-hot encoded labels)",\n      "hyperparameters": {\n        "learning_rate": 0.01,\n        "epoch": 15,\n        "batch_size": 128,\n        "filters": [32, 64],\n        "kernel_size": [5, 5],\n        "units": 1024,\n        "dropout_rate": 0.2\n      }\n    }\n  ]\n}